{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nMtKfkp2JElN",
        "qx2HjXFLJvD_",
        "_S1jtSyFJ84i",
        "qgwtdVsZKNO-",
        "LMptc2nRKW-P",
        "E-OTxEkIKlqe",
        "I4Ut_UzpK3Tk"
      ],
      "authorship_tag": "ABX9TyNJFbNNdq256/R6q61qssJY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TatanPerez/Teoria_Aprendizaje_Maquina/blob/main/Parciales/1_Dashboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelos de Regresion**\n",
        "\n",
        "**Elaborado por:** Wilmer Sebastian Perez C. wiperezc@unal.edu.co\n",
        "\n",
        "**Universidad Nacional de Colombia - Sede Manizales**\n",
        "\n",
        "**Mayo del 2025-I**"
      ],
      "metadata": {
        "id": "hkTbrxI1Hd_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conjunto de datos Ames Housing Dataset como ejemplo completo para un problema de regresión usando sci-kitlearn**\n",
        "\n",
        "El siguiente ejemplo presenta las etapas básicas de un proyecto de analítica de datos en una tarea de regresión, orientadas a:\n",
        "\n",
        "- Preproceso de atributos con campos vacios y tipo texto.\n",
        "- Entrenamiento y selección de un modelo de regresión bajo una estrategia de validación cruzada.\n",
        "- La utilización de diccionarios para la sintonización de hiperparámetros.\n",
        "- Se ilustra también la creación de clases (objetos) propios compatibles con la clase pipeline de sci-kitlearn.\n",
        "\n",
        "**Base de datos utilizada**: [Ames Housing - Kaggle](https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset)."
      ],
      "metadata": {
        "id": "q0e2zCziHCpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalación de librerías**"
      ],
      "metadata": {
        "id": "c9oNY0WsIn5G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVJDTHZ14XXd",
        "outputId": "be28291e-5de3-4999-eff1-948a06f814dc",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.5.0)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-25.1.0 scikit-optimize-0.10.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m591.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.8\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.8)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.39.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-optimize\n",
        "!pip install streamlit -q #instalación de librerías\n",
        "!pip install pyngrok\n",
        "!pip install optuna\n",
        "!pip install streamlit pandas matplotlib seaborn scikit-learn pyngrok kagglehub\n",
        "!pip install pyngrok streamlit --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Crear carpeta pages para trabajar Multiapp en Streamlit"
      ],
      "metadata": {
        "id": "MGT_uqEnI3nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pages"
      ],
      "metadata": {
        "id": "wR2Dd1LK_73w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Página principal**"
      ],
      "metadata": {
        "id": "nMtKfkp2JElN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile TAM.py\n",
        "import streamlit as st\n",
        "from pyngrok import ngrok\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import joblib\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import ElasticNet, SGDRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from scipy.stats import loguniform, uniform\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# -----------------------\n",
        "# CONFIGURACIÓN INICIAL\n",
        "# -----------------------\n",
        "st.set_page_config(page_title=\"TAM - Modelos Regresión\", page_icon=\"👋\", layout=\"wide\")\n",
        "st.write(\"Parcial 1 TAM - Modelos de regresión\")\n",
        "\n",
        "# Sidebar con información del equipo\n",
        "st.sidebar.title(\"Equipo del Proyecto\")\n",
        "st.sidebar.success(\"Seleccciona una modelo a explorar.\")\n",
        "st.markdown(\"\"\"\n",
        "### Conjunto de datos Ames Housing Dataset como ejemplo completo para un problema de regresión usando sci-kitlearn\n",
        "\n",
        "El siguiente ejemplo presenta las etapas básicas de un proyecto de analítica de datos en una tarea de regresión, orientadas a:\n",
        "\n",
        "- Preproceso de atributos con campos vacios y tipo texto.\n",
        "- Entrenamiento y selección de un modelo de regresión bajo una estrategia de validación cruzada.\n",
        "- La utilización de diccionarios para la sintonización de hiperparámetros.\n",
        "- Se ilustra también la creación de clases (objetos) propios compatibles con la clase pipeline de sci-kitlearn.\n",
        "\n",
        "**Base de datos utilizada**: [Ames Housing - Kaggle](https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset).\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_n3VEV63djm",
        "outputId": "ae7924f6-6c27-49d1-8ebd-a91eda6f4877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing TAM.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Páginas**"
      ],
      "metadata": {
        "id": "0bHXSB7-JXSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analisis De Datos**"
      ],
      "metadata": {
        "id": "qx2HjXFLJvD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 1_📊_ANALISIS_EXPLORATORIO.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "st.set_page_config(page_title=\"Analisis Exploratorio\", page_icon=\"📊\")\n",
        "\n",
        "st.markdown(\"Analisis  Exploratorio\")\n",
        "st.sidebar.header(\"Analisis Exploratorio\")\n",
        "\n",
        "# ===============================\n",
        "# 1. Cargar la base de datos\n",
        "# ===============================\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    path = kagglehub.dataset_download(\"shashanknecrothapa/ames-housing-dataset\")\n",
        "    csv_file_path = os.path.join(path, \"AmesHousing.csv\")\n",
        "    return pd.read_csv(csv_file_path)\n",
        "\n",
        "with st.spinner('Cargando datos...'):\n",
        "    df = load_data()\n",
        "    Xdata = df.copy()\n",
        "\n",
        "# Mostrar opciones de visualización de datos\n",
        "if st.checkbox('Mostrar datos crudos'):\n",
        "    st.dataframe(Xdata.head())\n",
        "\n",
        "# ===============================\n",
        "# 2. Análisis Exploratorio\n",
        "# ===============================\n",
        "st.header(\"🔍 Análisis Exploratorio\")\n",
        "\n",
        "# Mostrar información básica\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.metric(\"Número de filas\", Xdata.shape[0])\n",
        "    st.metric(\"Número de columnas\", Xdata.shape[1])\n",
        "\n",
        "with col2:\n",
        "    st.write(\"Tipos de variables:\")\n",
        "    st.write(Xdata.dtypes.value_counts())\n",
        "\n",
        "# Columnas numéricas y categóricas\n",
        "num_cols = Xdata.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "cat_cols = Xdata.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "# Valores nulos\n",
        "st.subheader(\"Valores nulos\")\n",
        "missing_values = Xdata.isnull().sum()\n",
        "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
        "st.bar_chart(missing_values)\n",
        "\n",
        "# ===============================\n",
        "# 3. Análisis Descriptivo\n",
        "# ===============================\n",
        "st.header(\"📊 Análisis Descriptivo\")\n",
        "\n",
        "if st.checkbox('Mostrar estadísticas descriptivas'):\n",
        "    desc_stats = Xdata[num_cols].describe().T\n",
        "    st.dataframe(desc_stats)\n",
        "\n",
        "# Correlación con el target\n",
        "st.subheader(\"Correlación con SalePrice\")\n",
        "correlation_with_target = Xdata[num_cols].corr()[\"SalePrice\"].sort_values(ascending=False)\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.write(\"Correlación positiva más fuerte:\")\n",
        "    st.dataframe(correlation_with_target.head(10))\n",
        "\n",
        "with col2:\n",
        "    st.write(\"Correlación negativa más fuerte:\")\n",
        "    st.dataframe(correlation_with_target.tail(10))\n",
        "\n",
        "# ===============================\n",
        "# 4. Análisis Relacional (Correlación visual)\n",
        "# ===============================\n",
        "st.header(\"🔗 Análisis Relacional\")\n",
        "\n",
        "# Agregar slider en el sidebar para seleccionar el umbral de correlación\n",
        "min_corr = st.sidebar.slider(\n",
        "    \"Umbral mínimo de correlación (|r|)\",\n",
        "    min_value=0.1,\n",
        "    max_value=0.9,\n",
        "    value=0.3,\n",
        "    step=0.05,\n",
        "    help=\"Seleccione el valor mínimo absoluto de correlación para incluir variables en el análisis\"\n",
        ")\n",
        "\n",
        "if st.checkbox('Mostrar matriz de correlación'):\n",
        "    # Usar el valor seleccionado en el slider\n",
        "    strong_corr = Xdata[num_cols].corr()[\"SalePrice\"].abs()\n",
        "    strong_vars = strong_corr[strong_corr > min_corr].index\n",
        "\n",
        "    # Verificar que hayamos seleccionado al menos 2 variables\n",
        "    if len(strong_vars) < 2:\n",
        "        st.warning(f\"No hay suficientes variables con correlación |r| > {min_corr}. Reduzca el umbral.\")\n",
        "    else:\n",
        "        filtered_corr = Xdata[strong_vars].corr()\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "        sns.heatmap(filtered_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True, ax=ax)\n",
        "        ax.set_title(f\"Matriz de Correlación (|r| > {min_corr}) con SalePrice\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Explicación del análisis\n",
        "        st.subheader(\"🧠 ¿Qué hace este análisis?\")\n",
        "\n",
        "        explanation = f\"\"\"\n",
        "        **Este análisis muestra las correlaciones entre variables con una relación significativa con SalePrice (|r| > {min_corr}):**\n",
        "\n",
        "        - **Variables incluidas:** {len(strong_vars)} variables numéricas\n",
        "        - **Umbral de correlación:** |r| > {min_corr}\n",
        "        - **Interpretación de colores:**\n",
        "          - 🔴 Rojo: Correlación positiva\n",
        "          - 🔵 Azul: Correlación negativa\n",
        "          - Color más intenso = Correlación más fuerte\n",
        "\n",
        "        **Variables seleccionadas:** {', '.join(strong_vars)}\n",
        "        \"\"\"\n",
        "\n",
        "        st.markdown(explanation)\n",
        "\n",
        "        # Mostrar correlaciones individuales con SalePrice\n",
        "        st.write(\"**Correlaciones individuales con SalePrice:**\")\n",
        "        corr_with_target = Xdata[strong_vars].corr()[\"SalePrice\"].sort_values(ascending=False)\n",
        "        st.dataframe(corr_with_target.to_frame(\"Correlación\"))\n",
        "\n",
        "        # Consejo sobre multicolinealidad\n",
        "        st.info(\"\"\"\n",
        "        💡 **Consejo profesional:**\n",
        "        Si dos variables predictoras tienen correlación > 0.8 entre sí, considere:\n",
        "        - Eliminar una de ellas\n",
        "        - Crear una nueva característica combinada\n",
        "        - Usar técnicas de reducción de dimensionalidad como PCA\n",
        "        \"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 5. Análisis Visual de Relaciones Clave\n",
        "# ===============================\n",
        "st.header(\"📈 Análisis Visual\")\n",
        "\n",
        "important_features = st.multiselect(\n",
        "    \"Seleccione características para visualizar\",\n",
        "    options=num_cols.tolist(),\n",
        "    default=[\"1st Flr SF\", \"Year Built\", \"Overall Qual\"]\n",
        ")\n",
        "\n",
        "if important_features:\n",
        "    plot_data = Xdata[important_features + [\"SalePrice\"]].copy()\n",
        "\n",
        "    # Normalizar los datos\n",
        "    scaler = StandardScaler()\n",
        "    plot_data_normalized = pd.DataFrame(scaler.fit_transform(plot_data),\n",
        "                                      columns=plot_data.columns,\n",
        "                                      index=plot_data.index)\n",
        "\n",
        "    for feature in important_features:\n",
        "        st.subheader(f\"Relación entre {feature} y SalePrice\")\n",
        "\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            fig, ax = plt.subplots()\n",
        "            sns.scatterplot(data=plot_data, x=feature, y=\"SalePrice\", ax=ax)\n",
        "            ax.set_title(f\"Original\")\n",
        "            st.pyplot(fig)\n",
        "\n",
        "        with col2:\n",
        "            fig, ax = plt.subplots()\n",
        "            sns.scatterplot(data=plot_data_normalized, x=feature, y=\"SalePrice\", ax=ax)\n",
        "            ax.set_title(f\"Normalizado\")\n",
        "            st.pyplot(fig)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# 6. Limpieza de datos\n",
        "# ===============================\n",
        "st.header(\"🧹 Limpieza de Datos\")\n",
        "\n",
        "if st.checkbox('Realizar limpieza de datos'):\n",
        "    # Guardar el estado original para comparación\n",
        "    original_shape = Xdata.shape\n",
        "    original_columns = Xdata.columns.tolist()\n",
        "\n",
        "    # Realizar limpieza\n",
        "    Xdata = Xdata.sample(frac=0.20, random_state=42)\n",
        "    cols_to_drop = ['Order', 'PID']\n",
        "    Xdata.drop(columns=[col for col in cols_to_drop if col in Xdata.columns], inplace=True)\n",
        "\n",
        "    high_null_cols = Xdata.columns[Xdata.isnull().mean() > 0.4].tolist()\n",
        "    Xdata.drop(columns=high_null_cols, inplace=True)\n",
        "\n",
        "    st.success(\"Limpieza completada!\")\n",
        "\n",
        "    # Mostrar comparación visual\n",
        "    st.subheader(\"📊 Comparación Antes/Después de la Limpieza\")\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        st.markdown(\"**Antes de la limpieza**\")\n",
        "        st.metric(\"Número de filas\", original_shape[0])\n",
        "        st.metric(\"Número de columnas\", original_shape[1])\n",
        "        st.write(\"Ejemplo de datos:\")\n",
        "        st.dataframe(df.head(3))\n",
        "\n",
        "    with col2:\n",
        "        st.markdown(\"**Después de la limpieza**\")\n",
        "        st.metric(\"Número de filas\", Xdata.shape[0], delta=f\"{-((original_shape[0]-Xdata.shape[0])/original_shape[0]*100):.1f}%\")\n",
        "        st.metric(\"Número de columnas\", Xdata.shape[1], delta=f\"{-((original_shape[1]-Xdata.shape[1])/original_shape[1]*100):.1f}%\")\n",
        "        st.write(\"Ejemplo de datos limpios:\")\n",
        "        st.dataframe(Xdata.head(3))\n",
        "\n",
        "    # Mostrar columnas eliminadas y conservadas\n",
        "    st.subheader(\"🔍 Detalles de la Limpieza\")\n",
        "\n",
        "    dropped_columns = list(set(original_columns) - set(Xdata.columns))\n",
        "    st.write(f\"✅ Columnas conservadas: {len(Xdata.columns)}\")\n",
        "    st.write(f\"❌ Columnas eliminadas: {len(dropped_columns)}\")\n",
        "\n",
        "    if dropped_columns:\n",
        "        st.write(\"Columnas eliminadas:\")\n",
        "        st.write(dropped_columns)\n",
        "\n",
        "    # Visualización de valores nulos después de la limpieza\n",
        "    st.subheader(\"🔎 Valores Nulos Restantes\")\n",
        "    missing_after = Xdata.isnull().sum()\n",
        "    missing_after = missing_after[missing_after > 0].sort_values(ascending=False)\n",
        "\n",
        "    if not missing_after.empty:\n",
        "        st.bar_chart(missing_after)\n",
        "        st.write(\"Nota: Aún quedan algunas columnas con valores nulos que podrían necesitar tratamiento adicional.\")\n",
        "    else:\n",
        "        st.success(\"¡No hay valores nulos restantes en el dataset!\")\n",
        "\n",
        "    # Mostrar estructura final del dataset\n",
        "    st.subheader(\"🏁 Estructura Final del Dataset\")\n",
        "    st.write(\"Tipos de variables en el dataset limpio:\")\n",
        "    st.write(Xdata.dtypes.value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxUAoIf7FA-D",
        "outputId": "dc58f504-bfe9-4fb2-d030-c593e72178d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 1_📊_ANALISIS_EXPLORATORIO.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 1_📊_ANALISIS_EXPLORATORIO.py pages/"
      ],
      "metadata": {
        "id": "hAcbRIJVGuAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pre-Seleccion de modelos**"
      ],
      "metadata": {
        "id": "_S1jtSyFJ84i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 2_🔍_Preseleccion_de_modelo.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet, BayesianRidge, SGDRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "import kagglehub\n",
        "\n",
        "st.set_page_config(page_title=\"Modelados Predictivos\", page_icon=\"📊\", layout=\"wide\")\n",
        "st.markdown(\"Modelos preditivos\")\n",
        "st.sidebar.header(\"Pre-seleccion de modelo\")\n",
        "\n",
        "# ===============================\n",
        "# 1. Carga y Preparación de Datos\n",
        "# ===============================\n",
        "st.header(\"🏡 Carga y Preparación de Datos\")\n",
        "\n",
        "@st.cache_data\n",
        "def load_and_prepare_data():\n",
        "    with st.spinner('Descargando y preparando datos...'):\n",
        "        path = kagglehub.dataset_download(\"shashanknecrothapa/ames-housing-dataset\")\n",
        "        csv_file_path = os.path.join(path, \"AmesHousing.csv\")\n",
        "        Xdata = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Limpieza de datos\n",
        "        Xdata = Xdata.sample(frac=0.20, random_state=42)\n",
        "        cols_to_drop = ['Order', 'PID']\n",
        "        Xdata.drop(columns=[col for col in cols_to_drop if col in Xdata.columns], inplace=True)\n",
        "        high_null_cols = Xdata.columns[Xdata.isnull().mean() > 0.4].tolist()\n",
        "        Xdata.drop(columns=high_null_cols, inplace=True)\n",
        "\n",
        "        return Xdata\n",
        "\n",
        "Xdata = load_and_prepare_data()\n",
        "\n",
        "# Mostrar datos\n",
        "if st.checkbox('Mostrar datos preparados'):\n",
        "    st.dataframe(Xdata.head())\n",
        "\n",
        "# ===============================\n",
        "# 2. Transformación de Variables\n",
        "# ===============================\n",
        "st.header(\"📐 Transformación de Variables\")\n",
        "\n",
        "col_sal = \"SalePrice\"\n",
        "\n",
        "# Selector para visualizar transformación\n",
        "transform_col = st.selectbox(\"Seleccione columna para visualizar transformación\",\n",
        "                            options=Xdata.select_dtypes(include=['int64', 'float64']).columns)\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(Xdata[transform_col], kde=True)\n",
        "    plt.title(f'Distribución Original de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "with col2:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(np.log1p(Xdata[transform_col]), kde=True)\n",
        "    plt.title(f'Distribución con log1p de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "st.info(\"\"\"\n",
        "💡 **Transformación logarítmica (log1p):**\n",
        "- Se aplica para manejar distribuciones sesgadas\n",
        "- log1p = log(1 + x) evita problemas con valores cero\n",
        "- Ayuda a cumplir supuestos de normalidad en modelos lineales\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 3. División de Datos\n",
        "# ===============================\n",
        "st.header(\"✂️ División del Dataset\")\n",
        "\n",
        "test_size = st.slider(\"Porcentaje para test\", 10, 40, 30, 5)\n",
        "\n",
        "Xtrain, Xtest = train_test_split(Xdata, test_size=test_size/100, random_state=42)\n",
        "ytrain = np.log1p(Xtrain[col_sal])\n",
        "ytest = np.log1p(Xtest[col_sal])\n",
        "Xtrain = Xtrain.drop(columns=col_sal)\n",
        "Xtest = Xtest.drop(columns=col_sal)\n",
        "\n",
        "st.success(f\"\"\"\n",
        "División completada:\n",
        "- Entrenamiento: {Xtrain.shape[0]} registros ({100-test_size}%)\n",
        "- Prueba: {Xtest.shape[0]} registros ({test_size}%)\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 4. Preprocesamiento\n",
        "# ===============================\n",
        "st.header(\"🔧 Pipeline de Preprocesamiento\")\n",
        "\n",
        "numeric_cols = Xtrain.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = Xtrain.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "with st.expander(\"Ver detalles de preprocesamiento\"):\n",
        "    st.write(\"**Columnas numéricas:**\")\n",
        "    st.write(numeric_cols)\n",
        "    st.write(\"**Transformaciones:** Imputación con mediana + Estandarización\")\n",
        "\n",
        "    st.write(\"\\n**Columnas categóricas:**\")\n",
        "    st.write(categorical_cols)\n",
        "    st.write(\"**Transformaciones:** Imputación con 'missing' + One-Hot Encoding\")\n",
        "\n",
        "# ===============================\n",
        "# 5. Selección de Modelos\n",
        "# ===============================\n",
        "st.header(\"🤖 Selección de Modelos\")\n",
        "\n",
        "# Configuración de modelos\n",
        "models = {\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"Lasso\": Lasso(alpha=0.1),\n",
        "    \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
        "    \"KernelRidge\": KernelRidge(alpha=1.0),\n",
        "    \"SGDRegressor\": SGDRegressor(max_iter=1000, tol=1e-3),\n",
        "    \"BayesianRidge\": BayesianRidge(),\n",
        "    \"GaussianProcess\": GaussianProcessRegressor(),\n",
        "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"SVR\": SVR()\n",
        "}\n",
        "\n",
        "# Permitir selección de modelos\n",
        "selected_models = st.multiselect(\n",
        "    \"Seleccione modelos a evaluar\",\n",
        "    options=list(models.keys()),\n",
        "    default=[\"LinearRegression\", \"RandomForest\", \"Lasso\"]\n",
        ")\n",
        "\n",
        "# Configuración de métricas\n",
        "scoring = {\n",
        "    'MAE': 'neg_mean_absolute_error',\n",
        "    'MSE': 'neg_mean_squared_error',\n",
        "    'R2': 'r2',\n",
        "    'MAPE': 'neg_mean_absolute_percentage_error'\n",
        "}\n",
        "\n",
        "# ===============================\n",
        "# 6. Evaluación de Modelos (Versión Mejorada)\n",
        "# ===============================\n",
        "if st.button(\"Evaluar Modelos\"):\n",
        "    st.header(\"📊 Resultados de Evaluación\")\n",
        "\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    # Estructura para almacenar resultados\n",
        "    results = {\n",
        "        'Modelo': [],\n",
        "        'MAE_mean': [], 'MAE_std': [],\n",
        "        'MSE_mean': [], 'MSE_std': [],\n",
        "        'R2_mean': [], 'R2_std': [],\n",
        "        'MAPE_mean': [], 'MAPE_std': []\n",
        "    }\n",
        "\n",
        "    for i, (name, regressor) in enumerate([(m, models[m]) for m in selected_models]):\n",
        "        status_text.text(f\"Evaluando {name}...\")\n",
        "        progress_bar.progress((i+1)/len(selected_models))\n",
        "\n",
        "        model = Pipeline(steps=[\n",
        "            (\"preprocessing\", preprocessor),\n",
        "            (\"regressor\", regressor)\n",
        "        ])\n",
        "\n",
        "        cv_results = cross_validate(model, Xtrain, ytrain, cv=5, scoring=scoring)\n",
        "\n",
        "        # Almacenar resultados con media y desviación estándar\n",
        "        results['Modelo'].append(name)\n",
        "\n",
        "        # MAE\n",
        "        results['MAE_mean'].append(-cv_results['test_MAE'].mean())\n",
        "        results['MAE_std'].append(cv_results['test_MAE'].std())\n",
        "\n",
        "        # MSE\n",
        "        results['MSE_mean'].append(-cv_results['test_MSE'].mean())\n",
        "        results['MSE_std'].append(cv_results['test_MSE'].std())\n",
        "\n",
        "        # R2\n",
        "        results['R2_mean'].append(cv_results['test_R2'].mean())\n",
        "        results['R2_std'].append(cv_results['test_R2'].std())\n",
        "\n",
        "        # MAPE (convertido a porcentaje)\n",
        "        results['MAPE_mean'].append(-cv_results['test_MAPE'].mean() * 100)\n",
        "        results['MAPE_std'].append(cv_results['test_MAPE'].std() * 100)\n",
        "\n",
        "    # Crear DataFrame con los resultados\n",
        "    results_df = pd.DataFrame(results).set_index('Modelo')\n",
        "\n",
        "    # Mostrar resultados en una tabla expandible\n",
        "    with st.expander(\"📋 Resultados Detallados (Media ± Desviación Estándar)\", expanded=True):\n",
        "        # Crear representación de cadenas para media ± std\n",
        "        display_df = pd.DataFrame(index=results_df.index)\n",
        "\n",
        "        for metric in ['MAE', 'MSE', 'R2', 'MAPE']:\n",
        "            display_df[metric] = results_df.apply(\n",
        "                lambda x: f\"{x[f'{metric}_mean']:.4f} ± {x[f'{metric}_std']:.4f}\",\n",
        "                axis=1\n",
        "            )\n",
        "            # Formato especial para MAPE (porcentaje)\n",
        "            if metric == 'MAPE':\n",
        "                display_df[metric] = results_df.apply(\n",
        "                    lambda x: f\"{x[f'{metric}_mean']:.2f}% ± {x[f'{metric}_std']:.2f}%\",\n",
        "                    axis=1\n",
        "                )\n",
        "\n",
        "        st.dataframe(display_df.style.background_gradient(cmap='Blues', axis=0))\n",
        "\n",
        "    # Visualización de resultados con intervalos de confianza\n",
        "    st.subheader(\"📈 Comparación Visual con Variabilidad\")\n",
        "\n",
        "    metric_to_plot = st.selectbox(\"Seleccione métrica para visualizar\",\n",
        "                                options=['MAE', 'MSE', 'R2', 'MAPE'])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Ordenar modelos por la métrica seleccionada\n",
        "    sorted_models = results_df[f'{metric_to_plot}_mean'].sort_values().index\n",
        "\n",
        "    # Crear gráfico de barras con barras de error\n",
        "    y_pos = range(len(sorted_models))\n",
        "    means = results_df.loc[sorted_models, f'{metric_to_plot}_mean']\n",
        "    stds = results_df.loc[sorted_models, f'{metric_to_plot}_std']\n",
        "\n",
        "    bars = ax.barh(y_pos, means, xerr=stds, align='center', alpha=0.7, capsize=5)\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(sorted_models)\n",
        "    ax.invert_yaxis()  # Mejor modelo en la parte superior\n",
        "\n",
        "    if metric_to_plot == 'MAPE':\n",
        "        ax.set_xlabel(f\"{metric_to_plot} (%)\")\n",
        "    else:\n",
        "        ax.set_xlabel(metric_to_plot)\n",
        "\n",
        "    ax.set_title(f\"Comparación de {metric_to_plot} entre Modelos\\n(con desviación estándar)\")\n",
        "\n",
        "    # Añadir valores numéricos a las barras\n",
        "    for bar, mean, std in zip(bars, means, stds):\n",
        "        if metric_to_plot == 'MAPE':\n",
        "            ax.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{mean:.2f} ± {std:.2f}',\n",
        "                   va='center', ha='left')\n",
        "        else:\n",
        "            ax.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{mean:.4f} ± {std:.4f}',\n",
        "                   va='center', ha='left')\n",
        "\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Análisis de estabilidad de modelos\n",
        "    st.subheader(\"🔍 Análisis de Estabilidad de Modelos\")\n",
        "\n",
        "    # Calcular coeficiente de variación para cada modelo (std/mean)\n",
        "    stability_df = pd.DataFrame(index=results_df.index)\n",
        "    for metric in ['MAE', 'MSE', 'R2', 'MAPE']:\n",
        "        stability_df[f'CV_{metric}'] = results_df[f'{metric}_std'] / results_df[f'{metric}_mean']\n",
        "\n",
        "    # Identificar modelos más estables (menor variabilidad relativa)\n",
        "    st.write(\"**Coeficiente de Variación (CV = σ/μ) - Menor es mejor:**\")\n",
        "    st.dataframe(stability_df.style.background_gradient(cmap='Greens_r', axis=0))\n",
        "\n",
        "    st.info(\"\"\"\n",
        "    **Interpretación:**\n",
        "    - La desviación estándar muestra cuánto varían los resultados entre los folds de validación cruzada\n",
        "    - Un modelo con baja desviación estándar es más consistente\n",
        "    - El coeficiente de variación (CV) normaliza la variabilidad respecto a la media\n",
        "    \"\"\")\n",
        "\n",
        "    # Recomendación del mejor modelo considerando media y variabilidad\n",
        "    best_model_mean = results_df['R2_mean'].idxmax()\n",
        "    best_model_stable = stability_df['CV_R2'].idxmin()\n",
        "\n",
        "    if best_model_mean == best_model_stable:\n",
        "        st.success(f\"🎯 **Mejor modelo:** {best_model_mean} (Mayor R²: {results_df.loc[best_model_mean, 'R2_mean']:.4f} y menor variabilidad)\")\n",
        "    else:\n",
        "        st.success(f\"\"\"\n",
        "        🎯 **Recomendaciones:**\n",
        "        - **Mejor rendimiento:** {best_model_mean} (R² = {results_df.loc[best_model_mean, 'R2_mean']:.4f})\n",
        "        - **Más estable:** {best_model_stable} (CV = {stability_df.loc[best_model_stable, 'CV_R2']:.4f})\n",
        "        \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXQIXpGdaVTd",
        "outputId": "12195497-141e-437f-d71e-54bb0d56065e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 2_🔍_Preseleccion_de_modelo.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 2_🔍_Preseleccion_de_modelo.py pages/"
      ],
      "metadata": {
        "id": "UEoFgycSa7Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. 🔗 Elastic Net: Función de optimización**\n",
        "\n",
        "El modelo **Elastic Net** busca minimizar la siguiente función objetivo:\n",
        "\n",
        "$$\n",
        "\\min_{\\beta} \\; \\frac{1}{2n} \\| y - X\\beta \\|_2^2 + \\lambda \\left( \\alpha \\|\\beta\\|_1 + \\frac{1 - \\alpha}{2} \\|\\beta\\|_2^2 \\right)\n",
        "$$\n",
        "\n",
        "**Donde:**\n",
        "- $X \\in \\mathbb{R}^{n \\times p} \\$ : matriz de características (n muestras, p variables).\n",
        "\n",
        "\n",
        "- $y \\in \\mathbb{R}^{n} \\$  : vector de salida.\n",
        "\n",
        "\n",
        "- $\\beta \\in \\mathbb{R}^{p} \\$  : vector de coeficientes a estimar.\n",
        "\n",
        "\n",
        "- $ \\lambda \\geq 0 \\$  : parámetro de regularización total.\n",
        "\n",
        "\n",
        "- $ \\alpha \\in [0, 1] \\$  : coeficiente de mezcla entre L1 y L2.\n",
        "\n",
        "\n",
        "**Interpretación:**\n",
        "\n",
        "- Si $\\alpha=1$, Elastic Net es equivalente a **Lasso**.\n",
        "\n",
        "- Si $ \\alpha = 0 \\$, es equivalente a **Ridge**.\n",
        "\n",
        "- Si $ \\alpha \\in (0, 1) $, se obtiene una combinación convexa de ambos métodos.\n",
        "\n"
      ],
      "metadata": {
        "id": "qgwtdVsZKNO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 3_ElasticNet.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from scipy.stats import loguniform, uniform\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_contour\n",
        "\n",
        "st.set_page_config(page_title=\"Optimización de Hiperparámetros\", page_icon=\"⚙️\", layout=\"wide\")\n",
        "\n",
        "# ===============================\n",
        "# 1. Carga y Preparación de Datos\n",
        "# ===============================\n",
        "@st.cache_data\n",
        "def load_and_prepare_data():\n",
        "    with st.spinner('Descargando y preparando datos...'):\n",
        "        path = kagglehub.dataset_download(\"shashanknecrothapa/ames-housing-dataset\")\n",
        "        csv_file_path = os.path.join(path, \"AmesHousing.csv\")\n",
        "        Xdata = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Limpieza de datos\n",
        "        Xdata = Xdata.sample(frac=0.20, random_state=42)\n",
        "        cols_to_drop = ['Order', 'PID']\n",
        "        Xdata.drop(columns=[col for col in cols_to_drop if col in Xdata.columns], inplace=True)\n",
        "        high_null_cols = Xdata.columns[Xdata.isnull().mean() > 0.4].tolist()\n",
        "        Xdata.drop(columns=high_null_cols, inplace=True)\n",
        "\n",
        "        return Xdata\n",
        "\n",
        "Xdata = load_and_prepare_data()\n",
        "\n",
        "# ===============================\n",
        "# 2. Transformación de Variables\n",
        "# ===============================\n",
        "st.header(\"📐 Transformación de Variables\")\n",
        "\n",
        "col_sal = \"SalePrice\"\n",
        "\n",
        "# Selector para visualizar transformación\n",
        "transform_col = st.selectbox(\"Seleccione columna para visualizar transformación\",\n",
        "                            options=Xdata.select_dtypes(include=['int64', 'float64']).columns)\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(Xdata[transform_col], kde=True)\n",
        "    plt.title(f'Distribución Original de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "with col2:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(np.log1p(Xdata[transform_col]), kde=True)\n",
        "    plt.title(f'Distribución con log1p de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "st.info(\"\"\"\n",
        "💡 **Transformación logarítmica (log1p):**\n",
        "- Se aplica para manejar distribuciones sesgadas\n",
        "- log1p = log(1 + x) evita problemas con valores cero\n",
        "- Ayuda a cumplir supuestos de normalidad en modelos lineales\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 3. División de Datos\n",
        "# ===============================\n",
        "st.header(\"✂️ División del Dataset\")\n",
        "\n",
        "test_size = st.slider(\"Porcentaje para test\", 10, 40, 30, 5)\n",
        "\n",
        "Xtrain, Xtest = train_test_split(Xdata, test_size=test_size/100, random_state=42)\n",
        "ytrain = np.log1p(Xtrain[col_sal])\n",
        "ytest = np.log1p(Xtest[col_sal])\n",
        "Xtrain = Xtrain.drop(columns=col_sal)\n",
        "Xtest = Xtest.drop(columns=col_sal)\n",
        "\n",
        "st.success(f\"\"\"\n",
        "División completada:\n",
        "- Entrenamiento: {Xtrain.shape[0]} registros ({100-test_size}%)\n",
        "- Prueba: {Xtest.shape[0]} registros ({test_size}%)\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 4. Preprocesamiento\n",
        "# ===============================\n",
        "st.header(\"🔧 Pipeline de Preprocesamiento\")\n",
        "\n",
        "numeric_cols = Xtrain.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = Xtrain.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "with st.expander(\"Ver detalles de preprocesamiento\"):\n",
        "    st.write(\"**Columnas numéricas:**\")\n",
        "    st.write(numeric_cols)\n",
        "    st.write(\"**Transformaciones:** Imputación con mediana + Estandarización\")\n",
        "\n",
        "    st.write(\"\\n**Columnas categóricas:**\")\n",
        "    st.write(categorical_cols)\n",
        "    st.write(\"**Transformaciones:** Imputación con 'missing' + One-Hot Encoding\")\n",
        "\n",
        "# ===============================\n",
        "# 5. Optimización de Hiperparámetros\n",
        "# ===============================\n",
        "st.header(\"⚙️ Optimización de Hiperparámetros - ElasticNet\")\n",
        "\n",
        "# Configuración común\n",
        "cv = st.sidebar.slider(\"Número de folds para CV\", 3, 10, 3)\n",
        "random_state = st.sidebar.number_input(\"Random state\", 42)\n",
        "scoring = 'neg_mean_squared_error'\n",
        "\n",
        "# Configuración de parámetros\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.subheader(\"Grid Search\")\n",
        "    alpha_min = st.number_input(\"Alpha mínimo (log)\", -4, 2, -4)\n",
        "    alpha_max = st.number_input(\"Alpha máximo (log)\", -4, 2, 2)\n",
        "    alpha_points = st.slider(\"Puntos para alpha\", 5, 20, 10)\n",
        "\n",
        "with col2:\n",
        "    st.subheader(\"Random Search\")\n",
        "    n_iter = st.slider(\"Número de iteraciones\", 10, 100, 20)\n",
        "    bayesian_trials = st.slider(\"Número de trials Bayesianos\", 10, 100, 20)\n",
        "\n",
        "if st.button(\"Ejecutar Optimización\"):\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    # Preparar parámetros\n",
        "    param_grid = {\n",
        "        'regressor__alpha': np.logspace(alpha_min, alpha_max, alpha_points),\n",
        "        'regressor__l1_ratio': np.linspace(0, 1, 10)\n",
        "    }\n",
        "\n",
        "    param_dist = {\n",
        "        'regressor__alpha': loguniform(1e-4, 1e2),\n",
        "        'regressor__l1_ratio': uniform(0, 1)\n",
        "    }\n",
        "\n",
        "    # 1. Grid Search\n",
        "    status_text.text(\"Ejecutando Grid Search...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        Pipeline(steps=[('preprocessing', preprocessor), ('regressor', ElasticNet())]),\n",
        "        param_grid, scoring=scoring, cv=cv\n",
        "    )\n",
        "    grid_search.fit(Xtrain, ytrain)\n",
        "    grid_results = [\n",
        "        (params['regressor__alpha'], params['regressor__l1_ratio'], -score)\n",
        "        for params, score in zip(grid_search.cv_results_['params'],\n",
        "                               grid_search.cv_results_['mean_test_score'])\n",
        "    ]\n",
        "    progress_bar.progress(33)\n",
        "\n",
        "    # 2. Random Search\n",
        "    status_text.text(\"Ejecutando Random Search...\")\n",
        "    random_search = RandomizedSearchCV(\n",
        "        Pipeline(steps=[('preprocessing', preprocessor), ('regressor', ElasticNet())]),\n",
        "        param_dist, n_iter=n_iter, scoring=scoring, cv=cv, random_state=random_state\n",
        "    )\n",
        "    random_search.fit(Xtrain, ytrain)\n",
        "    random_results = [\n",
        "        (params['regressor__alpha'], params['regressor__l1_ratio'], -score)\n",
        "        for params, score in zip(random_search.cv_results_['params'],\n",
        "                               random_search.cv_results_['mean_test_score'])\n",
        "    ]\n",
        "    progress_bar.progress(66)\n",
        "\n",
        "    # 3. Bayesian Optimization\n",
        "    status_text.text(\"Ejecutando Bayesian Optimization...\")\n",
        "\n",
        "    def objective_elasticnet(trial):\n",
        "        alpha = trial.suggest_float('alpha', 1e-4, 1e2, log=True)\n",
        "        l1_ratio = trial.suggest_float('l1_ratio', 0, 1)\n",
        "        model = Pipeline(steps=[\n",
        "            ('preprocessing', preprocessor),\n",
        "            ('regressor', ElasticNet(alpha=alpha, l1_ratio=l1_ratio))\n",
        "        ])\n",
        "        try:\n",
        "            return -cross_val_score(model, Xtrain, ytrain, scoring=scoring, cv=cv).mean()\n",
        "        except:\n",
        "            return float('inf')\n",
        "\n",
        "    study_elasticnet = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
        "    study_elasticnet.optimize(objective_elasticnet, n_trials=bayesian_trials)\n",
        "    progress_bar.progress(100)\n",
        "\n",
        "    # Almacenar resultados\n",
        "    best_params = {\n",
        "        'GridSearch': grid_search.best_params_,\n",
        "        'RandomSearch': random_search.best_params_,\n",
        "        'Bayesian': study_elasticnet.best_params\n",
        "    }\n",
        "\n",
        "    # ===============================\n",
        "    # 6. Visualización de Resultados\n",
        "    # ===============================\n",
        "    st.success(\"Optimización completada!\")\n",
        "\n",
        "    # Mostrar mejores parámetros\n",
        "    st.subheader(\"🏆 Mejores Parámetros Encontrados\")\n",
        "    cols = st.columns(3)\n",
        "    with cols[0]:\n",
        "        st.metric(\"Grid Search - Alpha\", best_params['GridSearch']['regressor__alpha'])\n",
        "        st.metric(\"Grid Search - L1 Ratio\", best_params['GridSearch']['regressor__l1_ratio'])\n",
        "    with cols[1]:\n",
        "        st.metric(\"Random Search - Alpha\", best_params['RandomSearch']['regressor__alpha'])\n",
        "        st.metric(\"Random Search - L1 Ratio\", best_params['RandomSearch']['regressor__l1_ratio'])\n",
        "    with cols[2]:\n",
        "        st.metric(\"Bayesian - Alpha\", best_params['Bayesian']['alpha'])\n",
        "        st.metric(\"Bayesian - L1 Ratio\", best_params['Bayesian']['l1_ratio'])\n",
        "\n",
        "    # Gráficos de comparación\n",
        "    st.subheader(\"📊 Comparación de Métodos de Optimización\")\n",
        "\n",
        "    tab1, tab2, tab3 = st.tabs([\"Grid Search\", \"Random Search\", \"Bayesian Optimization\"])\n",
        "\n",
        "    with tab1:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        x_values = [r[0] for r in grid_results]\n",
        "        y_values = [r[1] for r in grid_results]\n",
        "        scores = [r[2] for r in grid_results]\n",
        "        scatter = ax.scatter(x_values, y_values, c=scores, cmap='viridis')\n",
        "        plt.colorbar(scatter, ax=ax, label='MSE')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_xlabel('alpha')\n",
        "        ax.set_ylabel('l1_ratio')\n",
        "        ax.set_title('Grid Search - ElasticNet')\n",
        "        ax.grid(True, which='both', ls='--')\n",
        "        st.pyplot(fig)\n",
        "        st.write(f\"Mejor MSE: {-grid_search.best_score_:.4f}\")\n",
        "\n",
        "    with tab2:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        x_values = [r[0] for r in random_results]\n",
        "        y_values = [r[1] for r in random_results]\n",
        "        scores = [r[2] for r in random_results]\n",
        "        scatter = ax.scatter(x_values, y_values, c=scores, cmap='viridis')\n",
        "        plt.colorbar(scatter, ax=ax, label='MSE')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_xlabel('alpha')\n",
        "        ax.set_ylabel('l1_ratio')\n",
        "        ax.set_title('Random Search - ElasticNet')\n",
        "        ax.grid(True, which='both', ls='--')\n",
        "        st.pyplot(fig)\n",
        "        st.write(f\"Mejor MSE: {-random_search.best_score_:.4f}\")\n",
        "\n",
        "    with tab3:\n",
        "        st.plotly_chart(plot_optimization_history(study_elasticnet))\n",
        "        st.plotly_chart(plot_param_importances(study_elasticnet))\n",
        "        st.plotly_chart(plot_contour(study_elasticnet, params=[\"alpha\", \"l1_ratio\"]))\n",
        "        st.write(f\"Mejor MSE: {study_elasticnet.best_value:.4f}\")\n",
        "\n",
        "    # Análisis comparativo\n",
        "    st.subheader(\"🔍 Análisis Comparativo Interactivo\")\n",
        "\n",
        "    # Crear un diccionario con todas las métricas disponibles\n",
        "    metrics_data = {\n",
        "        'MAE': {\n",
        "            'GridSearch': mean_absolute_error(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': mean_absolute_error(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': mean_absolute_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', ElasticNet(**study_elasticnet.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'MSE': {\n",
        "            'GridSearch': mean_squared_error(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': mean_squared_error(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': mean_squared_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', ElasticNet(**study_elasticnet.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'R2': {\n",
        "            'GridSearch': r2_score(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': r2_score(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': r2_score(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', ElasticNet(**study_elasticnet.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'MAPE': {\n",
        "            'GridSearch': mean_absolute_percentage_error(ytrain, grid_search.best_estimator_.predict(Xtrain)) * 100,\n",
        "            'RandomSearch': mean_absolute_percentage_error(ytrain, random_search.best_estimator_.predict(Xtrain)) * 100,\n",
        "            'Bayesian': mean_absolute_percentage_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', ElasticNet(**study_elasticnet.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain)) * 100\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Selector de métrica\n",
        "    selected_metric = st.selectbox(\n",
        "        \"Seleccione la métrica a visualizar:\",\n",
        "        options=['MAE', 'MSE', 'R2', 'MAPE'],\n",
        "        index=1  # MSE por defecto\n",
        "    )\n",
        "\n",
        "    # Configuración de visualización según la métrica\n",
        "    if selected_metric == 'MAPE':\n",
        "        ylabel = 'MAPE (%)'\n",
        "        title = f'Comparación de {selected_metric} entre Métodos'\n",
        "        fmt = '.2f%'\n",
        "        ascending = True\n",
        "    elif selected_metric == 'R2':\n",
        "        ylabel = 'R²'\n",
        "        title = f'Comparación de {selected_metric} entre Métodos'\n",
        "        fmt = '.4f'\n",
        "        ascending = False\n",
        "    else:\n",
        "        ylabel = selected_metric\n",
        "        title = f'Comparación de {selected_metric} entre Métodos'\n",
        "        fmt = '.4f'\n",
        "        ascending = True\n",
        "\n",
        "    # Ordenar métodos según el rendimiento\n",
        "    methods = pd.DataFrame({\n",
        "        'Method': ['GridSearch', 'RandomSearch', 'Bayesian'],\n",
        "        'Value': [metrics_data[selected_metric]['GridSearch'],\n",
        "                metrics_data[selected_metric]['RandomSearch'],\n",
        "                metrics_data[selected_metric]['Bayesian']]\n",
        "    }).sort_values('Value', ascending=ascending)\n",
        "\n",
        "    # Crear la figura\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    colors = ['skyblue', 'lightgreen', 'salmon']\n",
        "    bars = ax.bar(methods['Method'], methods['Value'], color=colors)\n",
        "\n",
        "    # Configurar el gráfico\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Añadir los valores a las barras\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        if selected_metric == 'MAPE':\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.2f}%',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "        else:\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:{fmt}}',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    # Rotar etiquetas del eje x para mejor legibilidad\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Mostrar el gráfico en Streamlit\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Mostrar tabla con todas las métricas\n",
        "    st.subheader(\"📊 Resumen de Métricas\")\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "    metrics_df['MAPE'] = metrics_df['MAPE'].map('{:.2f}%'.format)\n",
        "    metrics_df['MAE'] = metrics_df['MAE'].map('{:.4f}'.format)\n",
        "    metrics_df['MSE'] = metrics_df['MSE'].map('{:.4f}'.format)\n",
        "    metrics_df['R2'] = metrics_df['R2'].map('{:.4f}'.format)\n",
        "    st.dataframe(metrics_df.style.background_gradient(cmap='Blues', axis=0))\n",
        "\n",
        "    # Mostrar recomendación basada en la métrica seleccionada\n",
        "    best_method = methods.iloc[0]['Method']\n",
        "    best_value = methods.iloc[0]['Value']\n",
        "\n",
        "    if selected_metric == 'MAPE':\n",
        "        st.success(f\"✅ **Mejor modelo según {selected_metric}:** {best_method} ({best_value:.2f}%)\")\n",
        "    elif selected_metric == 'R2':\n",
        "        st.success(f\"✅ **Mejor modelo según {selected_metric}:** {best_method} ({best_value:.4f})\")\n",
        "    else:\n",
        "        st.success(f\"✅ **Mejor modelo según {selected_metric}:** {best_method} ({best_value:.4f})\")\n",
        "\n",
        "# ===============================\n",
        "# 7. Información Adicional\n",
        "# ===============================\n",
        "with st.expander(\"ℹ️ Instrucciones de Uso\", expanded=True):\n",
        "    st.write(\"\"\"\n",
        "    1. Configura los parámetros de búsqueda en el panel lateral\n",
        "    2. Haz clic en \"Ejecutar Optimización\"\n",
        "    3. Explora los resultados en las diferentes pestañas\n",
        "    4. Compara el rendimiento de los diferentes métodos\n",
        "\n",
        "    **Tipos de Búsqueda:**\n",
        "    - **Grid Search:** Búsqueda exhaustiva en una grilla definida\n",
        "    - **Random Search:** Muestreo aleatorio del espacio de parámetros\n",
        "    - **Bayesian Optimization:** Optimización inteligente basada en modelos\n",
        "    \"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 8. Evaluación del Modelo en Test\n",
        "# ===============================\n",
        "st.header(\"📊 Evaluación del Modelo ElasticNet en Datos de Test\")\n",
        "\n",
        "# Seleccionar el mejor modelo (usaremos el de Bayesian Optimization por defecto)\n",
        "best_params = study_elasticnet.best_params\n",
        "final_model = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('regressor', ElasticNet(**best_params))\n",
        "])\n",
        "final_model.fit(Xtrain, ytrain)\n",
        "\n",
        "# Predecir en test\n",
        "ypred = final_model.predict(Xtest)\n",
        "\n",
        "# Calcular métricas\n",
        "test_mae = mean_absolute_error(ytest, ypred)\n",
        "test_mse = mean_squared_error(ytest, ypred)\n",
        "test_r2 = r2_score(ytest, ypred)\n",
        "test_mape = mean_absolute_percentage_error(ytest, ypred) * 100\n",
        "\n",
        "# Mostrar métricas\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "with col1:\n",
        "    st.metric(\"MAE (Test)\", f\"{test_mae:.4f}\")\n",
        "with col2:\n",
        "    st.metric(\"MSE (Test)\", f\"{test_mse:.4f}\")\n",
        "with col3:\n",
        "    st.metric(\"R² (Test)\", f\"{test_r2:.4f}\")\n",
        "with col4:\n",
        "    st.metric(\"MAPE (Test)\", f\"{test_mape:.2f}%\")\n",
        "\n",
        "# Gráficos de evaluación\n",
        "st.subheader(\"🔍 Gráficos de Evaluación\")\n",
        "\n",
        "tab1, tab2, tab3, tab4 = st.tabs([\"Predicciones vs Reales\", \"Residuos\", \"Distribución de Errores\", \"Importancia de Variables\"])\n",
        "\n",
        "with tab1:\n",
        "    # Gráfico de predicciones vs valores reales\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(ytest, ypred, alpha=0.5)\n",
        "    ax.plot([ytest.min(), ytest.max()], [ytest.min(), ytest.max()], 'k--', lw=2)\n",
        "    ax.set_xlabel('Valores Reales (log1p)')\n",
        "    ax.set_ylabel('Predicciones (log1p)')\n",
        "    ax.set_title('Predicciones vs Valores Reales (ElasticNet)')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretación:**\n",
        "    - Los puntos deberían estar cerca de la línea diagonal\n",
        "    - Dispersión simétrica indica buen ajuste\n",
        "    - ElasticNet tiende a ser más conservador que KernelRidge en sus predicciones\n",
        "    \"\"\")\n",
        "\n",
        "with tab2:\n",
        "    # Gráfico de residuos\n",
        "    residuals = ytest - ypred\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(ypred, residuals, alpha=0.5)\n",
        "    ax.axhline(y=0, color='r', linestyle='--')\n",
        "    ax.set_xlabel('Predicciones (log1p)')\n",
        "    ax.set_ylabel('Residuos')\n",
        "    ax.set_title('Gráfico de Residuos (ElasticNet)')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretación:**\n",
        "    - Residuos aleatorios alrededor de cero son deseables\n",
        "    - ElasticNet suele mostrar residuos más homogéneos que otros modelos\n",
        "    - Patrones no aleatorios pueden indicar relaciones no capturadas\n",
        "    \"\"\")\n",
        "\n",
        "with tab3:\n",
        "    # Distribución de errores\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.histplot(residuals, kde=True, ax=ax)\n",
        "    ax.set_xlabel('Error de Predicción')\n",
        "    ax.set_title('Distribución de Errores (ElasticNet)')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretación:**\n",
        "    - Distribución centrada en cero indica predicciones no sesgadas\n",
        "    - La regularización L1+L2 de ElasticNet suele producir distribuciones más compactas\n",
        "    - Colas pesadas pueden indicar valores atípicos problemáticos\n",
        "    \"\"\")\n",
        "\n",
        "with tab4:\n",
        "    # Importancia de variables (solo para ElasticNet)\n",
        "    try:\n",
        "        # Obtener los coeficientes del modelo\n",
        "        feature_names = numeric_cols + list(final_model.named_steps['preprocessing'].named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(categorical_cols))\n",
        "        coefficients = final_model.named_steps['regressor'].coef_\n",
        "\n",
        "        # Crear DataFrame con coeficientes\n",
        "        coef_df = pd.DataFrame({\n",
        "            'Variable': feature_names,\n",
        "            'Coeficiente': coefficients\n",
        "        }).sort_values('Coeficiente', key=abs, ascending=False).head(20)\n",
        "\n",
        "        # Gráfico de importancia\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        sns.barplot(data=coef_df, x='Coeficiente', y='Variable', ax=ax)\n",
        "        ax.set_title('Top 20 Variables Más Importantes (ElasticNet)')\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        st.write(\"\"\"\n",
        "        **Interpretación:**\n",
        "        - Muestra las variables con mayor impacto en las predicciones\n",
        "        - ElasticNet realiza selección de variables (algunos coeficientes son exactamente cero)\n",
        "        - Coeficientes positivos aumentan el precio predicho, negativos lo disminuyen\n",
        "        \"\"\")\n",
        "    except Exception as e:\n",
        "        st.warning(f\"No se pudo generar el gráfico de importancia de variables: {str(e)}\")\n",
        "\n",
        "# Gráfico de valores reales vs predichos en escala original\n",
        "st.subheader(\"💰 Predicciones en Escala Original (USD)\")\n",
        "\n",
        "# Convertir a escala original\n",
        "ytest_orig = np.expm1(ytest)\n",
        "ypred_orig = np.expm1(ypred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.scatter(ytest_orig, ypred_orig, alpha=0.5)\n",
        "ax.plot([ytest_orig.min(), ytest_orig.max()], [ytest_orig.min(), ytest_orig.max()], 'k--', lw=2)\n",
        "ax.set_xlabel('Valores Reales (USD)')\n",
        "ax.set_ylabel('Predicciones (USD)')\n",
        "ax.set_title('Predicciones vs Valores Reales - Escala Original (ElasticNet)')\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Calcular métricas en escala original\n",
        "mae_orig = mean_absolute_error(ytest_orig, ypred_orig)\n",
        "mape_orig = mean_absolute_percentage_error(ytest_orig, ypred_orig) * 100\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.metric(\"MAE (USD)\", f\"${mae_orig:,.2f}\")\n",
        "with col2:\n",
        "    st.metric(\"MAPE (USD)\", f\"{mape_orig:.2f}%\")\n",
        "\n",
        "# Análisis de errores por rango de precio\n",
        "st.subheader(\"📈 Análisis de Errores por Rango de Precio\")\n",
        "\n",
        "# Crear categorías de precios\n",
        "price_bins = pd.qcut(ytest_orig, q=5, duplicates='drop')\n",
        "error_analysis = pd.DataFrame({\n",
        "    'Precio Real': ytest_orig,\n",
        "    'Error Absoluto': np.abs(ytest_orig - ypred_orig),\n",
        "    'Rango Precio': price_bins\n",
        "})\n",
        "\n",
        "# Calcular métricas por rango\n",
        "error_by_price = error_analysis.groupby('Rango Precio').agg({\n",
        "    'Precio Real': 'mean',\n",
        "    'Error Absoluto': ['mean', 'median', 'std']\n",
        "}).reset_index()\n",
        "\n",
        "error_by_price.columns = ['Rango Precio', 'Precio Promedio', 'MAE', 'Error Mediano', 'Desviación Error']\n",
        "\n",
        "# Gráfico de MAE por rango de precio\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.barplot(data=error_by_price, x='Rango Precio', y='MAE', ax=ax)\n",
        "ax.set_title('Error Absoluto Medio por Rango de Precio (ElasticNet)')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "ax.set_ylabel('MAE (USD)')\n",
        "ax.set_xlabel('Rango de Precio')\n",
        "st.pyplot(fig)\n",
        "\n",
        "st.write(\"\"\"\n",
        "**Interpretación:**\n",
        "- ElasticNet suele tener un rendimiento más consistente en diferentes rangos de precio\n",
        "- Los errores pueden aumentar en los extremos (propiedades muy baratas o muy caras)\n",
        "- Puede sugerir la necesidad de ajustar los parámetros de regularización para ciertos rangos\n",
        "\"\"\")\n",
        "\n",
        "# Mostrar tabla de análisis\n",
        "st.dataframe(error_by_price.style.background_gradient(subset=['MAE'], cmap='Reds'))\n",
        "\n",
        "# Comparación con modelo naive (promedio)\n",
        "st.subheader(\"🤔 Comparación con Modelo Naive (Promedio)\")\n",
        "\n",
        "naive_pred = np.full_like(ytest, ytrain.mean())\n",
        "naive_mae = mean_absolute_error(ytest, naive_pred)\n",
        "improvement = (naive_mae - test_mae) / naive_mae * 100\n",
        "\n",
        "st.metric(\"Mejora sobre modelo naive\", f\"{improvement:.2f}%\",\n",
        "          delta=f\"MAE naive: {naive_mae:.4f}, MAE ElasticNet: {test_mae:.4f}\")\n",
        "\n",
        "st.write(\"\"\"\n",
        "**Interpretación:**\n",
        "- Muestra cuánto mejor es el modelo ElasticNet respecto a simplemente predecir el promedio\n",
        "- Una mejora positiva indica que el modelo está aprendiendo patrones útiles\n",
        "- ElasticNet suele superar significativamente al modelo naive en problemas de precios de viviendas\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJixjHUWu_iz",
        "outputId": "486aa9e9-d38b-4b67-a41e-ac912056502a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 3_ElasticNet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 3_ElasticNet.py pages/"
      ],
      "metadata": {
        "id": "JPqUBrakjg9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. 📘 Kernel Ridge Regression: Función de optimización**\n",
        "\n",
        "El modelo **Kernel Ridge Regression** extiende Ridge Regression utilizando **funciones núcleo** (kernels) para permitir la regresión en espacios de alta dimensión de características implícitas.\n",
        "\n",
        "Minimiza la siguiente función objetivo:\n",
        "\n",
        "$$\n",
        "\\min_{\\alpha} \\; \\| y - K \\alpha \\|_2^2 + \\lambda \\alpha^\\top K \\alpha\n",
        "$$\n",
        "\n",
        "**Donde:**\n",
        "\n",
        "- \\$ y \\in \\mathbb{R}^n \\$: vector de salidas (valores observados).\n",
        "- \\$K \\in \\mathbb{R}^{n \\times n} \\$: matriz de kernel, donde \\$K_{ij} = k(x_i, x_j) \\$\n",
        "- \\$ \\alpha \\in \\mathbb{R}^n \\$: coeficientes duales a estimar.\n",
        "- \\$ \\lambda > 0 \\$: parámetro de regularización.\n",
        "\n",
        "---\n",
        "\n",
        "### 💡 Formulación dual\n",
        "\n",
        "El vector solución se obtiene como:\n",
        "\n",
        "$$\n",
        "\\hat{\\alpha} = (K + \\lambda I)^{-1} y\n",
        "$$\n",
        "\n",
        "y la predicción para un nuevo punto \\( x \\) es:\n",
        "\n",
        "$$\n",
        "\\hat{y}(x) = \\sum_{i=1}^n \\alpha_i \\, k(x_i, x)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Interpretación\n",
        "\n",
        "- Si \\$ k(x_i, x_j) = x_i^\\top x_j \\$, el modelo se reduce a Ridge Regression clásico.\n",
        "- Permite ajustar relaciones no lineales mediante kernels como:\n",
        "  - **Lineal**: \\$ k(x, x') = x^\\top x' \\$\n",
        "  - **Polinomial**: \\$ k(x, x') = (x^\\top x' + c)^d \\$\n",
        "  - **RBF (Gaussiano)**: \\$ k(x, x') = \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\sigma^2}\\right) \\$\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Ventajas\n",
        "\n",
        "- Combina la robustez de Ridge con el poder representativo de los kernels.\n",
        "- No necesita transformar explícitamente los datos al espacio de características.\n"
      ],
      "metadata": {
        "id": "LMptc2nRKW-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 4_KernelRidge.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from scipy.stats import loguniform, uniform\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_contour\n",
        "\n",
        "st.set_page_config(page_title=\"Optimización de Hiperparámetros\", page_icon=\"⚙️\", layout=\"wide\")\n",
        "\n",
        "# ===============================\n",
        "# 1. Carga y Preparación de Datos\n",
        "# ===============================\n",
        "@st.cache_data\n",
        "def load_and_prepare_data():\n",
        "    with st.spinner('Descargando y preparando datos...'):\n",
        "        path = kagglehub.dataset_download(\"shashanknecrothapa/ames-housing-dataset\")\n",
        "        csv_file_path = os.path.join(path, \"AmesHousing.csv\")\n",
        "        Xdata = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Limpieza de datos\n",
        "        Xdata = Xdata.sample(frac=0.20, random_state=42)\n",
        "        cols_to_drop = ['Order', 'PID']\n",
        "        Xdata.drop(columns=[col for col in cols_to_drop if col in Xdata.columns], inplace=True)\n",
        "        high_null_cols = Xdata.columns[Xdata.isnull().mean() > 0.4].tolist()\n",
        "        Xdata.drop(columns=high_null_cols, inplace=True)\n",
        "\n",
        "        return Xdata\n",
        "\n",
        "Xdata = load_and_prepare_data()\n",
        "\n",
        "# ===============================\n",
        "# 2. Transformación de Variables\n",
        "# ===============================\n",
        "st.header(\"📐 Transformación de Variables\")\n",
        "\n",
        "col_sal = \"SalePrice\"\n",
        "\n",
        "# Selector para visualizar transformación\n",
        "transform_col = st.selectbox(\"Seleccione columna para visualizar transformación\",\n",
        "                            options=Xdata.select_dtypes(include=['int64', 'float64']).columns)\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(Xdata[transform_col], kde=True)\n",
        "    plt.title(f'Distribución Original de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "with col2:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(np.log1p(Xdata[transform_col]), kde=True)\n",
        "    plt.title(f'Distribución con log1p de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "st.info(\"\"\"\n",
        "💡 **Transformación logarítmica (log1p):**\n",
        "- Se aplica para manejar distribuciones sesgadas\n",
        "- log1p = log(1 + x) evita problemas con valores cero\n",
        "- Ayuda a cumplir supuestos de normalidad en modelos lineales\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 3. División de Datos\n",
        "# ===============================\n",
        "st.header(\"✂️ División del Dataset\")\n",
        "\n",
        "test_size = st.slider(\"Porcentaje para test\", 10, 40, 30, 5)\n",
        "\n",
        "Xtrain, Xtest = train_test_split(Xdata, test_size=test_size/100, random_state=42)\n",
        "ytrain = np.log1p(Xtrain[col_sal])\n",
        "ytest = np.log1p(Xtest[col_sal])\n",
        "Xtrain = Xtrain.drop(columns=col_sal)\n",
        "Xtest = Xtest.drop(columns=col_sal)\n",
        "\n",
        "st.success(f\"\"\"\n",
        "División completada:\n",
        "- Entrenamiento: {Xtrain.shape[0]} registros ({100-test_size}%)\n",
        "- Prueba: {Xtest.shape[0]} registros ({test_size}%)\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 4. Preprocesamiento\n",
        "# ===============================\n",
        "st.header(\"🔧 Pipeline de Preprocesamiento\")\n",
        "\n",
        "numeric_cols = Xtrain.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = Xtrain.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "with st.expander(\"Ver detalles de preprocesamiento\"):\n",
        "    st.write(\"**Columnas numéricas:**\")\n",
        "    st.write(numeric_cols)\n",
        "    st.write(\"**Transformaciones:** Imputación con mediana + Estandarización\")\n",
        "\n",
        "    st.write(\"\\n**Columnas categóricas:**\")\n",
        "    st.write(categorical_cols)\n",
        "    st.write(\"**Transformaciones:** Imputación con 'missing' + One-Hot Encoding\")\n",
        "\n",
        "# ===============================\n",
        "# 5. Optimización de Hiperparámetros (KernelRidge)\n",
        "# ===============================\n",
        "st.header(\"⚙️ Optimización de Hiperparámetros - KernelRidge\")\n",
        "\n",
        "# Configuración común\n",
        "cv = st.sidebar.slider(\"Número de folds para CV\", 3, 10, 3)\n",
        "random_state = st.sidebar.number_input(\"Random state\", 42)\n",
        "scoring = 'neg_mean_squared_error'\n",
        "\n",
        "# Configuración de parámetros\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.subheader(\"Grid Search\")\n",
        "    alpha_min = st.number_input(\"Alpha mínimo (log)\", -4, 2, -4)\n",
        "    alpha_max = st.number_input(\"Alpha máximo (log)\", -4, 2, 2)\n",
        "    alpha_points = st.slider(\"Puntos para alpha\", 5, 20, 10)\n",
        "    gamma_min = st.number_input(\"Gamma mínimo (log)\", -4, 2, -4)\n",
        "    gamma_max = st.number_input(\"Gamma máximo (log)\", -4, 2, 2)\n",
        "    gamma_points = st.slider(\"Puntos para gamma\", 5, 20, 10)\n",
        "\n",
        "with col2:\n",
        "    st.subheader(\"Random Search\")\n",
        "    n_iter = st.slider(\"Número de iteraciones\", 10, 100, 20)\n",
        "    bayesian_trials = st.slider(\"Número de trials Bayesianos\", 10, 100, 20)\n",
        "\n",
        "if st.button(\"Ejecutar Optimización\"):\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    # Preparar parámetros\n",
        "    param_grid = {\n",
        "        'regressor__alpha': np.logspace(alpha_min, alpha_max, alpha_points),\n",
        "        'regressor__gamma': np.logspace(gamma_min, gamma_max, gamma_points)\n",
        "    }\n",
        "\n",
        "    param_dist = {\n",
        "        'regressor__alpha': loguniform(1e-4, 1e2),\n",
        "        'regressor__gamma': loguniform(1e-4, 1e2)\n",
        "    }\n",
        "\n",
        "    # 1. Grid Search\n",
        "    status_text.text(\"Ejecutando Grid Search...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        Pipeline(steps=[('preprocessing', preprocessor), ('regressor', KernelRidge())]),\n",
        "        param_grid, scoring=scoring, cv=cv\n",
        "    )\n",
        "    grid_search.fit(Xtrain, ytrain)\n",
        "    grid_results = [\n",
        "        (params['regressor__alpha'], params['regressor__gamma'], -score)\n",
        "        for params, score in zip(grid_search.cv_results_['params'],\n",
        "                               grid_search.cv_results_['mean_test_score'])\n",
        "    ]\n",
        "    progress_bar.progress(33)\n",
        "\n",
        "    # 2. Random Search\n",
        "    status_text.text(\"Ejecutando Random Search...\")\n",
        "    random_search = RandomizedSearchCV(\n",
        "        Pipeline(steps=[('preprocessing', preprocessor), ('regressor', KernelRidge())]),\n",
        "        param_dist, n_iter=n_iter, scoring=scoring, cv=cv, random_state=random_state\n",
        "    )\n",
        "    random_search.fit(Xtrain, ytrain)\n",
        "    random_results = [\n",
        "        (params['regressor__alpha'], params['regressor__gamma'], -score)\n",
        "        for params, score in zip(random_search.cv_results_['params'],\n",
        "                               random_search.cv_results_['mean_test_score'])\n",
        "    ]\n",
        "    progress_bar.progress(66)\n",
        "\n",
        "    # 3. Bayesian Optimization\n",
        "    status_text.text(\"Ejecutando Bayesian Optimization...\")\n",
        "\n",
        "    def objective_kernelridge(trial):\n",
        "        alpha = trial.suggest_float('alpha', 1e-4, 1e2, log=True)\n",
        "        gamma = trial.suggest_float('gamma', 1e-4, 1e2, log=True)\n",
        "        model = Pipeline(steps=[\n",
        "            ('preprocessing', preprocessor),\n",
        "            ('regressor', KernelRidge(alpha=alpha, gamma=gamma))\n",
        "        ])\n",
        "        try:\n",
        "            return -cross_val_score(model, Xtrain, ytrain, scoring=scoring, cv=cv).mean()\n",
        "        except:\n",
        "            return float('inf')\n",
        "\n",
        "    study_kernelridge = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
        "    study_kernelridge.optimize(objective_kernelridge, n_trials=bayesian_trials)\n",
        "    progress_bar.progress(100)\n",
        "\n",
        "    # Almacenar resultados\n",
        "    best_params = {\n",
        "        'GridSearch': grid_search.best_params_,\n",
        "        'RandomSearch': random_search.best_params_,\n",
        "        'Bayesian': study_kernelridge.best_params\n",
        "    }\n",
        "\n",
        "    # ===============================\n",
        "    # 6. Visualización de Resultados\n",
        "    # ===============================\n",
        "    st.success(\"Optimización completada!\")\n",
        "\n",
        "    # Mostrar mejores parámetros\n",
        "    st.subheader(\"🏆 Mejores Parámetros Encontrados\")\n",
        "    cols = st.columns(3)\n",
        "    with cols[0]:\n",
        "        st.metric(\"Grid Search - Alpha\", best_params['GridSearch']['regressor__alpha'])\n",
        "        st.metric(\"Grid Search - Gamma\", best_params['GridSearch']['regressor__gamma'])\n",
        "    with cols[1]:\n",
        "        st.metric(\"Random Search - Alpha\", best_params['RandomSearch']['regressor__alpha'])\n",
        "        st.metric(\"Random Search - Gamma\", best_params['RandomSearch']['regressor__gamma'])\n",
        "    with cols[2]:\n",
        "        st.metric(\"Bayesian - Alpha\", best_params['Bayesian']['alpha'])\n",
        "        st.metric(\"Bayesian - Gamma\", best_params['Bayesian']['gamma'])\n",
        "\n",
        "    # Gráficos de comparación\n",
        "    st.subheader(\"📊 Comparación de Métodos de Optimización\")\n",
        "\n",
        "    tab1, tab2, tab3 = st.tabs([\"Grid Search\", \"Random Search\", \"Bayesian Optimization\"])\n",
        "\n",
        "    with tab1:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        x_values = [r[0] for r in grid_results]\n",
        "        y_values = [r[1] for r in grid_results]\n",
        "        scores = [r[2] for r in grid_results]\n",
        "        scatter = ax.scatter(x_values, y_values, c=scores, cmap='viridis')\n",
        "        plt.colorbar(scatter, ax=ax, label='MSE')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_xlabel('alpha')\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_ylabel('gamma')\n",
        "        ax.set_title('Grid Search - KernelRidge')\n",
        "        ax.grid(True, which='both', ls='--')\n",
        "        st.pyplot(fig)\n",
        "        st.write(f\"Mejor MSE: {-grid_search.best_score_:.4f}\")\n",
        "\n",
        "    with tab2:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        x_values = [r[0] for r in random_results]\n",
        "        y_values = [r[1] for r in random_results]\n",
        "        scores = [r[2] for r in random_results]\n",
        "        scatter = ax.scatter(x_values, y_values, c=scores, cmap='viridis')\n",
        "        plt.colorbar(scatter, ax=ax, label='MSE')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_xlabel('alpha')\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_ylabel('gamma')\n",
        "        ax.set_title('Random Search - KernelRidge')\n",
        "        ax.grid(True, which='both', ls='--')\n",
        "        st.pyplot(fig)\n",
        "        st.write(f\"Mejor MSE: {-random_search.best_score_:.4f}\")\n",
        "\n",
        "    with tab3:\n",
        "        st.plotly_chart(plot_optimization_history(study_kernelridge))\n",
        "        st.plotly_chart(plot_param_importances(study_kernelridge))\n",
        "        st.plotly_chart(plot_contour(study_kernelridge, params=[\"alpha\", \"gamma\"]))\n",
        "        st.write(f\"Mejor MSE: {study_kernelridge.best_value:.4f}\")\n",
        "\n",
        "    # Análisis comparativo interactivo\n",
        "    st.subheader(\"🔍 Análisis Comparativo Interactivo\")\n",
        "\n",
        "    # Crear un diccionario con todas las métricas disponibles\n",
        "    metrics_data = {\n",
        "        'MAE': {\n",
        "            'GridSearch': mean_absolute_error(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': mean_absolute_error(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': mean_absolute_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', KernelRidge(**study_kernelridge.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'MSE': {\n",
        "            'GridSearch': mean_squared_error(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': mean_squared_error(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': mean_squared_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', KernelRidge(**study_kernelridge.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'R2': {\n",
        "            'GridSearch': r2_score(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': r2_score(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': r2_score(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', KernelRidge(**study_kernelridge.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'MAPE': {\n",
        "            'GridSearch': mean_absolute_percentage_error(ytrain, grid_search.best_estimator_.predict(Xtrain)) * 100,\n",
        "            'RandomSearch': mean_absolute_percentage_error(ytrain, random_search.best_estimator_.predict(Xtrain)) * 100,\n",
        "            'Bayesian': mean_absolute_percentage_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', KernelRidge(**study_kernelridge.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain)) * 100\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Selector de métrica\n",
        "    selected_metric = st.selectbox(\n",
        "        \"Seleccione la métrica a visualizar:\",\n",
        "        options=['MAE', 'MSE', 'R2', 'MAPE'],\n",
        "        index=1  # MSE por defecto\n",
        "    )\n",
        "\n",
        "    # Configuración de visualización según la métrica\n",
        "    if selected_metric == 'MAPE':\n",
        "        ylabel = 'MAPE (%)'\n",
        "        title = f'Comparación de {selected_metric} entre Métodos'\n",
        "        fmt = '.2f%'\n",
        "        ascending = True\n",
        "    elif selected_metric == 'R2':\n",
        "        ylabel = 'R²'\n",
        "        title = f'Comparación de {selected_metric} entre Métodos'\n",
        "        fmt = '.4f'\n",
        "        ascending = False\n",
        "    else:\n",
        "        ylabel = selected_metric\n",
        "        title = f'Comparación de {selected_metric} entre Métodos'\n",
        "        fmt = '.4f'\n",
        "        ascending = True\n",
        "\n",
        "    # Ordenar métodos según el rendimiento\n",
        "    methods = pd.DataFrame({\n",
        "        'Method': ['GridSearch', 'RandomSearch', 'Bayesian'],\n",
        "        'Value': [metrics_data[selected_metric]['GridSearch'],\n",
        "                metrics_data[selected_metric]['RandomSearch'],\n",
        "                metrics_data[selected_metric]['Bayesian']]\n",
        "    }).sort_values('Value', ascending=ascending)\n",
        "\n",
        "    # Crear la figura\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    colors = ['skyblue', 'lightgreen', 'salmon']\n",
        "    bars = ax.bar(methods['Method'], methods['Value'], color=colors)\n",
        "\n",
        "    # Configurar el gráfico\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Añadir los valores a las barras\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        if selected_metric == 'MAPE':\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.2f}%',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "        else:\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:{fmt}}',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    # Rotar etiquetas del eje x para mejor legibilidad\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Mostrar el gráfico en Streamlit\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Mostrar tabla con todas las métricas\n",
        "    st.subheader(\"📊 Resumen de Métricas\")\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "    metrics_df['MAPE'] = metrics_df['MAPE'].map('{:.2f}%'.format)\n",
        "    metrics_df['MAE'] = metrics_df['MAE'].map('{:.4f}'.format)\n",
        "    metrics_df['MSE'] = metrics_df['MSE'].map('{:.4f}'.format)\n",
        "    metrics_df['R2'] = metrics_df['R2'].map('{:.4f}'.format)\n",
        "    st.dataframe(metrics_df.style.background_gradient(cmap='Blues', axis=0))\n",
        "\n",
        "    # Mostrar recomendación basada en la métrica seleccionada\n",
        "    best_method = methods.iloc[0]['Method']\n",
        "    best_value = methods.iloc[0]['Value']\n",
        "\n",
        "    if selected_metric == 'MAPE':\n",
        "        st.success(f\"✅ **Mejor modelo según {selected_metric}:** {best_method} ({best_value:.2f}%)\")\n",
        "    elif selected_metric == 'R2':\n",
        "        st.success(f\"✅ **Mejor modelo según {selected_metric}:** {best_method} ({best_value:.4f})\")\n",
        "    else:\n",
        "        st.success(f\"✅ **Mejor modelo según {selected_metric}:** {best_method} ({best_value:.4f})\")\n",
        "\n",
        "# ===============================\n",
        "# 7. Información Adicional\n",
        "# ===============================\n",
        "with st.expander(\"ℹ️ Instrucciones de Uso\", expanded=True):\n",
        "    st.write(\"\"\"\n",
        "    1. Configura los parámetros de búsqueda en el panel lateral\n",
        "    2. Haz clic en \"Ejecutar Optimización\"\n",
        "    3. Explora los resultados en las diferentes pestañas\n",
        "    4. Compara el rendimiento de los diferentes métodos\n",
        "\n",
        "    **Tipos de Búsqueda:**\n",
        "    - **Grid Search:** Búsqueda exhaustiva en una grilla definida\n",
        "    - **Random Search:** Muestreo aleatorio del espacio de parámetros\n",
        "    - **Bayesian Optimization:** Optimización inteligente basada en modelos\n",
        "\n",
        "    **Hiperparámetros de KernelRidge:**\n",
        "    - **alpha:** Parámetro de regularización (controla la penalización)\n",
        "    - **gamma:** Parámetro del kernel (controla el alcance de influencia)\n",
        "    \"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 8. Evaluación del Modelo en Test\n",
        "# ===============================\n",
        "st.header(\"📊 Evaluación del Modelo en Datos de Test\")\n",
        "\n",
        "# Seleccionar el mejor modelo (usaremos el de Bayesian Optimization por defecto)\n",
        "best_params = study_kernelridge.best_params\n",
        "final_model = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('regressor', KernelRidge(**best_params))\n",
        "])\n",
        "final_model.fit(Xtrain, ytrain)\n",
        "\n",
        "# Predecir en test\n",
        "ypred = final_model.predict(Xtest)\n",
        "\n",
        "# Calcular métricas\n",
        "test_mae = mean_absolute_error(ytest, ypred)\n",
        "test_mse = mean_squared_error(ytest, ypred)\n",
        "test_r2 = r2_score(ytest, ypred)\n",
        "test_mape = mean_absolute_percentage_error(ytest, ypred) * 100\n",
        "\n",
        "# Mostrar métricas\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "with col1:\n",
        "    st.metric(\"MAE (Test)\", f\"{test_mae:.4f}\")\n",
        "with col2:\n",
        "    st.metric(\"MSE (Test)\", f\"{test_mse:.4f}\")\n",
        "with col3:\n",
        "    st.metric(\"R² (Test)\", f\"{test_r2:.4f}\")\n",
        "with col4:\n",
        "    st.metric(\"MAPE (Test)\", f\"{test_mape:.2f}%\")\n",
        "\n",
        "# Gráficos de evaluación\n",
        "st.subheader(\"🔍 Gráficos de Evaluación\")\n",
        "\n",
        "tab1, tab2, tab3 = st.tabs([\"Predicciones vs Reales\", \"Residuos\", \"Distribución de Errores\"])\n",
        "\n",
        "with tab1:\n",
        "    # Gráfico de predicciones vs valores reales\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(ytest, ypred, alpha=0.5)\n",
        "    ax.plot([ytest.min(), ytest.max()], [ytest.min(), ytest.max()], 'k--', lw=2)\n",
        "    ax.set_xlabel('Valores Reales (log1p)')\n",
        "    ax.set_ylabel('Predicciones (log1p)')\n",
        "    ax.set_title('Predicciones vs Valores Reales')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretación:**\n",
        "    - Los puntos deberían estar cerca de la línea diagonal\n",
        "    - Dispersión simétrica indica buen ajuste\n",
        "    - Patrones no aleatorios pueden indicar problemas en el modelo\n",
        "    \"\"\")\n",
        "\n",
        "with tab2:\n",
        "    # Gráfico de residuos\n",
        "    residuals = ytest - ypred\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(ypred, residuals, alpha=0.5)\n",
        "    ax.axhline(y=0, color='r', linestyle='--')\n",
        "    ax.set_xlabel('Predicciones (log1p)')\n",
        "    ax.set_ylabel('Residuos')\n",
        "    ax.set_title('Gráfico de Residuos')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretación:**\n",
        "    - Los residuos deberían distribuirse aleatoriamente alrededor del cero\n",
        "    - Patrones visibles indican que el modelo no captura alguna relación en los datos\n",
        "    - Residuos heterocedásticos (que cambian de varianza) son problemáticos\n",
        "    \"\"\")\n",
        "\n",
        "with tab3:\n",
        "    # Distribución de errores\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.histplot(residuals, kde=True, ax=ax)\n",
        "    ax.set_xlabel('Error de Predicción')\n",
        "    ax.set_title('Distribución de Errores')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretación:**\n",
        "    - Distribución centrada en cero indica predicciones no sesgadas\n",
        "    - Forma aproximadamente normal es deseable\n",
        "    - Colas pesadas pueden indicar valores atípicos problemáticos\n",
        "    \"\"\")\n",
        "\n",
        "# Gráfico de valores reales vs predichos en escala original\n",
        "st.subheader(\"🔍 Predicciones en Escala Original\")\n",
        "\n",
        "# Convertir a escala original\n",
        "ytest_orig = np.expm1(ytest)\n",
        "ypred_orig = np.expm1(ypred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.scatter(ytest_orig, ypred_orig, alpha=0.5)\n",
        "ax.plot([ytest_orig.min(), ytest_orig.max()], [ytest_orig.min(), ytest_orig.max()], 'k--', lw=2)\n",
        "ax.set_xlabel('Valores Reales (USD)')\n",
        "ax.set_ylabel('Predicciones (USD)')\n",
        "ax.set_title('Predicciones vs Valores Reales (Escala Original)')\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Calcular métricas en escala original\n",
        "mae_orig = mean_absolute_error(ytest_orig, ypred_orig)\n",
        "mape_orig = mean_absolute_percentage_error(ytest_orig, ypred_orig) * 100\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.metric(\"MAE (USD)\", f\"${mae_orig:,.2f}\")\n",
        "with col2:\n",
        "    st.metric(\"MAPE (USD)\", f\"{mape_orig:.2f}%\")\n",
        "\n",
        "# Análisis de errores por rango de precio\n",
        "st.subheader(\"📈 Análisis de Errores por Rango de Precio\")\n",
        "\n",
        "# Crear categorías de precios\n",
        "price_bins = pd.qcut(ytest_orig, q=5, duplicates='drop')\n",
        "error_analysis = pd.DataFrame({\n",
        "    'Precio Real': ytest_orig,\n",
        "    'Error Absoluto': np.abs(ytest_orig - ypred_orig),\n",
        "    'Rango Precio': price_bins\n",
        "})\n",
        "\n",
        "# Calcular métricas por rango\n",
        "error_by_price = error_analysis.groupby('Rango Precio').agg({\n",
        "    'Precio Real': 'mean',\n",
        "    'Error Absoluto': ['mean', 'median', 'std']\n",
        "}).reset_index()\n",
        "\n",
        "error_by_price.columns = ['Rango Precio', 'Precio Promedio', 'MAE', 'Error Mediano', 'Desviación Error']\n",
        "\n",
        "# Gráfico de MAE por rango de precio\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.barplot(data=error_by_price, x='Rango Precio', y='MAE', ax=ax)\n",
        "ax.set_title('Error Absoluto Medio por Rango de Precio')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "ax.set_ylabel('MAE (USD)')\n",
        "ax.set_xlabel('Rango de Precio')\n",
        "st.pyplot(fig)\n",
        "\n",
        "st.write(\"\"\"\n",
        "**Interpretación:**\n",
        "- Identifica en qué rangos de precio el modelo tiene mayor error\n",
        "- Errores consistentes pueden indicar problemas con ciertos tipos de propiedades\n",
        "- Puede sugerir la necesidad de modelos segmentados o más datos en ciertos rangos\n",
        "\"\"\")\n",
        "\n",
        "# Mostrar tabla de análisis\n",
        "st.dataframe(error_by_price.style.background_gradient(subset=['MAE'], cmap='Reds'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7Gr77iqQJ-D",
        "outputId": "c383c1f6-0d54-469b-f2a3-63a1e36ec9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 4_KernelRidge.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 4_KernelRidge.py pages/"
      ],
      "metadata": {
        "id": "ByToVW4qQl_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. ⚙️ SGDRegressor: Función de optimización**\n",
        "\n",
        "El modelo **SGDRegressor** estima los coeficientes \\( \\beta \\in \\mathbb{R}^p \\) al minimizar una función de pérdida sobre los datos utilizando **descenso de gradiente estocástico** (SGD). La forma general de la función objetivo es:\n",
        "\n",
        "$$\n",
        "\\min_{\\beta} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\mathcal{L}(y_i, \\beta^\\top x_i) + \\lambda R(\\beta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 🧾 Componentes:\n",
        "\n",
        "- \\$\\mathcal{L}(y_i, \\hat{y}_i) \\$: función de pérdida (por defecto: **cuadrática**)\n",
        "  \n",
        "  $$\n",
        "  \\mathcal{L}(y_i, \\hat{y}_i) = \\frac{1}{2}(y_i - \\hat{y}_i)^2\n",
        "  $$\n",
        "\n",
        "- \\$ R(\\beta) \\$: término de regularización (puede ser L1, L2 o ambos)\n",
        "  \n",
        "  - **L2 (Ridge):** \\$ R(\\beta) = \\frac{1}{2} \\|\\beta\\|_2^2 \\$\n",
        "  - **L1 (Lasso):** \\$ R(\\beta) = \\|\\beta\\|_1 \\$\n",
        "  - **ElasticNet:** \\$ R(\\beta) = \\alpha \\|\\beta\\|_1 + \\frac{1 - \\alpha}{2} \\|\\beta\\|_2^2 \\$\n",
        "\n",
        "- \\$ \\lambda \\$: parámetro de penalización en `SGDRegressor`, \\$\\alpha=\\lambda$.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 Iteración de SGD\n",
        "\n",
        "En cada iteración, los parámetros se actualan con:\n",
        "\n",
        "$$\n",
        "\\beta \\leftarrow \\beta - \\eta \\cdot \\left( \\nabla_\\beta \\mathcal{L} + \\lambda \\nabla_\\beta R(\\beta) \\right)\n",
        "$$\n",
        "\n",
        "donde \\( \\eta \\) es la tasa de aprendizaje (*learning rate*).\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ Notas importantes:\n",
        "\n",
        "- El algoritmo trabaja sobre **minibatches** o **una muestra por iteración**.\n",
        "- Admite diferentes esquemas de actualización: `'constant'`, `'optimal'`, `'invscaling'`, etc.\n",
        "- Es ideal para conjuntos de datos grandes y dispersos (alta dimensionalidad).\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Recomendación\n",
        "\n",
        "Ajustar cuidadosamente:\n",
        "\n",
        "- `penalty`: `'l2'`, `'l1'`, `'elasticnet'`\n",
        "- `alpha`: fuerza de regularización\n",
        "- `learning_rate`: tipo de actualización\n",
        "- `eta0`: tasa de aprendizaje inicial\n"
      ],
      "metadata": {
        "id": "E-OTxEkIKlqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 5_SGDRegressor.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from scipy.stats import loguniform, uniform\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_contour\n",
        "\n",
        "st.set_page_config(page_title=\"Optimización de Hiperparámetros\", page_icon=\"⚙️\", layout=\"wide\")\n",
        "\n",
        "# ===============================\n",
        "# 1. Carga y Preparación de Datos\n",
        "# ===============================\n",
        "@st.cache_data\n",
        "def load_and_prepare_data():\n",
        "    with st.spinner('Descargando y preparando datos...'):\n",
        "        path = kagglehub.dataset_download(\"shashanknecrothapa/ames-housing-dataset\")\n",
        "        csv_file_path = os.path.join(path, \"AmesHousing.csv\")\n",
        "        Xdata = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Limpieza de datos\n",
        "        Xdata = Xdata.sample(frac=0.20, random_state=42)\n",
        "        cols_to_drop = ['Order', 'PID']\n",
        "        Xdata.drop(columns=[col for col in cols_to_drop if col in Xdata.columns], inplace=True)\n",
        "        high_null_cols = Xdata.columns[Xdata.isnull().mean() > 0.4].tolist()\n",
        "        Xdata.drop(columns=high_null_cols, inplace=True)\n",
        "\n",
        "        return Xdata\n",
        "\n",
        "Xdata = load_and_prepare_data()\n",
        "\n",
        "# ===============================\n",
        "# 2. Transformación de Variables\n",
        "# ===============================\n",
        "st.header(\"📐 Transformación de Variables\")\n",
        "\n",
        "col_sal = \"SalePrice\"\n",
        "\n",
        "# Selector para visualizar transformación\n",
        "transform_col = st.selectbox(\"Seleccione columna para visualizar transformación\",\n",
        "                            options=Xdata.select_dtypes(include=['int64', 'float64']).columns)\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(Xdata[transform_col], kde=True)\n",
        "    plt.title(f'Distribución Original de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "with col2:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(np.log1p(Xdata[transform_col]), kde=True)\n",
        "    plt.title(f'Distribución con log1p de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "st.info(\"\"\"\n",
        "💡 **Transformación logarítmica (log1p):**\n",
        "- Se aplica para manejar distribuciones sesgadas\n",
        "- log1p = log(1 + x) evita problemas con valores cero\n",
        "- Ayuda a cumplir supuestos de normalidad en modelos lineales\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 3. División de Datos\n",
        "# ===============================\n",
        "st.header(\"✂️ División del Dataset\")\n",
        "\n",
        "test_size = st.slider(\"Porcentaje para test\", 10, 40, 30, 5)\n",
        "\n",
        "Xtrain, Xtest = train_test_split(Xdata, test_size=test_size/100, random_state=42)\n",
        "ytrain = np.log1p(Xtrain[col_sal])\n",
        "ytest = np.log1p(Xtest[col_sal])\n",
        "Xtrain = Xtrain.drop(columns=col_sal)\n",
        "Xtest = Xtest.drop(columns=col_sal)\n",
        "\n",
        "st.success(f\"\"\"\n",
        "División completada:\n",
        "- Entrenamiento: {Xtrain.shape[0]} registros ({100-test_size}%)\n",
        "- Prueba: {Xtest.shape[0]} registros ({test_size}%)\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 4. Preprocesamiento\n",
        "# ===============================\n",
        "st.header(\"🔧 Pipeline de Preprocesamiento\")\n",
        "\n",
        "numeric_cols = Xtrain.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = Xtrain.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "with st.expander(\"Ver detalles de preprocesamiento\"):\n",
        "    st.write(\"**Columnas numéricas:**\")\n",
        "    st.write(numeric_cols)\n",
        "    st.write(\"**Transformaciones:** Imputación con mediana + Estandarización\")\n",
        "\n",
        "    st.write(\"\\n**Columnas categóricas:**\")\n",
        "    st.write(categorical_cols)\n",
        "    st.write(\"**Transformaciones:** Imputación con 'missing' + One-Hot Encoding\")\n",
        "\n",
        "# ===============================\n",
        "# 5. Optimización de Hiperparámetros (SGDRegressor)\n",
        "# ===============================\n",
        "st.header(\"⚙️ Optimización de Hiperparámetros - SGDRegressor\")\n",
        "\n",
        "# Configuración común\n",
        "cv = st.sidebar.slider(\"Número de folds para CV\", 3, 10, 3)\n",
        "random_state = st.sidebar.number_input(\"Random state\", 42)\n",
        "scoring = 'neg_mean_squared_error'\n",
        "\n",
        "# Configuración de parámetros\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.subheader(\"Grid Search\")\n",
        "    alpha_min = st.number_input(\"Alpha mínimo (log)\", -4, 2, -4)\n",
        "    alpha_max = st.number_input(\"Alpha máximo (log)\", -4, 2, 2)\n",
        "    alpha_points = st.slider(\"Puntos para alpha\", 5, 20, 10)\n",
        "    l1_ratio_points = st.slider(\"Puntos para l1_ratio\", 5, 20, 10)\n",
        "\n",
        "with col2:\n",
        "    st.subheader(\"Random Search\")\n",
        "    n_iter = st.slider(\"Número de iteraciones\", 10, 100, 20)\n",
        "    bayesian_trials = st.slider(\"Número de trials Bayesianos\", 10, 100, 20)\n",
        "\n",
        "if st.button(\"Ejecutar Optimización\"):\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    # Preparar parámetros\n",
        "    param_grid = {\n",
        "        'regressor__alpha': np.logspace(alpha_min, alpha_max, alpha_points),\n",
        "        'regressor__l1_ratio': np.linspace(0, 1, l1_ratio_points)\n",
        "    }\n",
        "\n",
        "    param_dist = {\n",
        "        'regressor__alpha': loguniform(1e-4, 1e2),\n",
        "        'regressor__l1_ratio': uniform(0, 1)\n",
        "    }\n",
        "\n",
        "    # 1. Grid Search\n",
        "    status_text.text(\"Ejecutando Grid Search...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        Pipeline(steps=[('preprocessing', preprocessor), ('regressor', SGDRegressor(random_state=random_state))]),\n",
        "        param_grid, scoring=scoring, cv=cv\n",
        "    )\n",
        "    grid_search.fit(Xtrain, ytrain)\n",
        "    grid_results = [\n",
        "        (params['regressor__alpha'], params['regressor__l1_ratio'], -score)\n",
        "        for params, score in zip(grid_search.cv_results_['params'],\n",
        "                               grid_search.cv_results_['mean_test_score'])\n",
        "    ]\n",
        "    progress_bar.progress(33)\n",
        "\n",
        "    # 2. Random Search\n",
        "    status_text.text(\"Ejecutando Random Search...\")\n",
        "    random_search = RandomizedSearchCV(\n",
        "        Pipeline(steps=[('preprocessing', preprocessor), ('regressor', SGDRegressor(random_state=random_state))]),\n",
        "        param_dist, n_iter=n_iter, scoring=scoring, cv=cv, random_state=random_state\n",
        "    )\n",
        "    random_search.fit(Xtrain, ytrain)\n",
        "    random_results = [\n",
        "        (params['regressor__alpha'], params['regressor__l1_ratio'], -score)\n",
        "        for params, score in zip(random_search.cv_results_['params'],\n",
        "                               random_search.cv_results_['mean_test_score'])\n",
        "    ]\n",
        "    progress_bar.progress(66)\n",
        "\n",
        "    # 3. Bayesian Optimization\n",
        "    status_text.text(\"Ejecutando Bayesian Optimization...\")\n",
        "\n",
        "    def objective_sgd(trial):\n",
        "        alpha = trial.suggest_float('alpha', 1e-4, 1e2, log=True)\n",
        "        l1_ratio = trial.suggest_float('l1_ratio', 0, 1)\n",
        "        model = Pipeline(steps=[\n",
        "            ('preprocessing', preprocessor),\n",
        "            ('regressor', SGDRegressor(\n",
        "                alpha=alpha,\n",
        "                l1_ratio=l1_ratio,\n",
        "                random_state=random_state\n",
        "            ))\n",
        "        ])\n",
        "        try:\n",
        "            return -cross_val_score(model, Xtrain, ytrain, scoring=scoring, cv=cv).mean()\n",
        "        except:\n",
        "            return float('inf')\n",
        "\n",
        "    study_sgd = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
        "    study_sgd.optimize(objective_sgd, n_trials=bayesian_trials)\n",
        "    progress_bar.progress(100)\n",
        "\n",
        "    # Almacenar resultados\n",
        "    best_params = {\n",
        "        'GridSearch': grid_search.best_params_,\n",
        "        'RandomSearch': random_search.best_params_,\n",
        "        'Bayesian': study_sgd.best_params\n",
        "    }\n",
        "\n",
        "    # ===============================\n",
        "    # 6. Visualización de Resultados\n",
        "    # ===============================\n",
        "    st.success(\"Optimización completada!\")\n",
        "\n",
        "    # Mostrar mejores parámetros\n",
        "    st.subheader(\"🏆 Mejores Parámetros Encontrados\")\n",
        "    cols = st.columns(3)\n",
        "    with cols[0]:\n",
        "        st.metric(\"Grid Search - Alpha\", best_params['GridSearch']['regressor__alpha'])\n",
        "        st.metric(\"Grid Search - L1 Ratio\", best_params['GridSearch']['regressor__l1_ratio'])\n",
        "    with cols[1]:\n",
        "        st.metric(\"Random Search - Alpha\", best_params['RandomSearch']['regressor__alpha'])\n",
        "        st.metric(\"Random Search - L1 Ratio\", best_params['RandomSearch']['regressor__l1_ratio'])\n",
        "    with cols[2]:\n",
        "        st.metric(\"Bayesian - Alpha\", best_params['Bayesian']['alpha'])\n",
        "        st.metric(\"Bayesian - L1 Ratio\", best_params['Bayesian']['l1_ratio'])\n",
        "\n",
        "    # Gráficos de comparación\n",
        "    st.subheader(\"📊 Comparación de Métodos de Optimización\")\n",
        "\n",
        "    tab1, tab2, tab3 = st.tabs([\"Grid Search\", \"Random Search\", \"Bayesian Optimization\"])\n",
        "\n",
        "    with tab1:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        x_values = [r[0] for r in grid_results]\n",
        "        y_values = [r[1] for r in grid_results]\n",
        "        scores = [r[2] for r in grid_results]\n",
        "        scatter = ax.scatter(x_values, y_values, c=scores, cmap='viridis')\n",
        "        plt.colorbar(scatter, ax=ax, label='MSE')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_xlabel('alpha')\n",
        "        ax.set_ylabel('l1_ratio')\n",
        "        ax.set_title('Grid Search - SGDRegressor')\n",
        "        ax.grid(True, which='both', ls='--')\n",
        "        st.pyplot(fig)\n",
        "        st.write(f\"Mejor MSE: {-grid_search.best_score_:.4f}\")\n",
        "\n",
        "    with tab2:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        x_values = [r[0] for r in random_results]\n",
        "        y_values = [r[1] for r in random_results]\n",
        "        scores = [r[2] for r in random_results]\n",
        "        scatter = ax.scatter(x_values, y_values, c=scores, cmap='viridis')\n",
        "        plt.colorbar(scatter, ax=ax, label='MSE')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_xlabel('alpha')\n",
        "        ax.set_ylabel('l1_ratio')\n",
        "        ax.set_title('Random Search - SGDRegressor')\n",
        "        ax.grid(True, which='both', ls='--')\n",
        "        st.pyplot(fig)\n",
        "        st.write(f\"Mejor MSE: {-random_search.best_score_:.4f}\")\n",
        "\n",
        "    with tab3:\n",
        "        st.plotly_chart(plot_optimization_history(study_sgd))\n",
        "        st.plotly_chart(plot_param_importances(study_sgd))\n",
        "        st.plotly_chart(plot_contour(study_sgd, params=[\"alpha\", \"l1_ratio\"]))\n",
        "        st.write(f\"Mejor MSE: {study_sgd.best_value:.4f}\")\n",
        "\n",
        "    # Análisis comparativo interactivo\n",
        "    st.subheader(\"🔍 Análisis Comparativo Interactivo\")\n",
        "\n",
        "    # Crear un diccionario con todas las métricas disponibles\n",
        "    metrics_data = {\n",
        "        'MAE': {\n",
        "            'GridSearch': mean_absolute_error(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': mean_absolute_error(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': mean_absolute_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', SGDRegressor(**study_sgd.best_params, random_state=random_state))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'MSE': {\n",
        "            'GridSearch': mean_squared_error(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': mean_squared_error(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': mean_squared_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', SGDRegressor(**study_sgd.best_params, random_state=random_state))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'R2': {\n",
        "            'GridSearch': r2_score(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': r2_score(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': r2_score(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', SGDRegressor(**study_sgd.best_params, random_state=random_state))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'MAPE': {\n",
        "            'GridSearch': mean_absolute_percentage_error(ytrain, grid_search.best_estimator_.predict(Xtrain)) * 100,\n",
        "            'RandomSearch': mean_absolute_percentage_error(ytrain, random_search.best_estimator_.predict(Xtrain)) * 100,\n",
        "            'Bayesian': mean_absolute_percentage_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', SGDRegressor(**study_sgd.best_params, random_state=random_state))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain)) * 100\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Selector de métrica\n",
        "    selected_metric = st.selectbox(\n",
        "        \"Seleccione la métrica a visualizar:\",\n",
        "        options=['MAE', 'MSE', 'R2', 'MAPE'],\n",
        "        index=1  # MSE por defecto\n",
        "    )\n",
        "\n",
        "    # Configuración de visualización según la métrica\n",
        "    if selected_metric == 'MAPE':\n",
        "        ylabel = 'MAPE (%)'\n",
        "        title = f'Comparación de {selected_metric} entre Métodos'\n",
        "        fmt = '.2f%'\n",
        "        ascending = True\n",
        "    elif selected_metric == 'R2':\n",
        "        ylabel = 'R²'\n",
        "        title = f'Comparación de {selected_metric} entre Métodos'\n",
        "        fmt = '.4f'\n",
        "        ascending = False\n",
        "    else:\n",
        "        ylabel = selected_metric\n",
        "        title = f'Comparación de {selected_metric} entre Métodos'\n",
        "        fmt = '.4f'\n",
        "        ascending = True\n",
        "\n",
        "    # Ordenar métodos según el rendimiento\n",
        "    methods = pd.DataFrame({\n",
        "        'Method': ['GridSearch', 'RandomSearch', 'Bayesian'],\n",
        "        'Value': [metrics_data[selected_metric]['GridSearch'],\n",
        "                metrics_data[selected_metric]['RandomSearch'],\n",
        "                metrics_data[selected_metric]['Bayesian']]\n",
        "    }).sort_values('Value', ascending=ascending)\n",
        "\n",
        "    # Crear la figura\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    colors = ['skyblue', 'lightgreen', 'salmon']\n",
        "    bars = ax.bar(methods['Method'], methods['Value'], color=colors)\n",
        "\n",
        "    # Configurar el gráfico\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Añadir los valores a las barras\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        if selected_metric == 'MAPE':\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.2f}%',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "        else:\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:{fmt}}',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    # Rotar etiquetas del eje x para mejor legibilidad\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Mostrar el gráfico en Streamlit\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Mostrar tabla con todas las métricas\n",
        "    st.subheader(\"📊 Resumen de Métricas\")\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "    metrics_df['MAPE'] = metrics_df['MAPE'].map('{:.2f}%'.format)\n",
        "    metrics_df['MAE'] = metrics_df['MAE'].map('{:.4f}'.format)\n",
        "    metrics_df['MSE'] = metrics_df['MSE'].map('{:.4f}'.format)\n",
        "    metrics_df['R2'] = metrics_df['R2'].map('{:.4f}'.format)\n",
        "    st.dataframe(metrics_df.style.background_gradient(cmap='Blues', axis=0))\n",
        "\n",
        "    # Mostrar recomendación basada en la métrica seleccionada\n",
        "    best_method = methods.iloc[0]['Method']\n",
        "    best_value = methods.iloc[0]['Value']\n",
        "\n",
        "    if selected_metric == 'MAPE':\n",
        "        st.success(f\"✅ **Mejor modelo según {selected_metric}:** {best_method} ({best_value:.2f}%)\")\n",
        "    elif selected_metric == 'R2':\n",
        "        st.success(f\"✅ **Mejor modelo según {selected_metric}:** {best_method} ({best_value:.4f})\")\n",
        "    else:\n",
        "        st.success(f\"✅ **Mejor modelo según {selected_metric}:** {best_method} ({best_value:.4f})\")\n",
        "\n",
        "# ===============================\n",
        "# 7. Información Adicional\n",
        "# ===============================\n",
        "with st.expander(\"ℹ️ Instrucciones de Uso\", expanded=True):\n",
        "    st.write(\"\"\"\n",
        "    1. Configura los parámetros de búsqueda en el panel lateral\n",
        "    2. Haz clic en \"Ejecutar Optimización\"\n",
        "    3. Explora los resultados en las diferentes pestañas\n",
        "    4. Compara el rendimiento de los diferentes métodos\n",
        "\n",
        "    **Tipos de Búsqueda:**\n",
        "    - **Grid Search:** Búsqueda exhaustiva en una grilla definida\n",
        "    - **Random Search:** Muestreo aleatorio del espacio de parámetros\n",
        "    - **Bayesian Optimization:** Optimización inteligente basada en modelos\n",
        "\n",
        "    **Hiperparámetros de SGDRegressor:**\n",
        "    - **alpha:** Parámetro de regularización (controla la penalización)\n",
        "    - **l1_ratio:** Balance entre regularización L1 y L2 (0 = L2, 1 = L1)\n",
        "    \"\"\")\n",
        "# ===============================\n",
        "# 8. Evaluación del Modelo en Test\n",
        "# ===============================\n",
        "st.header(\"📊 Evaluación del Modelo SGDRegressor en Datos de Test\")\n",
        "\n",
        "# Seleccionar el mejor modelo (usaremos el de Bayesian Optimization por defecto)\n",
        "best_params = study_sgd.best_params\n",
        "final_model = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('regressor', SGDRegressor(**best_params, random_state=random_state))\n",
        "])\n",
        "final_model.fit(Xtrain, ytrain)\n",
        "\n",
        "# Predecir en test\n",
        "ypred = final_model.predict(Xtest)\n",
        "\n",
        "# Calcular métricas\n",
        "test_mae = mean_absolute_error(ytest, ypred)\n",
        "test_mse = mean_squared_error(ytest, ypred)\n",
        "test_r2 = r2_score(ytest, ypred)\n",
        "test_mape = mean_absolute_percentage_error(ytest, ypred) * 100\n",
        "\n",
        "# Mostrar métricas\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "with col1:\n",
        "    st.metric(\"MAE (Test)\", f\"{test_mae:.4f}\")\n",
        "with col2:\n",
        "    st.metric(\"MSE (Test)\", f\"{test_mse:.4f}\")\n",
        "with col3:\n",
        "    st.metric(\"R² (Test)\", f\"{test_r2:.4f}\")\n",
        "with col4:\n",
        "    st.metric(\"MAPE (Test)\", f\"{test_mape:.2f}%\")\n",
        "\n",
        "# Gráficos de evaluación\n",
        "st.subheader(\"🔍 Gráficos de Evaluación\")\n",
        "\n",
        "tab1, tab2, tab3, tab4 = st.tabs([\"Predicciones vs Reales\", \"Residuos\", \"Distribución de Errores\", \"Convergencia\"])\n",
        "\n",
        "with tab1:\n",
        "    # Gráfico de predicciones vs valores reales\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(ytest, ypred, alpha=0.5)\n",
        "    ax.plot([ytest.min(), ytest.max()], [ytest.min(), ytest.max()], 'k--', lw=2)\n",
        "    ax.set_xlabel('Valores Reales (log1p)')\n",
        "    ax.set_ylabel('Predicciones (log1p)')\n",
        "    ax.set_title('Predicciones vs Valores Reales (SGDRegressor)')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretación:**\n",
        "    - SGD puede mostrar más dispersión que otros modelos debido a su naturaleza estocástica\n",
        "    - Los puntos deberían agruparse alrededor de la línea diagonal\n",
        "    - Patrones no lineales pueden indicar que se necesita un modelo más complejo\n",
        "    \"\"\")\n",
        "\n",
        "with tab2:\n",
        "    # Gráfico de residuos\n",
        "    residuals = ytest - ypred\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(ypred, residuals, alpha=0.5)\n",
        "    ax.axhline(y=0, color='r', linestyle='--')\n",
        "    ax.set_xlabel('Predicciones (log1p)')\n",
        "    ax.set_ylabel('Residuos')\n",
        "    ax.set_title('Gráfico de Residuos (SGDRegressor)')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretación:**\n",
        "    - Residuos deberían distribuirse aleatoriamente alrededor del cero\n",
        "    - SGD puede mostrar más variabilidad en los residuos que otros modelos\n",
        "    - Patrones visibles pueden indicar que el learning rate no fue óptimo\n",
        "    \"\"\")\n",
        "\n",
        "with tab3:\n",
        "    # Distribución de errores\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.histplot(residuals, kde=True, ax=ax)\n",
        "    ax.set_xlabel('Error de Predicción')\n",
        "    ax.set_title('Distribución de Errores (SGDRegressor)')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretación:**\n",
        "    - Distribución centrada en cero indica que no hay sesgo sistemático\n",
        "    - La forma de la distribución muestra cómo se comportan los errores\n",
        "    - SGD puede producir distribuciones con colas más pesadas que otros métodos\n",
        "    \"\"\")\n",
        "\n",
        "with tab4:\n",
        "    # Gráfico de convergencia (para SGD)\n",
        "    try:\n",
        "        # Entrenar modelo guardando pérdida en cada epoch\n",
        "        partial_model = Pipeline(steps=[\n",
        "            ('preprocessing', preprocessor),\n",
        "            ('regressor', SGDRegressor(**best_params, random_state=random_state))\n",
        "        ])\n",
        "\n",
        "        # Crear lista para guardar pérdidas\n",
        "        train_errors = []\n",
        "        test_errors = []\n",
        "\n",
        "        # Mini-batch para simular epochs de SGD\n",
        "        for epoch in range(1, 101):\n",
        "            partial_model.fit(Xtrain, ytrain)\n",
        "            train_errors.append(mean_squared_error(ytrain, partial_model.predict(Xtrain)))\n",
        "            test_errors.append(mean_squared_error(ytest, partial_model.predict(Xtest)))\n",
        "\n",
        "        # Gráfico de convergencia\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.plot(range(1, 101), train_errors, 'b-', label='Train MSE')\n",
        "        ax.plot(range(1, 101), test_errors, 'r-', label='Test MSE')\n",
        "        ax.set_xlabel('Epochs')\n",
        "        ax.set_ylabel('MSE')\n",
        "        ax.set_title('Curva de Aprendizaje (SGDRegressor)')\n",
        "        ax.legend()\n",
        "        ax.grid(True)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        st.write(\"\"\"\n",
        "        **Interpretación:**\n",
        "        - Muestra cómo evoluciona el error durante el entrenamiento\n",
        "        - SGD debería converger a un valor estable después de varias epochs\n",
        "        - Si las curvas no convergen, puede indicar que el learning rate es inadecuado\n",
        "        \"\"\")\n",
        "    except Exception as e:\n",
        "        st.warning(f\"No se pudo generar el gráfico de convergencia: {str(e)}\")\n",
        "\n",
        "# Gráfico de valores reales vs predichos en escala original\n",
        "st.subheader(\"💰 Predicciones en Escala Original (USD)\")\n",
        "\n",
        "# Convertir a escala original\n",
        "ytest_orig = np.expm1(ytest)\n",
        "ypred_orig = np.expm1(ypred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.scatter(ytest_orig, ypred_orig, alpha=0.5)\n",
        "ax.plot([ytest_orig.min(), ytest_orig.max()], [ytest_orig.min(), ytest_orig.max()], 'k--', lw=2)\n",
        "ax.set_xlabel('Valores Reales (USD)')\n",
        "ax.set_ylabel('Predicciones (USD)')\n",
        "ax.set_title('Predicciones vs Valores Reales - Escala Original (SGDRegressor)')\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Calcular métricas en escala original\n",
        "mae_orig = mean_absolute_error(ytest_orig, ypred_orig)\n",
        "mape_orig = mean_absolute_percentage_error(ytest_orig, ypred_orig) * 100\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.metric(\"MAE (USD)\", f\"${mae_orig:,.2f}\")\n",
        "with col2:\n",
        "    st.metric(\"MAPE (USD)\", f\"{mape_orig:.2f}%\")\n",
        "\n",
        "# Análisis de errores por rango de precio\n",
        "st.subheader(\"📈 Análisis de Errores por Rango de Precio\")\n",
        "\n",
        "# Crear categorías de precios\n",
        "price_bins = pd.qcut(ytest_orig, q=5, duplicates='drop')\n",
        "error_analysis = pd.DataFrame({\n",
        "    'Precio Real': ytest_orig,\n",
        "    'Error Absoluto': np.abs(ytest_orig - ypred_orig),\n",
        "    'Rango Precio': price_bins\n",
        "})\n",
        "\n",
        "# Calcular métricas por rango\n",
        "error_by_price = error_analysis.groupby('Rango Precio').agg({\n",
        "    'Precio Real': 'mean',\n",
        "    'Error Absoluto': ['mean', 'median', 'std']\n",
        "}).reset_index()\n",
        "\n",
        "error_by_price.columns = ['Rango Precio', 'Precio Promedio', 'MAE', 'Error Mediano', 'Desviación Error']\n",
        "\n",
        "# Gráfico de MAE por rango de precio\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.barplot(data=error_by_price, x='Rango Precio', y='MAE', ax=ax)\n",
        "ax.set_title('Error Absoluto Medio por Rango de Precio (SGDRegressor)')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "ax.set_ylabel('MAE (USD)')\n",
        "ax.set_xlabel('Rango de Precio')\n",
        "st.pyplot(fig)\n",
        "\n",
        "st.write(\"\"\"\n",
        "**Interpretación:**\n",
        "- SGD puede tener un rendimiento diferente en distintos rangos de precio\n",
        "- Errores mayores en extremos (propiedades muy baratas o muy caras) son comunes\n",
        "- Puede indicar la necesidad de ajustar parámetros de regularización\n",
        "\"\"\")\n",
        "\n",
        "# Mostrar tabla de análisis\n",
        "st.dataframe(error_by_price.style.background_gradient(subset=['MAE'], cmap='Reds'))\n",
        "\n",
        "# Comparación con modelo naive (promedio)\n",
        "st.subheader(\"🤔 Comparación con Modelo Naive (Promedio)\")\n",
        "\n",
        "naive_pred = np.full_like(ytest, ytrain.mean())\n",
        "naive_mae = mean_absolute_error(ytest, naive_pred)\n",
        "improvement = (naive_mae - test_mae) / naive_mae * 100\n",
        "\n",
        "st.metric(\"Mejora sobre modelo naive\", f\"{improvement:.2f}%\",\n",
        "          delta=f\"MAE naive: {naive_mae:.4f}, MAE SGD: {test_mae:.4f}\")\n",
        "\n",
        "st.write(\"\"\"\n",
        "**Interpretación:**\n",
        "- Muestra el valor añadido del modelo SGD respecto a una predicción simple\n",
        "- Una mejora positiva indica que el modelo está aprendiendo patrones útiles\n",
        "- SGD puede ser especialmente sensible a la escala de los datos y parámetros de regularización\n",
        "\"\"\")\n",
        "\n",
        "# Análisis de sensibilidad a parámetros\n",
        "st.subheader(\"🎚️ Sensibilidad a Parámetros\")\n",
        "\n",
        "# Crear gráfico de sensibilidad para alpha y l1_ratio\n",
        "try:\n",
        "    alphas = np.logspace(-3, 2, 10)\n",
        "    l1_ratios = np.linspace(0, 1, 5)\n",
        "\n",
        "    mse_values = []\n",
        "    for alpha in alphas:\n",
        "        for l1_ratio in l1_ratios:\n",
        "            model = Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', SGDRegressor(alpha=alpha, l1_ratio=l1_ratio, random_state=random_state))\n",
        "            ])\n",
        "            mse = -cross_val_score(model, Xtrain, ytrain, scoring='neg_mean_squared_error', cv=3).mean()\n",
        "            mse_values.append((alpha, l1_ratio, mse))\n",
        "\n",
        "    sens_df = pd.DataFrame(mse_values, columns=['alpha', 'l1_ratio', 'MSE'])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    for l1_ratio in l1_ratios:\n",
        "        subset = sens_df[sens_df['l1_ratio'] == l1_ratio]\n",
        "        ax.plot(subset['alpha'], subset['MSE'], 'o-', label=f'l1_ratio={l1_ratio:.1f}')\n",
        "\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xlabel('alpha (log scale)')\n",
        "    ax.set_ylabel('MSE')\n",
        "    ax.set_title('Sensibilidad del MSE a alpha y l1_ratio')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretación:**\n",
        "    - Muestra cómo cambia el rendimiento con diferentes parámetros\n",
        "    - SGD puede ser muy sensible a la elección de alpha (parámetro de regularización)\n",
        "    - El balance entre L1 y L2 (l1_ratio) afecta la selección de características\n",
        "    \"\"\")\n",
        "except Exception as e:\n",
        "    st.warning(f\"No se pudo generar el gráfico de sensibilidad: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys_J3LlcXBo_",
        "outputId": "b1b8908d-19ca-4002-978e-9e7764cbeca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 5_SGDRegressor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 5_SGDRegressor.py pages/"
      ],
      "metadata": {
        "id": "tL6SBH9nXW_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inicialización del Dashboard a partir de túnel local**"
      ],
      "metadata": {
        "id": "I4Ut_UzpK3Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 /usr/local/bin/cloudflared\n",
        "\n",
        "#Ejecutar Streamlit\n",
        "!streamlit run TAM.py &>/content/logs.txt & #Cambiar 0_👋_Hello.py por el nombre de tu archivo principal\n",
        "\n",
        "#Exponer el puerto 8501 con Cloudflare Tunnel\n",
        "!cloudflared tunnel --url http://localhost:8501 > /content/cloudflared.log 2>&1 &\n",
        "\n",
        "#Leer la URL pública generada por Cloudflare\n",
        "import time\n",
        "time.sleep(5)  # Esperar que se genere la URL\n",
        "\n",
        "import re\n",
        "found_context = False  # Indicador para saber si estamos en la sección correcta\n",
        "\n",
        "with open('/content/cloudflared.log') as f:\n",
        "    for line in f:\n",
        "        #Detecta el inicio del contexto que nos interesa\n",
        "        if \"Your quick Tunnel has been created\" in line:\n",
        "            found_context = True\n",
        "\n",
        "        #Busca una URL si ya se encontró el contexto relevante\n",
        "        if found_context:\n",
        "            match = re.search(r'https?://\\S+', line)\n",
        "            if match:\n",
        "                url = match.group(0)  #Extrae la URL encontrada\n",
        "                print(f'Tu aplicación está disponible en: {url}')\n",
        "                break  #Termina el bucle después de encontrar la URL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWx9MTraDhcs",
        "outputId": "6e33774c-328b-492b-c71e-0370c4524579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-24 02:19:23--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2025.5.0/cloudflared-linux-amd64 [following]\n",
            "--2025-05-24 02:19:23--  https://github.com/cloudflare/cloudflared/releases/download/2025.5.0/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/797840ed-70cb-47b8-a6fe-ecb4b3385c94?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250524%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250524T021805Z&X-Amz-Expires=300&X-Amz-Signature=f3770c8694699edf9705fb6b2d1f6d26606145f00cc982260689876750f958d6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-05-24 02:19:23--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/797840ed-70cb-47b8-a6fe-ecb4b3385c94?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250524%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250524T021805Z&X-Amz-Expires=300&X-Amz-Signature=f3770c8694699edf9705fb6b2d1f6d26606145f00cc982260689876750f958d6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37839075 (36M) [application/octet-stream]\n",
            "Saving to: ‘cloudflared-linux-amd64’\n",
            "\n",
            "cloudflared-linux-a 100%[===================>]  36.09M   142MB/s    in 0.3s    \n",
            "\n",
            "2025-05-24 02:19:24 (142 MB/s) - ‘cloudflared-linux-amd64’ saved [37839075/37839075]\n",
            "\n",
            "Tu aplicación está disponible en: https://christ-serving-pull-farm.trycloudflare.com\n"
          ]
        }
      ]
    }
  ]
}