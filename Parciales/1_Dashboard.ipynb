{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nMtKfkp2JElN",
        "qx2HjXFLJvD_",
        "_S1jtSyFJ84i",
        "qgwtdVsZKNO-",
        "LMptc2nRKW-P",
        "E-OTxEkIKlqe",
        "I4Ut_UzpK3Tk"
      ],
      "authorship_tag": "ABX9TyNJFbNNdq256/R6q61qssJY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TatanPerez/Teoria_Aprendizaje_Maquina/blob/main/Parciales/1_Dashboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelos de Regresion**\n",
        "\n",
        "**Elaborado por:** Wilmer Sebastian Perez C. wiperezc@unal.edu.co\n",
        "\n",
        "**Universidad Nacional de Colombia - Sede Manizales**\n",
        "\n",
        "**Mayo del 2025-I**"
      ],
      "metadata": {
        "id": "hkTbrxI1Hd_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conjunto de datos Ames Housing Dataset como ejemplo completo para un problema de regresi√≥n usando sci-kitlearn**\n",
        "\n",
        "El siguiente ejemplo presenta las etapas b√°sicas de un proyecto de anal√≠tica de datos en una tarea de regresi√≥n, orientadas a:\n",
        "\n",
        "- Preproceso de atributos con campos vacios y tipo texto.\n",
        "- Entrenamiento y selecci√≥n de un modelo de regresi√≥n bajo una estrategia de validaci√≥n cruzada.\n",
        "- La utilizaci√≥n de diccionarios para la sintonizaci√≥n de hiperpar√°metros.\n",
        "- Se ilustra tambi√©n la creaci√≥n de clases (objetos) propios compatibles con la clase pipeline de sci-kitlearn.\n",
        "\n",
        "**Base de datos utilizada**: [Ames Housing - Kaggle](https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset)."
      ],
      "metadata": {
        "id": "q0e2zCziHCpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalaci√≥n de librer√≠as**"
      ],
      "metadata": {
        "id": "c9oNY0WsIn5G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVJDTHZ14XXd",
        "outputId": "be28291e-5de3-4999-eff1-948a06f814dc",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.5.0)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-25.1.0 scikit-optimize-0.10.2\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m591.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.8\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.8)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.39.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-optimize\n",
        "!pip install streamlit -q #instalaci√≥n de librer√≠as\n",
        "!pip install pyngrok\n",
        "!pip install optuna\n",
        "!pip install streamlit pandas matplotlib seaborn scikit-learn pyngrok kagglehub\n",
        "!pip install pyngrok streamlit --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Crear carpeta pages para trabajar Multiapp en Streamlit"
      ],
      "metadata": {
        "id": "MGT_uqEnI3nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pages"
      ],
      "metadata": {
        "id": "wR2Dd1LK_73w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **P√°gina principal**"
      ],
      "metadata": {
        "id": "nMtKfkp2JElN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile TAM.py\n",
        "import streamlit as st\n",
        "from pyngrok import ngrok\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import joblib\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import ElasticNet, SGDRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from scipy.stats import loguniform, uniform\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# -----------------------\n",
        "# CONFIGURACI√ìN INICIAL\n",
        "# -----------------------\n",
        "st.set_page_config(page_title=\"TAM - Modelos Regresi√≥n\", page_icon=\"üëã\", layout=\"wide\")\n",
        "st.write(\"Parcial 1 TAM - Modelos de regresi√≥n\")\n",
        "\n",
        "# Sidebar con informaci√≥n del equipo\n",
        "st.sidebar.title(\"Equipo del Proyecto\")\n",
        "st.sidebar.success(\"Seleccciona una modelo a explorar.\")\n",
        "st.markdown(\"\"\"\n",
        "### Conjunto de datos Ames Housing Dataset como ejemplo completo para un problema de regresi√≥n usando sci-kitlearn\n",
        "\n",
        "El siguiente ejemplo presenta las etapas b√°sicas de un proyecto de anal√≠tica de datos en una tarea de regresi√≥n, orientadas a:\n",
        "\n",
        "- Preproceso de atributos con campos vacios y tipo texto.\n",
        "- Entrenamiento y selecci√≥n de un modelo de regresi√≥n bajo una estrategia de validaci√≥n cruzada.\n",
        "- La utilizaci√≥n de diccionarios para la sintonizaci√≥n de hiperpar√°metros.\n",
        "- Se ilustra tambi√©n la creaci√≥n de clases (objetos) propios compatibles con la clase pipeline de sci-kitlearn.\n",
        "\n",
        "**Base de datos utilizada**: [Ames Housing - Kaggle](https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset).\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_n3VEV63djm",
        "outputId": "ae7924f6-6c27-49d1-8ebd-a91eda6f4877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing TAM.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **P√°ginas**"
      ],
      "metadata": {
        "id": "0bHXSB7-JXSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analisis De Datos**"
      ],
      "metadata": {
        "id": "qx2HjXFLJvD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 1_üìä_ANALISIS_EXPLORATORIO.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "st.set_page_config(page_title=\"Analisis Exploratorio\", page_icon=\"üìä\")\n",
        "\n",
        "st.markdown(\"Analisis  Exploratorio\")\n",
        "st.sidebar.header(\"Analisis Exploratorio\")\n",
        "\n",
        "# ===============================\n",
        "# 1. Cargar la base de datos\n",
        "# ===============================\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    path = kagglehub.dataset_download(\"shashanknecrothapa/ames-housing-dataset\")\n",
        "    csv_file_path = os.path.join(path, \"AmesHousing.csv\")\n",
        "    return pd.read_csv(csv_file_path)\n",
        "\n",
        "with st.spinner('Cargando datos...'):\n",
        "    df = load_data()\n",
        "    Xdata = df.copy()\n",
        "\n",
        "# Mostrar opciones de visualizaci√≥n de datos\n",
        "if st.checkbox('Mostrar datos crudos'):\n",
        "    st.dataframe(Xdata.head())\n",
        "\n",
        "# ===============================\n",
        "# 2. An√°lisis Exploratorio\n",
        "# ===============================\n",
        "st.header(\"üîç An√°lisis Exploratorio\")\n",
        "\n",
        "# Mostrar informaci√≥n b√°sica\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.metric(\"N√∫mero de filas\", Xdata.shape[0])\n",
        "    st.metric(\"N√∫mero de columnas\", Xdata.shape[1])\n",
        "\n",
        "with col2:\n",
        "    st.write(\"Tipos de variables:\")\n",
        "    st.write(Xdata.dtypes.value_counts())\n",
        "\n",
        "# Columnas num√©ricas y categ√≥ricas\n",
        "num_cols = Xdata.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "cat_cols = Xdata.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "# Valores nulos\n",
        "st.subheader(\"Valores nulos\")\n",
        "missing_values = Xdata.isnull().sum()\n",
        "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
        "st.bar_chart(missing_values)\n",
        "\n",
        "# ===============================\n",
        "# 3. An√°lisis Descriptivo\n",
        "# ===============================\n",
        "st.header(\"üìä An√°lisis Descriptivo\")\n",
        "\n",
        "if st.checkbox('Mostrar estad√≠sticas descriptivas'):\n",
        "    desc_stats = Xdata[num_cols].describe().T\n",
        "    st.dataframe(desc_stats)\n",
        "\n",
        "# Correlaci√≥n con el target\n",
        "st.subheader(\"Correlaci√≥n con SalePrice\")\n",
        "correlation_with_target = Xdata[num_cols].corr()[\"SalePrice\"].sort_values(ascending=False)\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.write(\"Correlaci√≥n positiva m√°s fuerte:\")\n",
        "    st.dataframe(correlation_with_target.head(10))\n",
        "\n",
        "with col2:\n",
        "    st.write(\"Correlaci√≥n negativa m√°s fuerte:\")\n",
        "    st.dataframe(correlation_with_target.tail(10))\n",
        "\n",
        "# ===============================\n",
        "# 4. An√°lisis Relacional (Correlaci√≥n visual)\n",
        "# ===============================\n",
        "st.header(\"üîó An√°lisis Relacional\")\n",
        "\n",
        "# Agregar slider en el sidebar para seleccionar el umbral de correlaci√≥n\n",
        "min_corr = st.sidebar.slider(\n",
        "    \"Umbral m√≠nimo de correlaci√≥n (|r|)\",\n",
        "    min_value=0.1,\n",
        "    max_value=0.9,\n",
        "    value=0.3,\n",
        "    step=0.05,\n",
        "    help=\"Seleccione el valor m√≠nimo absoluto de correlaci√≥n para incluir variables en el an√°lisis\"\n",
        ")\n",
        "\n",
        "if st.checkbox('Mostrar matriz de correlaci√≥n'):\n",
        "    # Usar el valor seleccionado en el slider\n",
        "    strong_corr = Xdata[num_cols].corr()[\"SalePrice\"].abs()\n",
        "    strong_vars = strong_corr[strong_corr > min_corr].index\n",
        "\n",
        "    # Verificar que hayamos seleccionado al menos 2 variables\n",
        "    if len(strong_vars) < 2:\n",
        "        st.warning(f\"No hay suficientes variables con correlaci√≥n |r| > {min_corr}. Reduzca el umbral.\")\n",
        "    else:\n",
        "        filtered_corr = Xdata[strong_vars].corr()\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "        sns.heatmap(filtered_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True, ax=ax)\n",
        "        ax.set_title(f\"Matriz de Correlaci√≥n (|r| > {min_corr}) con SalePrice\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Explicaci√≥n del an√°lisis\n",
        "        st.subheader(\"üß† ¬øQu√© hace este an√°lisis?\")\n",
        "\n",
        "        explanation = f\"\"\"\n",
        "        **Este an√°lisis muestra las correlaciones entre variables con una relaci√≥n significativa con SalePrice (|r| > {min_corr}):**\n",
        "\n",
        "        - **Variables incluidas:** {len(strong_vars)} variables num√©ricas\n",
        "        - **Umbral de correlaci√≥n:** |r| > {min_corr}\n",
        "        - **Interpretaci√≥n de colores:**\n",
        "          - üî¥ Rojo: Correlaci√≥n positiva\n",
        "          - üîµ Azul: Correlaci√≥n negativa\n",
        "          - Color m√°s intenso = Correlaci√≥n m√°s fuerte\n",
        "\n",
        "        **Variables seleccionadas:** {', '.join(strong_vars)}\n",
        "        \"\"\"\n",
        "\n",
        "        st.markdown(explanation)\n",
        "\n",
        "        # Mostrar correlaciones individuales con SalePrice\n",
        "        st.write(\"**Correlaciones individuales con SalePrice:**\")\n",
        "        corr_with_target = Xdata[strong_vars].corr()[\"SalePrice\"].sort_values(ascending=False)\n",
        "        st.dataframe(corr_with_target.to_frame(\"Correlaci√≥n\"))\n",
        "\n",
        "        # Consejo sobre multicolinealidad\n",
        "        st.info(\"\"\"\n",
        "        üí° **Consejo profesional:**\n",
        "        Si dos variables predictoras tienen correlaci√≥n > 0.8 entre s√≠, considere:\n",
        "        - Eliminar una de ellas\n",
        "        - Crear una nueva caracter√≠stica combinada\n",
        "        - Usar t√©cnicas de reducci√≥n de dimensionalidad como PCA\n",
        "        \"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 5. An√°lisis Visual de Relaciones Clave\n",
        "# ===============================\n",
        "st.header(\"üìà An√°lisis Visual\")\n",
        "\n",
        "important_features = st.multiselect(\n",
        "    \"Seleccione caracter√≠sticas para visualizar\",\n",
        "    options=num_cols.tolist(),\n",
        "    default=[\"1st Flr SF\", \"Year Built\", \"Overall Qual\"]\n",
        ")\n",
        "\n",
        "if important_features:\n",
        "    plot_data = Xdata[important_features + [\"SalePrice\"]].copy()\n",
        "\n",
        "    # Normalizar los datos\n",
        "    scaler = StandardScaler()\n",
        "    plot_data_normalized = pd.DataFrame(scaler.fit_transform(plot_data),\n",
        "                                      columns=plot_data.columns,\n",
        "                                      index=plot_data.index)\n",
        "\n",
        "    for feature in important_features:\n",
        "        st.subheader(f\"Relaci√≥n entre {feature} y SalePrice\")\n",
        "\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            fig, ax = plt.subplots()\n",
        "            sns.scatterplot(data=plot_data, x=feature, y=\"SalePrice\", ax=ax)\n",
        "            ax.set_title(f\"Original\")\n",
        "            st.pyplot(fig)\n",
        "\n",
        "        with col2:\n",
        "            fig, ax = plt.subplots()\n",
        "            sns.scatterplot(data=plot_data_normalized, x=feature, y=\"SalePrice\", ax=ax)\n",
        "            ax.set_title(f\"Normalizado\")\n",
        "            st.pyplot(fig)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# 6. Limpieza de datos\n",
        "# ===============================\n",
        "st.header(\"üßπ Limpieza de Datos\")\n",
        "\n",
        "if st.checkbox('Realizar limpieza de datos'):\n",
        "    # Guardar el estado original para comparaci√≥n\n",
        "    original_shape = Xdata.shape\n",
        "    original_columns = Xdata.columns.tolist()\n",
        "\n",
        "    # Realizar limpieza\n",
        "    Xdata = Xdata.sample(frac=0.20, random_state=42)\n",
        "    cols_to_drop = ['Order', 'PID']\n",
        "    Xdata.drop(columns=[col for col in cols_to_drop if col in Xdata.columns], inplace=True)\n",
        "\n",
        "    high_null_cols = Xdata.columns[Xdata.isnull().mean() > 0.4].tolist()\n",
        "    Xdata.drop(columns=high_null_cols, inplace=True)\n",
        "\n",
        "    st.success(\"Limpieza completada!\")\n",
        "\n",
        "    # Mostrar comparaci√≥n visual\n",
        "    st.subheader(\"üìä Comparaci√≥n Antes/Despu√©s de la Limpieza\")\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        st.markdown(\"**Antes de la limpieza**\")\n",
        "        st.metric(\"N√∫mero de filas\", original_shape[0])\n",
        "        st.metric(\"N√∫mero de columnas\", original_shape[1])\n",
        "        st.write(\"Ejemplo de datos:\")\n",
        "        st.dataframe(df.head(3))\n",
        "\n",
        "    with col2:\n",
        "        st.markdown(\"**Despu√©s de la limpieza**\")\n",
        "        st.metric(\"N√∫mero de filas\", Xdata.shape[0], delta=f\"{-((original_shape[0]-Xdata.shape[0])/original_shape[0]*100):.1f}%\")\n",
        "        st.metric(\"N√∫mero de columnas\", Xdata.shape[1], delta=f\"{-((original_shape[1]-Xdata.shape[1])/original_shape[1]*100):.1f}%\")\n",
        "        st.write(\"Ejemplo de datos limpios:\")\n",
        "        st.dataframe(Xdata.head(3))\n",
        "\n",
        "    # Mostrar columnas eliminadas y conservadas\n",
        "    st.subheader(\"üîç Detalles de la Limpieza\")\n",
        "\n",
        "    dropped_columns = list(set(original_columns) - set(Xdata.columns))\n",
        "    st.write(f\"‚úÖ Columnas conservadas: {len(Xdata.columns)}\")\n",
        "    st.write(f\"‚ùå Columnas eliminadas: {len(dropped_columns)}\")\n",
        "\n",
        "    if dropped_columns:\n",
        "        st.write(\"Columnas eliminadas:\")\n",
        "        st.write(dropped_columns)\n",
        "\n",
        "    # Visualizaci√≥n de valores nulos despu√©s de la limpieza\n",
        "    st.subheader(\"üîé Valores Nulos Restantes\")\n",
        "    missing_after = Xdata.isnull().sum()\n",
        "    missing_after = missing_after[missing_after > 0].sort_values(ascending=False)\n",
        "\n",
        "    if not missing_after.empty:\n",
        "        st.bar_chart(missing_after)\n",
        "        st.write(\"Nota: A√∫n quedan algunas columnas con valores nulos que podr√≠an necesitar tratamiento adicional.\")\n",
        "    else:\n",
        "        st.success(\"¬°No hay valores nulos restantes en el dataset!\")\n",
        "\n",
        "    # Mostrar estructura final del dataset\n",
        "    st.subheader(\"üèÅ Estructura Final del Dataset\")\n",
        "    st.write(\"Tipos de variables en el dataset limpio:\")\n",
        "    st.write(Xdata.dtypes.value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxUAoIf7FA-D",
        "outputId": "dc58f504-bfe9-4fb2-d030-c593e72178d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 1_üìä_ANALISIS_EXPLORATORIO.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 1_üìä_ANALISIS_EXPLORATORIO.py pages/"
      ],
      "metadata": {
        "id": "hAcbRIJVGuAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pre-Seleccion de modelos**"
      ],
      "metadata": {
        "id": "_S1jtSyFJ84i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 2_üîç_Preseleccion_de_modelo.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet, BayesianRidge, SGDRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "import kagglehub\n",
        "\n",
        "st.set_page_config(page_title=\"Modelados Predictivos\", page_icon=\"üìä\", layout=\"wide\")\n",
        "st.markdown(\"Modelos preditivos\")\n",
        "st.sidebar.header(\"Pre-seleccion de modelo\")\n",
        "\n",
        "# ===============================\n",
        "# 1. Carga y Preparaci√≥n de Datos\n",
        "# ===============================\n",
        "st.header(\"üè° Carga y Preparaci√≥n de Datos\")\n",
        "\n",
        "@st.cache_data\n",
        "def load_and_prepare_data():\n",
        "    with st.spinner('Descargando y preparando datos...'):\n",
        "        path = kagglehub.dataset_download(\"shashanknecrothapa/ames-housing-dataset\")\n",
        "        csv_file_path = os.path.join(path, \"AmesHousing.csv\")\n",
        "        Xdata = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Limpieza de datos\n",
        "        Xdata = Xdata.sample(frac=0.20, random_state=42)\n",
        "        cols_to_drop = ['Order', 'PID']\n",
        "        Xdata.drop(columns=[col for col in cols_to_drop if col in Xdata.columns], inplace=True)\n",
        "        high_null_cols = Xdata.columns[Xdata.isnull().mean() > 0.4].tolist()\n",
        "        Xdata.drop(columns=high_null_cols, inplace=True)\n",
        "\n",
        "        return Xdata\n",
        "\n",
        "Xdata = load_and_prepare_data()\n",
        "\n",
        "# Mostrar datos\n",
        "if st.checkbox('Mostrar datos preparados'):\n",
        "    st.dataframe(Xdata.head())\n",
        "\n",
        "# ===============================\n",
        "# 2. Transformaci√≥n de Variables\n",
        "# ===============================\n",
        "st.header(\"üìê Transformaci√≥n de Variables\")\n",
        "\n",
        "col_sal = \"SalePrice\"\n",
        "\n",
        "# Selector para visualizar transformaci√≥n\n",
        "transform_col = st.selectbox(\"Seleccione columna para visualizar transformaci√≥n\",\n",
        "                            options=Xdata.select_dtypes(include=['int64', 'float64']).columns)\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(Xdata[transform_col], kde=True)\n",
        "    plt.title(f'Distribuci√≥n Original de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "with col2:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(np.log1p(Xdata[transform_col]), kde=True)\n",
        "    plt.title(f'Distribuci√≥n con log1p de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "st.info(\"\"\"\n",
        "üí° **Transformaci√≥n logar√≠tmica (log1p):**\n",
        "- Se aplica para manejar distribuciones sesgadas\n",
        "- log1p = log(1 + x) evita problemas con valores cero\n",
        "- Ayuda a cumplir supuestos de normalidad en modelos lineales\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 3. Divisi√≥n de Datos\n",
        "# ===============================\n",
        "st.header(\"‚úÇÔ∏è Divisi√≥n del Dataset\")\n",
        "\n",
        "test_size = st.slider(\"Porcentaje para test\", 10, 40, 30, 5)\n",
        "\n",
        "Xtrain, Xtest = train_test_split(Xdata, test_size=test_size/100, random_state=42)\n",
        "ytrain = np.log1p(Xtrain[col_sal])\n",
        "ytest = np.log1p(Xtest[col_sal])\n",
        "Xtrain = Xtrain.drop(columns=col_sal)\n",
        "Xtest = Xtest.drop(columns=col_sal)\n",
        "\n",
        "st.success(f\"\"\"\n",
        "Divisi√≥n completada:\n",
        "- Entrenamiento: {Xtrain.shape[0]} registros ({100-test_size}%)\n",
        "- Prueba: {Xtest.shape[0]} registros ({test_size}%)\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 4. Preprocesamiento\n",
        "# ===============================\n",
        "st.header(\"üîß Pipeline de Preprocesamiento\")\n",
        "\n",
        "numeric_cols = Xtrain.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = Xtrain.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "with st.expander(\"Ver detalles de preprocesamiento\"):\n",
        "    st.write(\"**Columnas num√©ricas:**\")\n",
        "    st.write(numeric_cols)\n",
        "    st.write(\"**Transformaciones:** Imputaci√≥n con mediana + Estandarizaci√≥n\")\n",
        "\n",
        "    st.write(\"\\n**Columnas categ√≥ricas:**\")\n",
        "    st.write(categorical_cols)\n",
        "    st.write(\"**Transformaciones:** Imputaci√≥n con 'missing' + One-Hot Encoding\")\n",
        "\n",
        "# ===============================\n",
        "# 5. Selecci√≥n de Modelos\n",
        "# ===============================\n",
        "st.header(\"ü§ñ Selecci√≥n de Modelos\")\n",
        "\n",
        "# Configuraci√≥n de modelos\n",
        "models = {\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"Lasso\": Lasso(alpha=0.1),\n",
        "    \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
        "    \"KernelRidge\": KernelRidge(alpha=1.0),\n",
        "    \"SGDRegressor\": SGDRegressor(max_iter=1000, tol=1e-3),\n",
        "    \"BayesianRidge\": BayesianRidge(),\n",
        "    \"GaussianProcess\": GaussianProcessRegressor(),\n",
        "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"SVR\": SVR()\n",
        "}\n",
        "\n",
        "# Permitir selecci√≥n de modelos\n",
        "selected_models = st.multiselect(\n",
        "    \"Seleccione modelos a evaluar\",\n",
        "    options=list(models.keys()),\n",
        "    default=[\"LinearRegression\", \"RandomForest\", \"Lasso\"]\n",
        ")\n",
        "\n",
        "# Configuraci√≥n de m√©tricas\n",
        "scoring = {\n",
        "    'MAE': 'neg_mean_absolute_error',\n",
        "    'MSE': 'neg_mean_squared_error',\n",
        "    'R2': 'r2',\n",
        "    'MAPE': 'neg_mean_absolute_percentage_error'\n",
        "}\n",
        "\n",
        "# ===============================\n",
        "# 6. Evaluaci√≥n de Modelos (Versi√≥n Mejorada)\n",
        "# ===============================\n",
        "if st.button(\"Evaluar Modelos\"):\n",
        "    st.header(\"üìä Resultados de Evaluaci√≥n\")\n",
        "\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    # Estructura para almacenar resultados\n",
        "    results = {\n",
        "        'Modelo': [],\n",
        "        'MAE_mean': [], 'MAE_std': [],\n",
        "        'MSE_mean': [], 'MSE_std': [],\n",
        "        'R2_mean': [], 'R2_std': [],\n",
        "        'MAPE_mean': [], 'MAPE_std': []\n",
        "    }\n",
        "\n",
        "    for i, (name, regressor) in enumerate([(m, models[m]) for m in selected_models]):\n",
        "        status_text.text(f\"Evaluando {name}...\")\n",
        "        progress_bar.progress((i+1)/len(selected_models))\n",
        "\n",
        "        model = Pipeline(steps=[\n",
        "            (\"preprocessing\", preprocessor),\n",
        "            (\"regressor\", regressor)\n",
        "        ])\n",
        "\n",
        "        cv_results = cross_validate(model, Xtrain, ytrain, cv=5, scoring=scoring)\n",
        "\n",
        "        # Almacenar resultados con media y desviaci√≥n est√°ndar\n",
        "        results['Modelo'].append(name)\n",
        "\n",
        "        # MAE\n",
        "        results['MAE_mean'].append(-cv_results['test_MAE'].mean())\n",
        "        results['MAE_std'].append(cv_results['test_MAE'].std())\n",
        "\n",
        "        # MSE\n",
        "        results['MSE_mean'].append(-cv_results['test_MSE'].mean())\n",
        "        results['MSE_std'].append(cv_results['test_MSE'].std())\n",
        "\n",
        "        # R2\n",
        "        results['R2_mean'].append(cv_results['test_R2'].mean())\n",
        "        results['R2_std'].append(cv_results['test_R2'].std())\n",
        "\n",
        "        # MAPE (convertido a porcentaje)\n",
        "        results['MAPE_mean'].append(-cv_results['test_MAPE'].mean() * 100)\n",
        "        results['MAPE_std'].append(cv_results['test_MAPE'].std() * 100)\n",
        "\n",
        "    # Crear DataFrame con los resultados\n",
        "    results_df = pd.DataFrame(results).set_index('Modelo')\n",
        "\n",
        "    # Mostrar resultados en una tabla expandible\n",
        "    with st.expander(\"üìã Resultados Detallados (Media ¬± Desviaci√≥n Est√°ndar)\", expanded=True):\n",
        "        # Crear representaci√≥n de cadenas para media ¬± std\n",
        "        display_df = pd.DataFrame(index=results_df.index)\n",
        "\n",
        "        for metric in ['MAE', 'MSE', 'R2', 'MAPE']:\n",
        "            display_df[metric] = results_df.apply(\n",
        "                lambda x: f\"{x[f'{metric}_mean']:.4f} ¬± {x[f'{metric}_std']:.4f}\",\n",
        "                axis=1\n",
        "            )\n",
        "            # Formato especial para MAPE (porcentaje)\n",
        "            if metric == 'MAPE':\n",
        "                display_df[metric] = results_df.apply(\n",
        "                    lambda x: f\"{x[f'{metric}_mean']:.2f}% ¬± {x[f'{metric}_std']:.2f}%\",\n",
        "                    axis=1\n",
        "                )\n",
        "\n",
        "        st.dataframe(display_df.style.background_gradient(cmap='Blues', axis=0))\n",
        "\n",
        "    # Visualizaci√≥n de resultados con intervalos de confianza\n",
        "    st.subheader(\"üìà Comparaci√≥n Visual con Variabilidad\")\n",
        "\n",
        "    metric_to_plot = st.selectbox(\"Seleccione m√©trica para visualizar\",\n",
        "                                options=['MAE', 'MSE', 'R2', 'MAPE'])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Ordenar modelos por la m√©trica seleccionada\n",
        "    sorted_models = results_df[f'{metric_to_plot}_mean'].sort_values().index\n",
        "\n",
        "    # Crear gr√°fico de barras con barras de error\n",
        "    y_pos = range(len(sorted_models))\n",
        "    means = results_df.loc[sorted_models, f'{metric_to_plot}_mean']\n",
        "    stds = results_df.loc[sorted_models, f'{metric_to_plot}_std']\n",
        "\n",
        "    bars = ax.barh(y_pos, means, xerr=stds, align='center', alpha=0.7, capsize=5)\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(sorted_models)\n",
        "    ax.invert_yaxis()  # Mejor modelo en la parte superior\n",
        "\n",
        "    if metric_to_plot == 'MAPE':\n",
        "        ax.set_xlabel(f\"{metric_to_plot} (%)\")\n",
        "    else:\n",
        "        ax.set_xlabel(metric_to_plot)\n",
        "\n",
        "    ax.set_title(f\"Comparaci√≥n de {metric_to_plot} entre Modelos\\n(con desviaci√≥n est√°ndar)\")\n",
        "\n",
        "    # A√±adir valores num√©ricos a las barras\n",
        "    for bar, mean, std in zip(bars, means, stds):\n",
        "        if metric_to_plot == 'MAPE':\n",
        "            ax.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{mean:.2f} ¬± {std:.2f}',\n",
        "                   va='center', ha='left')\n",
        "        else:\n",
        "            ax.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{mean:.4f} ¬± {std:.4f}',\n",
        "                   va='center', ha='left')\n",
        "\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # An√°lisis de estabilidad de modelos\n",
        "    st.subheader(\"üîç An√°lisis de Estabilidad de Modelos\")\n",
        "\n",
        "    # Calcular coeficiente de variaci√≥n para cada modelo (std/mean)\n",
        "    stability_df = pd.DataFrame(index=results_df.index)\n",
        "    for metric in ['MAE', 'MSE', 'R2', 'MAPE']:\n",
        "        stability_df[f'CV_{metric}'] = results_df[f'{metric}_std'] / results_df[f'{metric}_mean']\n",
        "\n",
        "    # Identificar modelos m√°s estables (menor variabilidad relativa)\n",
        "    st.write(\"**Coeficiente de Variaci√≥n (CV = œÉ/Œº) - Menor es mejor:**\")\n",
        "    st.dataframe(stability_df.style.background_gradient(cmap='Greens_r', axis=0))\n",
        "\n",
        "    st.info(\"\"\"\n",
        "    **Interpretaci√≥n:**\n",
        "    - La desviaci√≥n est√°ndar muestra cu√°nto var√≠an los resultados entre los folds de validaci√≥n cruzada\n",
        "    - Un modelo con baja desviaci√≥n est√°ndar es m√°s consistente\n",
        "    - El coeficiente de variaci√≥n (CV) normaliza la variabilidad respecto a la media\n",
        "    \"\"\")\n",
        "\n",
        "    # Recomendaci√≥n del mejor modelo considerando media y variabilidad\n",
        "    best_model_mean = results_df['R2_mean'].idxmax()\n",
        "    best_model_stable = stability_df['CV_R2'].idxmin()\n",
        "\n",
        "    if best_model_mean == best_model_stable:\n",
        "        st.success(f\"üéØ **Mejor modelo:** {best_model_mean} (Mayor R¬≤: {results_df.loc[best_model_mean, 'R2_mean']:.4f} y menor variabilidad)\")\n",
        "    else:\n",
        "        st.success(f\"\"\"\n",
        "        üéØ **Recomendaciones:**\n",
        "        - **Mejor rendimiento:** {best_model_mean} (R¬≤ = {results_df.loc[best_model_mean, 'R2_mean']:.4f})\n",
        "        - **M√°s estable:** {best_model_stable} (CV = {stability_df.loc[best_model_stable, 'CV_R2']:.4f})\n",
        "        \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXQIXpGdaVTd",
        "outputId": "12195497-141e-437f-d71e-54bb0d56065e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 2_üîç_Preseleccion_de_modelo.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 2_üîç_Preseleccion_de_modelo.py pages/"
      ],
      "metadata": {
        "id": "UEoFgycSa7Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. üîó Elastic Net: Funci√≥n de optimizaci√≥n**\n",
        "\n",
        "El modelo **Elastic Net** busca minimizar la siguiente funci√≥n objetivo:\n",
        "\n",
        "$$\n",
        "\\min_{\\beta} \\; \\frac{1}{2n} \\| y - X\\beta \\|_2^2 + \\lambda \\left( \\alpha \\|\\beta\\|_1 + \\frac{1 - \\alpha}{2} \\|\\beta\\|_2^2 \\right)\n",
        "$$\n",
        "\n",
        "**Donde:**\n",
        "- $X \\in \\mathbb{R}^{n \\times p} \\$ : matriz de caracter√≠sticas (n muestras, p variables).\n",
        "\n",
        "\n",
        "- $y \\in \\mathbb{R}^{n} \\$  : vector de salida.\n",
        "\n",
        "\n",
        "- $\\beta \\in \\mathbb{R}^{p} \\$  : vector de coeficientes a estimar.\n",
        "\n",
        "\n",
        "- $ \\lambda \\geq 0 \\$  : par√°metro de regularizaci√≥n total.\n",
        "\n",
        "\n",
        "- $ \\alpha \\in [0, 1] \\$  : coeficiente de mezcla entre L1 y L2.\n",
        "\n",
        "\n",
        "**Interpretaci√≥n:**\n",
        "\n",
        "- Si $\\alpha=1$, Elastic Net es equivalente a **Lasso**.\n",
        "\n",
        "- Si $ \\alpha = 0 \\$, es equivalente a **Ridge**.\n",
        "\n",
        "- Si $ \\alpha \\in (0, 1) $, se obtiene una combinaci√≥n convexa de ambos m√©todos.\n",
        "\n"
      ],
      "metadata": {
        "id": "qgwtdVsZKNO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 3_ElasticNet.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from scipy.stats import loguniform, uniform\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_contour\n",
        "\n",
        "st.set_page_config(page_title=\"Optimizaci√≥n de Hiperpar√°metros\", page_icon=\"‚öôÔ∏è\", layout=\"wide\")\n",
        "\n",
        "# ===============================\n",
        "# 1. Carga y Preparaci√≥n de Datos\n",
        "# ===============================\n",
        "@st.cache_data\n",
        "def load_and_prepare_data():\n",
        "    with st.spinner('Descargando y preparando datos...'):\n",
        "        path = kagglehub.dataset_download(\"shashanknecrothapa/ames-housing-dataset\")\n",
        "        csv_file_path = os.path.join(path, \"AmesHousing.csv\")\n",
        "        Xdata = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Limpieza de datos\n",
        "        Xdata = Xdata.sample(frac=0.20, random_state=42)\n",
        "        cols_to_drop = ['Order', 'PID']\n",
        "        Xdata.drop(columns=[col for col in cols_to_drop if col in Xdata.columns], inplace=True)\n",
        "        high_null_cols = Xdata.columns[Xdata.isnull().mean() > 0.4].tolist()\n",
        "        Xdata.drop(columns=high_null_cols, inplace=True)\n",
        "\n",
        "        return Xdata\n",
        "\n",
        "Xdata = load_and_prepare_data()\n",
        "\n",
        "# ===============================\n",
        "# 2. Transformaci√≥n de Variables\n",
        "# ===============================\n",
        "st.header(\"üìê Transformaci√≥n de Variables\")\n",
        "\n",
        "col_sal = \"SalePrice\"\n",
        "\n",
        "# Selector para visualizar transformaci√≥n\n",
        "transform_col = st.selectbox(\"Seleccione columna para visualizar transformaci√≥n\",\n",
        "                            options=Xdata.select_dtypes(include=['int64', 'float64']).columns)\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(Xdata[transform_col], kde=True)\n",
        "    plt.title(f'Distribuci√≥n Original de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "with col2:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(np.log1p(Xdata[transform_col]), kde=True)\n",
        "    plt.title(f'Distribuci√≥n con log1p de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "st.info(\"\"\"\n",
        "üí° **Transformaci√≥n logar√≠tmica (log1p):**\n",
        "- Se aplica para manejar distribuciones sesgadas\n",
        "- log1p = log(1 + x) evita problemas con valores cero\n",
        "- Ayuda a cumplir supuestos de normalidad en modelos lineales\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 3. Divisi√≥n de Datos\n",
        "# ===============================\n",
        "st.header(\"‚úÇÔ∏è Divisi√≥n del Dataset\")\n",
        "\n",
        "test_size = st.slider(\"Porcentaje para test\", 10, 40, 30, 5)\n",
        "\n",
        "Xtrain, Xtest = train_test_split(Xdata, test_size=test_size/100, random_state=42)\n",
        "ytrain = np.log1p(Xtrain[col_sal])\n",
        "ytest = np.log1p(Xtest[col_sal])\n",
        "Xtrain = Xtrain.drop(columns=col_sal)\n",
        "Xtest = Xtest.drop(columns=col_sal)\n",
        "\n",
        "st.success(f\"\"\"\n",
        "Divisi√≥n completada:\n",
        "- Entrenamiento: {Xtrain.shape[0]} registros ({100-test_size}%)\n",
        "- Prueba: {Xtest.shape[0]} registros ({test_size}%)\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 4. Preprocesamiento\n",
        "# ===============================\n",
        "st.header(\"üîß Pipeline de Preprocesamiento\")\n",
        "\n",
        "numeric_cols = Xtrain.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = Xtrain.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "with st.expander(\"Ver detalles de preprocesamiento\"):\n",
        "    st.write(\"**Columnas num√©ricas:**\")\n",
        "    st.write(numeric_cols)\n",
        "    st.write(\"**Transformaciones:** Imputaci√≥n con mediana + Estandarizaci√≥n\")\n",
        "\n",
        "    st.write(\"\\n**Columnas categ√≥ricas:**\")\n",
        "    st.write(categorical_cols)\n",
        "    st.write(\"**Transformaciones:** Imputaci√≥n con 'missing' + One-Hot Encoding\")\n",
        "\n",
        "# ===============================\n",
        "# 5. Optimizaci√≥n de Hiperpar√°metros\n",
        "# ===============================\n",
        "st.header(\"‚öôÔ∏è Optimizaci√≥n de Hiperpar√°metros - ElasticNet\")\n",
        "\n",
        "# Configuraci√≥n com√∫n\n",
        "cv = st.sidebar.slider(\"N√∫mero de folds para CV\", 3, 10, 3)\n",
        "random_state = st.sidebar.number_input(\"Random state\", 42)\n",
        "scoring = 'neg_mean_squared_error'\n",
        "\n",
        "# Configuraci√≥n de par√°metros\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.subheader(\"Grid Search\")\n",
        "    alpha_min = st.number_input(\"Alpha m√≠nimo (log)\", -4, 2, -4)\n",
        "    alpha_max = st.number_input(\"Alpha m√°ximo (log)\", -4, 2, 2)\n",
        "    alpha_points = st.slider(\"Puntos para alpha\", 5, 20, 10)\n",
        "\n",
        "with col2:\n",
        "    st.subheader(\"Random Search\")\n",
        "    n_iter = st.slider(\"N√∫mero de iteraciones\", 10, 100, 20)\n",
        "    bayesian_trials = st.slider(\"N√∫mero de trials Bayesianos\", 10, 100, 20)\n",
        "\n",
        "if st.button(\"Ejecutar Optimizaci√≥n\"):\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    # Preparar par√°metros\n",
        "    param_grid = {\n",
        "        'regressor__alpha': np.logspace(alpha_min, alpha_max, alpha_points),\n",
        "        'regressor__l1_ratio': np.linspace(0, 1, 10)\n",
        "    }\n",
        "\n",
        "    param_dist = {\n",
        "        'regressor__alpha': loguniform(1e-4, 1e2),\n",
        "        'regressor__l1_ratio': uniform(0, 1)\n",
        "    }\n",
        "\n",
        "    # 1. Grid Search\n",
        "    status_text.text(\"Ejecutando Grid Search...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        Pipeline(steps=[('preprocessing', preprocessor), ('regressor', ElasticNet())]),\n",
        "        param_grid, scoring=scoring, cv=cv\n",
        "    )\n",
        "    grid_search.fit(Xtrain, ytrain)\n",
        "    grid_results = [\n",
        "        (params['regressor__alpha'], params['regressor__l1_ratio'], -score)\n",
        "        for params, score in zip(grid_search.cv_results_['params'],\n",
        "                               grid_search.cv_results_['mean_test_score'])\n",
        "    ]\n",
        "    progress_bar.progress(33)\n",
        "\n",
        "    # 2. Random Search\n",
        "    status_text.text(\"Ejecutando Random Search...\")\n",
        "    random_search = RandomizedSearchCV(\n",
        "        Pipeline(steps=[('preprocessing', preprocessor), ('regressor', ElasticNet())]),\n",
        "        param_dist, n_iter=n_iter, scoring=scoring, cv=cv, random_state=random_state\n",
        "    )\n",
        "    random_search.fit(Xtrain, ytrain)\n",
        "    random_results = [\n",
        "        (params['regressor__alpha'], params['regressor__l1_ratio'], -score)\n",
        "        for params, score in zip(random_search.cv_results_['params'],\n",
        "                               random_search.cv_results_['mean_test_score'])\n",
        "    ]\n",
        "    progress_bar.progress(66)\n",
        "\n",
        "    # 3. Bayesian Optimization\n",
        "    status_text.text(\"Ejecutando Bayesian Optimization...\")\n",
        "\n",
        "    def objective_elasticnet(trial):\n",
        "        alpha = trial.suggest_float('alpha', 1e-4, 1e2, log=True)\n",
        "        l1_ratio = trial.suggest_float('l1_ratio', 0, 1)\n",
        "        model = Pipeline(steps=[\n",
        "            ('preprocessing', preprocessor),\n",
        "            ('regressor', ElasticNet(alpha=alpha, l1_ratio=l1_ratio))\n",
        "        ])\n",
        "        try:\n",
        "            return -cross_val_score(model, Xtrain, ytrain, scoring=scoring, cv=cv).mean()\n",
        "        except:\n",
        "            return float('inf')\n",
        "\n",
        "    study_elasticnet = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
        "    study_elasticnet.optimize(objective_elasticnet, n_trials=bayesian_trials)\n",
        "    progress_bar.progress(100)\n",
        "\n",
        "    # Almacenar resultados\n",
        "    best_params = {\n",
        "        'GridSearch': grid_search.best_params_,\n",
        "        'RandomSearch': random_search.best_params_,\n",
        "        'Bayesian': study_elasticnet.best_params\n",
        "    }\n",
        "\n",
        "    # ===============================\n",
        "    # 6. Visualizaci√≥n de Resultados\n",
        "    # ===============================\n",
        "    st.success(\"Optimizaci√≥n completada!\")\n",
        "\n",
        "    # Mostrar mejores par√°metros\n",
        "    st.subheader(\"üèÜ Mejores Par√°metros Encontrados\")\n",
        "    cols = st.columns(3)\n",
        "    with cols[0]:\n",
        "        st.metric(\"Grid Search - Alpha\", best_params['GridSearch']['regressor__alpha'])\n",
        "        st.metric(\"Grid Search - L1 Ratio\", best_params['GridSearch']['regressor__l1_ratio'])\n",
        "    with cols[1]:\n",
        "        st.metric(\"Random Search - Alpha\", best_params['RandomSearch']['regressor__alpha'])\n",
        "        st.metric(\"Random Search - L1 Ratio\", best_params['RandomSearch']['regressor__l1_ratio'])\n",
        "    with cols[2]:\n",
        "        st.metric(\"Bayesian - Alpha\", best_params['Bayesian']['alpha'])\n",
        "        st.metric(\"Bayesian - L1 Ratio\", best_params['Bayesian']['l1_ratio'])\n",
        "\n",
        "    # Gr√°ficos de comparaci√≥n\n",
        "    st.subheader(\"üìä Comparaci√≥n de M√©todos de Optimizaci√≥n\")\n",
        "\n",
        "    tab1, tab2, tab3 = st.tabs([\"Grid Search\", \"Random Search\", \"Bayesian Optimization\"])\n",
        "\n",
        "    with tab1:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        x_values = [r[0] for r in grid_results]\n",
        "        y_values = [r[1] for r in grid_results]\n",
        "        scores = [r[2] for r in grid_results]\n",
        "        scatter = ax.scatter(x_values, y_values, c=scores, cmap='viridis')\n",
        "        plt.colorbar(scatter, ax=ax, label='MSE')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_xlabel('alpha')\n",
        "        ax.set_ylabel('l1_ratio')\n",
        "        ax.set_title('Grid Search - ElasticNet')\n",
        "        ax.grid(True, which='both', ls='--')\n",
        "        st.pyplot(fig)\n",
        "        st.write(f\"Mejor MSE: {-grid_search.best_score_:.4f}\")\n",
        "\n",
        "    with tab2:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        x_values = [r[0] for r in random_results]\n",
        "        y_values = [r[1] for r in random_results]\n",
        "        scores = [r[2] for r in random_results]\n",
        "        scatter = ax.scatter(x_values, y_values, c=scores, cmap='viridis')\n",
        "        plt.colorbar(scatter, ax=ax, label='MSE')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_xlabel('alpha')\n",
        "        ax.set_ylabel('l1_ratio')\n",
        "        ax.set_title('Random Search - ElasticNet')\n",
        "        ax.grid(True, which='both', ls='--')\n",
        "        st.pyplot(fig)\n",
        "        st.write(f\"Mejor MSE: {-random_search.best_score_:.4f}\")\n",
        "\n",
        "    with tab3:\n",
        "        st.plotly_chart(plot_optimization_history(study_elasticnet))\n",
        "        st.plotly_chart(plot_param_importances(study_elasticnet))\n",
        "        st.plotly_chart(plot_contour(study_elasticnet, params=[\"alpha\", \"l1_ratio\"]))\n",
        "        st.write(f\"Mejor MSE: {study_elasticnet.best_value:.4f}\")\n",
        "\n",
        "    # An√°lisis comparativo\n",
        "    st.subheader(\"üîç An√°lisis Comparativo Interactivo\")\n",
        "\n",
        "    # Crear un diccionario con todas las m√©tricas disponibles\n",
        "    metrics_data = {\n",
        "        'MAE': {\n",
        "            'GridSearch': mean_absolute_error(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': mean_absolute_error(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': mean_absolute_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', ElasticNet(**study_elasticnet.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'MSE': {\n",
        "            'GridSearch': mean_squared_error(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': mean_squared_error(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': mean_squared_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', ElasticNet(**study_elasticnet.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'R2': {\n",
        "            'GridSearch': r2_score(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': r2_score(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': r2_score(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', ElasticNet(**study_elasticnet.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'MAPE': {\n",
        "            'GridSearch': mean_absolute_percentage_error(ytrain, grid_search.best_estimator_.predict(Xtrain)) * 100,\n",
        "            'RandomSearch': mean_absolute_percentage_error(ytrain, random_search.best_estimator_.predict(Xtrain)) * 100,\n",
        "            'Bayesian': mean_absolute_percentage_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', ElasticNet(**study_elasticnet.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain)) * 100\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Selector de m√©trica\n",
        "    selected_metric = st.selectbox(\n",
        "        \"Seleccione la m√©trica a visualizar:\",\n",
        "        options=['MAE', 'MSE', 'R2', 'MAPE'],\n",
        "        index=1  # MSE por defecto\n",
        "    )\n",
        "\n",
        "    # Configuraci√≥n de visualizaci√≥n seg√∫n la m√©trica\n",
        "    if selected_metric == 'MAPE':\n",
        "        ylabel = 'MAPE (%)'\n",
        "        title = f'Comparaci√≥n de {selected_metric} entre M√©todos'\n",
        "        fmt = '.2f%'\n",
        "        ascending = True\n",
        "    elif selected_metric == 'R2':\n",
        "        ylabel = 'R¬≤'\n",
        "        title = f'Comparaci√≥n de {selected_metric} entre M√©todos'\n",
        "        fmt = '.4f'\n",
        "        ascending = False\n",
        "    else:\n",
        "        ylabel = selected_metric\n",
        "        title = f'Comparaci√≥n de {selected_metric} entre M√©todos'\n",
        "        fmt = '.4f'\n",
        "        ascending = True\n",
        "\n",
        "    # Ordenar m√©todos seg√∫n el rendimiento\n",
        "    methods = pd.DataFrame({\n",
        "        'Method': ['GridSearch', 'RandomSearch', 'Bayesian'],\n",
        "        'Value': [metrics_data[selected_metric]['GridSearch'],\n",
        "                metrics_data[selected_metric]['RandomSearch'],\n",
        "                metrics_data[selected_metric]['Bayesian']]\n",
        "    }).sort_values('Value', ascending=ascending)\n",
        "\n",
        "    # Crear la figura\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    colors = ['skyblue', 'lightgreen', 'salmon']\n",
        "    bars = ax.bar(methods['Method'], methods['Value'], color=colors)\n",
        "\n",
        "    # Configurar el gr√°fico\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # A√±adir los valores a las barras\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        if selected_metric == 'MAPE':\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.2f}%',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "        else:\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:{fmt}}',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    # Rotar etiquetas del eje x para mejor legibilidad\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Mostrar el gr√°fico en Streamlit\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Mostrar tabla con todas las m√©tricas\n",
        "    st.subheader(\"üìä Resumen de M√©tricas\")\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "    metrics_df['MAPE'] = metrics_df['MAPE'].map('{:.2f}%'.format)\n",
        "    metrics_df['MAE'] = metrics_df['MAE'].map('{:.4f}'.format)\n",
        "    metrics_df['MSE'] = metrics_df['MSE'].map('{:.4f}'.format)\n",
        "    metrics_df['R2'] = metrics_df['R2'].map('{:.4f}'.format)\n",
        "    st.dataframe(metrics_df.style.background_gradient(cmap='Blues', axis=0))\n",
        "\n",
        "    # Mostrar recomendaci√≥n basada en la m√©trica seleccionada\n",
        "    best_method = methods.iloc[0]['Method']\n",
        "    best_value = methods.iloc[0]['Value']\n",
        "\n",
        "    if selected_metric == 'MAPE':\n",
        "        st.success(f\"‚úÖ **Mejor modelo seg√∫n {selected_metric}:** {best_method} ({best_value:.2f}%)\")\n",
        "    elif selected_metric == 'R2':\n",
        "        st.success(f\"‚úÖ **Mejor modelo seg√∫n {selected_metric}:** {best_method} ({best_value:.4f})\")\n",
        "    else:\n",
        "        st.success(f\"‚úÖ **Mejor modelo seg√∫n {selected_metric}:** {best_method} ({best_value:.4f})\")\n",
        "\n",
        "# ===============================\n",
        "# 7. Informaci√≥n Adicional\n",
        "# ===============================\n",
        "with st.expander(\"‚ÑπÔ∏è Instrucciones de Uso\", expanded=True):\n",
        "    st.write(\"\"\"\n",
        "    1. Configura los par√°metros de b√∫squeda en el panel lateral\n",
        "    2. Haz clic en \"Ejecutar Optimizaci√≥n\"\n",
        "    3. Explora los resultados en las diferentes pesta√±as\n",
        "    4. Compara el rendimiento de los diferentes m√©todos\n",
        "\n",
        "    **Tipos de B√∫squeda:**\n",
        "    - **Grid Search:** B√∫squeda exhaustiva en una grilla definida\n",
        "    - **Random Search:** Muestreo aleatorio del espacio de par√°metros\n",
        "    - **Bayesian Optimization:** Optimizaci√≥n inteligente basada en modelos\n",
        "    \"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 8. Evaluaci√≥n del Modelo en Test\n",
        "# ===============================\n",
        "st.header(\"üìä Evaluaci√≥n del Modelo ElasticNet en Datos de Test\")\n",
        "\n",
        "# Seleccionar el mejor modelo (usaremos el de Bayesian Optimization por defecto)\n",
        "best_params = study_elasticnet.best_params\n",
        "final_model = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('regressor', ElasticNet(**best_params))\n",
        "])\n",
        "final_model.fit(Xtrain, ytrain)\n",
        "\n",
        "# Predecir en test\n",
        "ypred = final_model.predict(Xtest)\n",
        "\n",
        "# Calcular m√©tricas\n",
        "test_mae = mean_absolute_error(ytest, ypred)\n",
        "test_mse = mean_squared_error(ytest, ypred)\n",
        "test_r2 = r2_score(ytest, ypred)\n",
        "test_mape = mean_absolute_percentage_error(ytest, ypred) * 100\n",
        "\n",
        "# Mostrar m√©tricas\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "with col1:\n",
        "    st.metric(\"MAE (Test)\", f\"{test_mae:.4f}\")\n",
        "with col2:\n",
        "    st.metric(\"MSE (Test)\", f\"{test_mse:.4f}\")\n",
        "with col3:\n",
        "    st.metric(\"R¬≤ (Test)\", f\"{test_r2:.4f}\")\n",
        "with col4:\n",
        "    st.metric(\"MAPE (Test)\", f\"{test_mape:.2f}%\")\n",
        "\n",
        "# Gr√°ficos de evaluaci√≥n\n",
        "st.subheader(\"üîç Gr√°ficos de Evaluaci√≥n\")\n",
        "\n",
        "tab1, tab2, tab3, tab4 = st.tabs([\"Predicciones vs Reales\", \"Residuos\", \"Distribuci√≥n de Errores\", \"Importancia de Variables\"])\n",
        "\n",
        "with tab1:\n",
        "    # Gr√°fico de predicciones vs valores reales\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(ytest, ypred, alpha=0.5)\n",
        "    ax.plot([ytest.min(), ytest.max()], [ytest.min(), ytest.max()], 'k--', lw=2)\n",
        "    ax.set_xlabel('Valores Reales (log1p)')\n",
        "    ax.set_ylabel('Predicciones (log1p)')\n",
        "    ax.set_title('Predicciones vs Valores Reales (ElasticNet)')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretaci√≥n:**\n",
        "    - Los puntos deber√≠an estar cerca de la l√≠nea diagonal\n",
        "    - Dispersi√≥n sim√©trica indica buen ajuste\n",
        "    - ElasticNet tiende a ser m√°s conservador que KernelRidge en sus predicciones\n",
        "    \"\"\")\n",
        "\n",
        "with tab2:\n",
        "    # Gr√°fico de residuos\n",
        "    residuals = ytest - ypred\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(ypred, residuals, alpha=0.5)\n",
        "    ax.axhline(y=0, color='r', linestyle='--')\n",
        "    ax.set_xlabel('Predicciones (log1p)')\n",
        "    ax.set_ylabel('Residuos')\n",
        "    ax.set_title('Gr√°fico de Residuos (ElasticNet)')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretaci√≥n:**\n",
        "    - Residuos aleatorios alrededor de cero son deseables\n",
        "    - ElasticNet suele mostrar residuos m√°s homog√©neos que otros modelos\n",
        "    - Patrones no aleatorios pueden indicar relaciones no capturadas\n",
        "    \"\"\")\n",
        "\n",
        "with tab3:\n",
        "    # Distribuci√≥n de errores\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.histplot(residuals, kde=True, ax=ax)\n",
        "    ax.set_xlabel('Error de Predicci√≥n')\n",
        "    ax.set_title('Distribuci√≥n de Errores (ElasticNet)')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretaci√≥n:**\n",
        "    - Distribuci√≥n centrada en cero indica predicciones no sesgadas\n",
        "    - La regularizaci√≥n L1+L2 de ElasticNet suele producir distribuciones m√°s compactas\n",
        "    - Colas pesadas pueden indicar valores at√≠picos problem√°ticos\n",
        "    \"\"\")\n",
        "\n",
        "with tab4:\n",
        "    # Importancia de variables (solo para ElasticNet)\n",
        "    try:\n",
        "        # Obtener los coeficientes del modelo\n",
        "        feature_names = numeric_cols + list(final_model.named_steps['preprocessing'].named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(categorical_cols))\n",
        "        coefficients = final_model.named_steps['regressor'].coef_\n",
        "\n",
        "        # Crear DataFrame con coeficientes\n",
        "        coef_df = pd.DataFrame({\n",
        "            'Variable': feature_names,\n",
        "            'Coeficiente': coefficients\n",
        "        }).sort_values('Coeficiente', key=abs, ascending=False).head(20)\n",
        "\n",
        "        # Gr√°fico de importancia\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        sns.barplot(data=coef_df, x='Coeficiente', y='Variable', ax=ax)\n",
        "        ax.set_title('Top 20 Variables M√°s Importantes (ElasticNet)')\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        st.write(\"\"\"\n",
        "        **Interpretaci√≥n:**\n",
        "        - Muestra las variables con mayor impacto en las predicciones\n",
        "        - ElasticNet realiza selecci√≥n de variables (algunos coeficientes son exactamente cero)\n",
        "        - Coeficientes positivos aumentan el precio predicho, negativos lo disminuyen\n",
        "        \"\"\")\n",
        "    except Exception as e:\n",
        "        st.warning(f\"No se pudo generar el gr√°fico de importancia de variables: {str(e)}\")\n",
        "\n",
        "# Gr√°fico de valores reales vs predichos en escala original\n",
        "st.subheader(\"üí∞ Predicciones en Escala Original (USD)\")\n",
        "\n",
        "# Convertir a escala original\n",
        "ytest_orig = np.expm1(ytest)\n",
        "ypred_orig = np.expm1(ypred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.scatter(ytest_orig, ypred_orig, alpha=0.5)\n",
        "ax.plot([ytest_orig.min(), ytest_orig.max()], [ytest_orig.min(), ytest_orig.max()], 'k--', lw=2)\n",
        "ax.set_xlabel('Valores Reales (USD)')\n",
        "ax.set_ylabel('Predicciones (USD)')\n",
        "ax.set_title('Predicciones vs Valores Reales - Escala Original (ElasticNet)')\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Calcular m√©tricas en escala original\n",
        "mae_orig = mean_absolute_error(ytest_orig, ypred_orig)\n",
        "mape_orig = mean_absolute_percentage_error(ytest_orig, ypred_orig) * 100\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.metric(\"MAE (USD)\", f\"${mae_orig:,.2f}\")\n",
        "with col2:\n",
        "    st.metric(\"MAPE (USD)\", f\"{mape_orig:.2f}%\")\n",
        "\n",
        "# An√°lisis de errores por rango de precio\n",
        "st.subheader(\"üìà An√°lisis de Errores por Rango de Precio\")\n",
        "\n",
        "# Crear categor√≠as de precios\n",
        "price_bins = pd.qcut(ytest_orig, q=5, duplicates='drop')\n",
        "error_analysis = pd.DataFrame({\n",
        "    'Precio Real': ytest_orig,\n",
        "    'Error Absoluto': np.abs(ytest_orig - ypred_orig),\n",
        "    'Rango Precio': price_bins\n",
        "})\n",
        "\n",
        "# Calcular m√©tricas por rango\n",
        "error_by_price = error_analysis.groupby('Rango Precio').agg({\n",
        "    'Precio Real': 'mean',\n",
        "    'Error Absoluto': ['mean', 'median', 'std']\n",
        "}).reset_index()\n",
        "\n",
        "error_by_price.columns = ['Rango Precio', 'Precio Promedio', 'MAE', 'Error Mediano', 'Desviaci√≥n Error']\n",
        "\n",
        "# Gr√°fico de MAE por rango de precio\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.barplot(data=error_by_price, x='Rango Precio', y='MAE', ax=ax)\n",
        "ax.set_title('Error Absoluto Medio por Rango de Precio (ElasticNet)')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "ax.set_ylabel('MAE (USD)')\n",
        "ax.set_xlabel('Rango de Precio')\n",
        "st.pyplot(fig)\n",
        "\n",
        "st.write(\"\"\"\n",
        "**Interpretaci√≥n:**\n",
        "- ElasticNet suele tener un rendimiento m√°s consistente en diferentes rangos de precio\n",
        "- Los errores pueden aumentar en los extremos (propiedades muy baratas o muy caras)\n",
        "- Puede sugerir la necesidad de ajustar los par√°metros de regularizaci√≥n para ciertos rangos\n",
        "\"\"\")\n",
        "\n",
        "# Mostrar tabla de an√°lisis\n",
        "st.dataframe(error_by_price.style.background_gradient(subset=['MAE'], cmap='Reds'))\n",
        "\n",
        "# Comparaci√≥n con modelo naive (promedio)\n",
        "st.subheader(\"ü§î Comparaci√≥n con Modelo Naive (Promedio)\")\n",
        "\n",
        "naive_pred = np.full_like(ytest, ytrain.mean())\n",
        "naive_mae = mean_absolute_error(ytest, naive_pred)\n",
        "improvement = (naive_mae - test_mae) / naive_mae * 100\n",
        "\n",
        "st.metric(\"Mejora sobre modelo naive\", f\"{improvement:.2f}%\",\n",
        "          delta=f\"MAE naive: {naive_mae:.4f}, MAE ElasticNet: {test_mae:.4f}\")\n",
        "\n",
        "st.write(\"\"\"\n",
        "**Interpretaci√≥n:**\n",
        "- Muestra cu√°nto mejor es el modelo ElasticNet respecto a simplemente predecir el promedio\n",
        "- Una mejora positiva indica que el modelo est√° aprendiendo patrones √∫tiles\n",
        "- ElasticNet suele superar significativamente al modelo naive en problemas de precios de viviendas\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJixjHUWu_iz",
        "outputId": "486aa9e9-d38b-4b67-a41e-ac912056502a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 3_ElasticNet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 3_ElasticNet.py pages/"
      ],
      "metadata": {
        "id": "JPqUBrakjg9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. üìò Kernel Ridge Regression: Funci√≥n de optimizaci√≥n**\n",
        "\n",
        "El modelo **Kernel Ridge Regression** extiende Ridge Regression utilizando **funciones n√∫cleo** (kernels) para permitir la regresi√≥n en espacios de alta dimensi√≥n de caracter√≠sticas impl√≠citas.\n",
        "\n",
        "Minimiza la siguiente funci√≥n objetivo:\n",
        "\n",
        "$$\n",
        "\\min_{\\alpha} \\; \\| y - K \\alpha \\|_2^2 + \\lambda \\alpha^\\top K \\alpha\n",
        "$$\n",
        "\n",
        "**Donde:**\n",
        "\n",
        "- \\$ y \\in \\mathbb{R}^n \\$: vector de salidas (valores observados).\n",
        "- \\$K \\in \\mathbb{R}^{n \\times n} \\$: matriz de kernel, donde \\$K_{ij} = k(x_i, x_j) \\$\n",
        "- \\$ \\alpha \\in \\mathbb{R}^n \\$: coeficientes duales a estimar.\n",
        "- \\$ \\lambda > 0 \\$: par√°metro de regularizaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Formulaci√≥n dual\n",
        "\n",
        "El vector soluci√≥n se obtiene como:\n",
        "\n",
        "$$\n",
        "\\hat{\\alpha} = (K + \\lambda I)^{-1} y\n",
        "$$\n",
        "\n",
        "y la predicci√≥n para un nuevo punto \\( x \\) es:\n",
        "\n",
        "$$\n",
        "\\hat{y}(x) = \\sum_{i=1}^n \\alpha_i \\, k(x_i, x)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Interpretaci√≥n\n",
        "\n",
        "- Si \\$ k(x_i, x_j) = x_i^\\top x_j \\$, el modelo se reduce a Ridge Regression cl√°sico.\n",
        "- Permite ajustar relaciones no lineales mediante kernels como:\n",
        "  - **Lineal**: \\$ k(x, x') = x^\\top x' \\$\n",
        "  - **Polinomial**: \\$ k(x, x') = (x^\\top x' + c)^d \\$\n",
        "  - **RBF (Gaussiano)**: \\$ k(x, x') = \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\sigma^2}\\right) \\$\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Ventajas\n",
        "\n",
        "- Combina la robustez de Ridge con el poder representativo de los kernels.\n",
        "- No necesita transformar expl√≠citamente los datos al espacio de caracter√≠sticas.\n"
      ],
      "metadata": {
        "id": "LMptc2nRKW-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 4_KernelRidge.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from scipy.stats import loguniform, uniform\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_contour\n",
        "\n",
        "st.set_page_config(page_title=\"Optimizaci√≥n de Hiperpar√°metros\", page_icon=\"‚öôÔ∏è\", layout=\"wide\")\n",
        "\n",
        "# ===============================\n",
        "# 1. Carga y Preparaci√≥n de Datos\n",
        "# ===============================\n",
        "@st.cache_data\n",
        "def load_and_prepare_data():\n",
        "    with st.spinner('Descargando y preparando datos...'):\n",
        "        path = kagglehub.dataset_download(\"shashanknecrothapa/ames-housing-dataset\")\n",
        "        csv_file_path = os.path.join(path, \"AmesHousing.csv\")\n",
        "        Xdata = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Limpieza de datos\n",
        "        Xdata = Xdata.sample(frac=0.20, random_state=42)\n",
        "        cols_to_drop = ['Order', 'PID']\n",
        "        Xdata.drop(columns=[col for col in cols_to_drop if col in Xdata.columns], inplace=True)\n",
        "        high_null_cols = Xdata.columns[Xdata.isnull().mean() > 0.4].tolist()\n",
        "        Xdata.drop(columns=high_null_cols, inplace=True)\n",
        "\n",
        "        return Xdata\n",
        "\n",
        "Xdata = load_and_prepare_data()\n",
        "\n",
        "# ===============================\n",
        "# 2. Transformaci√≥n de Variables\n",
        "# ===============================\n",
        "st.header(\"üìê Transformaci√≥n de Variables\")\n",
        "\n",
        "col_sal = \"SalePrice\"\n",
        "\n",
        "# Selector para visualizar transformaci√≥n\n",
        "transform_col = st.selectbox(\"Seleccione columna para visualizar transformaci√≥n\",\n",
        "                            options=Xdata.select_dtypes(include=['int64', 'float64']).columns)\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(Xdata[transform_col], kde=True)\n",
        "    plt.title(f'Distribuci√≥n Original de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "with col2:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(np.log1p(Xdata[transform_col]), kde=True)\n",
        "    plt.title(f'Distribuci√≥n con log1p de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "st.info(\"\"\"\n",
        "üí° **Transformaci√≥n logar√≠tmica (log1p):**\n",
        "- Se aplica para manejar distribuciones sesgadas\n",
        "- log1p = log(1 + x) evita problemas con valores cero\n",
        "- Ayuda a cumplir supuestos de normalidad en modelos lineales\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 3. Divisi√≥n de Datos\n",
        "# ===============================\n",
        "st.header(\"‚úÇÔ∏è Divisi√≥n del Dataset\")\n",
        "\n",
        "test_size = st.slider(\"Porcentaje para test\", 10, 40, 30, 5)\n",
        "\n",
        "Xtrain, Xtest = train_test_split(Xdata, test_size=test_size/100, random_state=42)\n",
        "ytrain = np.log1p(Xtrain[col_sal])\n",
        "ytest = np.log1p(Xtest[col_sal])\n",
        "Xtrain = Xtrain.drop(columns=col_sal)\n",
        "Xtest = Xtest.drop(columns=col_sal)\n",
        "\n",
        "st.success(f\"\"\"\n",
        "Divisi√≥n completada:\n",
        "- Entrenamiento: {Xtrain.shape[0]} registros ({100-test_size}%)\n",
        "- Prueba: {Xtest.shape[0]} registros ({test_size}%)\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 4. Preprocesamiento\n",
        "# ===============================\n",
        "st.header(\"üîß Pipeline de Preprocesamiento\")\n",
        "\n",
        "numeric_cols = Xtrain.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = Xtrain.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "with st.expander(\"Ver detalles de preprocesamiento\"):\n",
        "    st.write(\"**Columnas num√©ricas:**\")\n",
        "    st.write(numeric_cols)\n",
        "    st.write(\"**Transformaciones:** Imputaci√≥n con mediana + Estandarizaci√≥n\")\n",
        "\n",
        "    st.write(\"\\n**Columnas categ√≥ricas:**\")\n",
        "    st.write(categorical_cols)\n",
        "    st.write(\"**Transformaciones:** Imputaci√≥n con 'missing' + One-Hot Encoding\")\n",
        "\n",
        "# ===============================\n",
        "# 5. Optimizaci√≥n de Hiperpar√°metros (KernelRidge)\n",
        "# ===============================\n",
        "st.header(\"‚öôÔ∏è Optimizaci√≥n de Hiperpar√°metros - KernelRidge\")\n",
        "\n",
        "# Configuraci√≥n com√∫n\n",
        "cv = st.sidebar.slider(\"N√∫mero de folds para CV\", 3, 10, 3)\n",
        "random_state = st.sidebar.number_input(\"Random state\", 42)\n",
        "scoring = 'neg_mean_squared_error'\n",
        "\n",
        "# Configuraci√≥n de par√°metros\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.subheader(\"Grid Search\")\n",
        "    alpha_min = st.number_input(\"Alpha m√≠nimo (log)\", -4, 2, -4)\n",
        "    alpha_max = st.number_input(\"Alpha m√°ximo (log)\", -4, 2, 2)\n",
        "    alpha_points = st.slider(\"Puntos para alpha\", 5, 20, 10)\n",
        "    gamma_min = st.number_input(\"Gamma m√≠nimo (log)\", -4, 2, -4)\n",
        "    gamma_max = st.number_input(\"Gamma m√°ximo (log)\", -4, 2, 2)\n",
        "    gamma_points = st.slider(\"Puntos para gamma\", 5, 20, 10)\n",
        "\n",
        "with col2:\n",
        "    st.subheader(\"Random Search\")\n",
        "    n_iter = st.slider(\"N√∫mero de iteraciones\", 10, 100, 20)\n",
        "    bayesian_trials = st.slider(\"N√∫mero de trials Bayesianos\", 10, 100, 20)\n",
        "\n",
        "if st.button(\"Ejecutar Optimizaci√≥n\"):\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    # Preparar par√°metros\n",
        "    param_grid = {\n",
        "        'regressor__alpha': np.logspace(alpha_min, alpha_max, alpha_points),\n",
        "        'regressor__gamma': np.logspace(gamma_min, gamma_max, gamma_points)\n",
        "    }\n",
        "\n",
        "    param_dist = {\n",
        "        'regressor__alpha': loguniform(1e-4, 1e2),\n",
        "        'regressor__gamma': loguniform(1e-4, 1e2)\n",
        "    }\n",
        "\n",
        "    # 1. Grid Search\n",
        "    status_text.text(\"Ejecutando Grid Search...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        Pipeline(steps=[('preprocessing', preprocessor), ('regressor', KernelRidge())]),\n",
        "        param_grid, scoring=scoring, cv=cv\n",
        "    )\n",
        "    grid_search.fit(Xtrain, ytrain)\n",
        "    grid_results = [\n",
        "        (params['regressor__alpha'], params['regressor__gamma'], -score)\n",
        "        for params, score in zip(grid_search.cv_results_['params'],\n",
        "                               grid_search.cv_results_['mean_test_score'])\n",
        "    ]\n",
        "    progress_bar.progress(33)\n",
        "\n",
        "    # 2. Random Search\n",
        "    status_text.text(\"Ejecutando Random Search...\")\n",
        "    random_search = RandomizedSearchCV(\n",
        "        Pipeline(steps=[('preprocessing', preprocessor), ('regressor', KernelRidge())]),\n",
        "        param_dist, n_iter=n_iter, scoring=scoring, cv=cv, random_state=random_state\n",
        "    )\n",
        "    random_search.fit(Xtrain, ytrain)\n",
        "    random_results = [\n",
        "        (params['regressor__alpha'], params['regressor__gamma'], -score)\n",
        "        for params, score in zip(random_search.cv_results_['params'],\n",
        "                               random_search.cv_results_['mean_test_score'])\n",
        "    ]\n",
        "    progress_bar.progress(66)\n",
        "\n",
        "    # 3. Bayesian Optimization\n",
        "    status_text.text(\"Ejecutando Bayesian Optimization...\")\n",
        "\n",
        "    def objective_kernelridge(trial):\n",
        "        alpha = trial.suggest_float('alpha', 1e-4, 1e2, log=True)\n",
        "        gamma = trial.suggest_float('gamma', 1e-4, 1e2, log=True)\n",
        "        model = Pipeline(steps=[\n",
        "            ('preprocessing', preprocessor),\n",
        "            ('regressor', KernelRidge(alpha=alpha, gamma=gamma))\n",
        "        ])\n",
        "        try:\n",
        "            return -cross_val_score(model, Xtrain, ytrain, scoring=scoring, cv=cv).mean()\n",
        "        except:\n",
        "            return float('inf')\n",
        "\n",
        "    study_kernelridge = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
        "    study_kernelridge.optimize(objective_kernelridge, n_trials=bayesian_trials)\n",
        "    progress_bar.progress(100)\n",
        "\n",
        "    # Almacenar resultados\n",
        "    best_params = {\n",
        "        'GridSearch': grid_search.best_params_,\n",
        "        'RandomSearch': random_search.best_params_,\n",
        "        'Bayesian': study_kernelridge.best_params\n",
        "    }\n",
        "\n",
        "    # ===============================\n",
        "    # 6. Visualizaci√≥n de Resultados\n",
        "    # ===============================\n",
        "    st.success(\"Optimizaci√≥n completada!\")\n",
        "\n",
        "    # Mostrar mejores par√°metros\n",
        "    st.subheader(\"üèÜ Mejores Par√°metros Encontrados\")\n",
        "    cols = st.columns(3)\n",
        "    with cols[0]:\n",
        "        st.metric(\"Grid Search - Alpha\", best_params['GridSearch']['regressor__alpha'])\n",
        "        st.metric(\"Grid Search - Gamma\", best_params['GridSearch']['regressor__gamma'])\n",
        "    with cols[1]:\n",
        "        st.metric(\"Random Search - Alpha\", best_params['RandomSearch']['regressor__alpha'])\n",
        "        st.metric(\"Random Search - Gamma\", best_params['RandomSearch']['regressor__gamma'])\n",
        "    with cols[2]:\n",
        "        st.metric(\"Bayesian - Alpha\", best_params['Bayesian']['alpha'])\n",
        "        st.metric(\"Bayesian - Gamma\", best_params['Bayesian']['gamma'])\n",
        "\n",
        "    # Gr√°ficos de comparaci√≥n\n",
        "    st.subheader(\"üìä Comparaci√≥n de M√©todos de Optimizaci√≥n\")\n",
        "\n",
        "    tab1, tab2, tab3 = st.tabs([\"Grid Search\", \"Random Search\", \"Bayesian Optimization\"])\n",
        "\n",
        "    with tab1:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        x_values = [r[0] for r in grid_results]\n",
        "        y_values = [r[1] for r in grid_results]\n",
        "        scores = [r[2] for r in grid_results]\n",
        "        scatter = ax.scatter(x_values, y_values, c=scores, cmap='viridis')\n",
        "        plt.colorbar(scatter, ax=ax, label='MSE')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_xlabel('alpha')\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_ylabel('gamma')\n",
        "        ax.set_title('Grid Search - KernelRidge')\n",
        "        ax.grid(True, which='both', ls='--')\n",
        "        st.pyplot(fig)\n",
        "        st.write(f\"Mejor MSE: {-grid_search.best_score_:.4f}\")\n",
        "\n",
        "    with tab2:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        x_values = [r[0] for r in random_results]\n",
        "        y_values = [r[1] for r in random_results]\n",
        "        scores = [r[2] for r in random_results]\n",
        "        scatter = ax.scatter(x_values, y_values, c=scores, cmap='viridis')\n",
        "        plt.colorbar(scatter, ax=ax, label='MSE')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_xlabel('alpha')\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_ylabel('gamma')\n",
        "        ax.set_title('Random Search - KernelRidge')\n",
        "        ax.grid(True, which='both', ls='--')\n",
        "        st.pyplot(fig)\n",
        "        st.write(f\"Mejor MSE: {-random_search.best_score_:.4f}\")\n",
        "\n",
        "    with tab3:\n",
        "        st.plotly_chart(plot_optimization_history(study_kernelridge))\n",
        "        st.plotly_chart(plot_param_importances(study_kernelridge))\n",
        "        st.plotly_chart(plot_contour(study_kernelridge, params=[\"alpha\", \"gamma\"]))\n",
        "        st.write(f\"Mejor MSE: {study_kernelridge.best_value:.4f}\")\n",
        "\n",
        "    # An√°lisis comparativo interactivo\n",
        "    st.subheader(\"üîç An√°lisis Comparativo Interactivo\")\n",
        "\n",
        "    # Crear un diccionario con todas las m√©tricas disponibles\n",
        "    metrics_data = {\n",
        "        'MAE': {\n",
        "            'GridSearch': mean_absolute_error(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': mean_absolute_error(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': mean_absolute_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', KernelRidge(**study_kernelridge.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'MSE': {\n",
        "            'GridSearch': mean_squared_error(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': mean_squared_error(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': mean_squared_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', KernelRidge(**study_kernelridge.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'R2': {\n",
        "            'GridSearch': r2_score(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': r2_score(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': r2_score(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', KernelRidge(**study_kernelridge.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'MAPE': {\n",
        "            'GridSearch': mean_absolute_percentage_error(ytrain, grid_search.best_estimator_.predict(Xtrain)) * 100,\n",
        "            'RandomSearch': mean_absolute_percentage_error(ytrain, random_search.best_estimator_.predict(Xtrain)) * 100,\n",
        "            'Bayesian': mean_absolute_percentage_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', KernelRidge(**study_kernelridge.best_params))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain)) * 100\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Selector de m√©trica\n",
        "    selected_metric = st.selectbox(\n",
        "        \"Seleccione la m√©trica a visualizar:\",\n",
        "        options=['MAE', 'MSE', 'R2', 'MAPE'],\n",
        "        index=1  # MSE por defecto\n",
        "    )\n",
        "\n",
        "    # Configuraci√≥n de visualizaci√≥n seg√∫n la m√©trica\n",
        "    if selected_metric == 'MAPE':\n",
        "        ylabel = 'MAPE (%)'\n",
        "        title = f'Comparaci√≥n de {selected_metric} entre M√©todos'\n",
        "        fmt = '.2f%'\n",
        "        ascending = True\n",
        "    elif selected_metric == 'R2':\n",
        "        ylabel = 'R¬≤'\n",
        "        title = f'Comparaci√≥n de {selected_metric} entre M√©todos'\n",
        "        fmt = '.4f'\n",
        "        ascending = False\n",
        "    else:\n",
        "        ylabel = selected_metric\n",
        "        title = f'Comparaci√≥n de {selected_metric} entre M√©todos'\n",
        "        fmt = '.4f'\n",
        "        ascending = True\n",
        "\n",
        "    # Ordenar m√©todos seg√∫n el rendimiento\n",
        "    methods = pd.DataFrame({\n",
        "        'Method': ['GridSearch', 'RandomSearch', 'Bayesian'],\n",
        "        'Value': [metrics_data[selected_metric]['GridSearch'],\n",
        "                metrics_data[selected_metric]['RandomSearch'],\n",
        "                metrics_data[selected_metric]['Bayesian']]\n",
        "    }).sort_values('Value', ascending=ascending)\n",
        "\n",
        "    # Crear la figura\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    colors = ['skyblue', 'lightgreen', 'salmon']\n",
        "    bars = ax.bar(methods['Method'], methods['Value'], color=colors)\n",
        "\n",
        "    # Configurar el gr√°fico\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # A√±adir los valores a las barras\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        if selected_metric == 'MAPE':\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.2f}%',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "        else:\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:{fmt}}',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    # Rotar etiquetas del eje x para mejor legibilidad\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Mostrar el gr√°fico en Streamlit\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Mostrar tabla con todas las m√©tricas\n",
        "    st.subheader(\"üìä Resumen de M√©tricas\")\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "    metrics_df['MAPE'] = metrics_df['MAPE'].map('{:.2f}%'.format)\n",
        "    metrics_df['MAE'] = metrics_df['MAE'].map('{:.4f}'.format)\n",
        "    metrics_df['MSE'] = metrics_df['MSE'].map('{:.4f}'.format)\n",
        "    metrics_df['R2'] = metrics_df['R2'].map('{:.4f}'.format)\n",
        "    st.dataframe(metrics_df.style.background_gradient(cmap='Blues', axis=0))\n",
        "\n",
        "    # Mostrar recomendaci√≥n basada en la m√©trica seleccionada\n",
        "    best_method = methods.iloc[0]['Method']\n",
        "    best_value = methods.iloc[0]['Value']\n",
        "\n",
        "    if selected_metric == 'MAPE':\n",
        "        st.success(f\"‚úÖ **Mejor modelo seg√∫n {selected_metric}:** {best_method} ({best_value:.2f}%)\")\n",
        "    elif selected_metric == 'R2':\n",
        "        st.success(f\"‚úÖ **Mejor modelo seg√∫n {selected_metric}:** {best_method} ({best_value:.4f})\")\n",
        "    else:\n",
        "        st.success(f\"‚úÖ **Mejor modelo seg√∫n {selected_metric}:** {best_method} ({best_value:.4f})\")\n",
        "\n",
        "# ===============================\n",
        "# 7. Informaci√≥n Adicional\n",
        "# ===============================\n",
        "with st.expander(\"‚ÑπÔ∏è Instrucciones de Uso\", expanded=True):\n",
        "    st.write(\"\"\"\n",
        "    1. Configura los par√°metros de b√∫squeda en el panel lateral\n",
        "    2. Haz clic en \"Ejecutar Optimizaci√≥n\"\n",
        "    3. Explora los resultados en las diferentes pesta√±as\n",
        "    4. Compara el rendimiento de los diferentes m√©todos\n",
        "\n",
        "    **Tipos de B√∫squeda:**\n",
        "    - **Grid Search:** B√∫squeda exhaustiva en una grilla definida\n",
        "    - **Random Search:** Muestreo aleatorio del espacio de par√°metros\n",
        "    - **Bayesian Optimization:** Optimizaci√≥n inteligente basada en modelos\n",
        "\n",
        "    **Hiperpar√°metros de KernelRidge:**\n",
        "    - **alpha:** Par√°metro de regularizaci√≥n (controla la penalizaci√≥n)\n",
        "    - **gamma:** Par√°metro del kernel (controla el alcance de influencia)\n",
        "    \"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 8. Evaluaci√≥n del Modelo en Test\n",
        "# ===============================\n",
        "st.header(\"üìä Evaluaci√≥n del Modelo en Datos de Test\")\n",
        "\n",
        "# Seleccionar el mejor modelo (usaremos el de Bayesian Optimization por defecto)\n",
        "best_params = study_kernelridge.best_params\n",
        "final_model = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('regressor', KernelRidge(**best_params))\n",
        "])\n",
        "final_model.fit(Xtrain, ytrain)\n",
        "\n",
        "# Predecir en test\n",
        "ypred = final_model.predict(Xtest)\n",
        "\n",
        "# Calcular m√©tricas\n",
        "test_mae = mean_absolute_error(ytest, ypred)\n",
        "test_mse = mean_squared_error(ytest, ypred)\n",
        "test_r2 = r2_score(ytest, ypred)\n",
        "test_mape = mean_absolute_percentage_error(ytest, ypred) * 100\n",
        "\n",
        "# Mostrar m√©tricas\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "with col1:\n",
        "    st.metric(\"MAE (Test)\", f\"{test_mae:.4f}\")\n",
        "with col2:\n",
        "    st.metric(\"MSE (Test)\", f\"{test_mse:.4f}\")\n",
        "with col3:\n",
        "    st.metric(\"R¬≤ (Test)\", f\"{test_r2:.4f}\")\n",
        "with col4:\n",
        "    st.metric(\"MAPE (Test)\", f\"{test_mape:.2f}%\")\n",
        "\n",
        "# Gr√°ficos de evaluaci√≥n\n",
        "st.subheader(\"üîç Gr√°ficos de Evaluaci√≥n\")\n",
        "\n",
        "tab1, tab2, tab3 = st.tabs([\"Predicciones vs Reales\", \"Residuos\", \"Distribuci√≥n de Errores\"])\n",
        "\n",
        "with tab1:\n",
        "    # Gr√°fico de predicciones vs valores reales\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(ytest, ypred, alpha=0.5)\n",
        "    ax.plot([ytest.min(), ytest.max()], [ytest.min(), ytest.max()], 'k--', lw=2)\n",
        "    ax.set_xlabel('Valores Reales (log1p)')\n",
        "    ax.set_ylabel('Predicciones (log1p)')\n",
        "    ax.set_title('Predicciones vs Valores Reales')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretaci√≥n:**\n",
        "    - Los puntos deber√≠an estar cerca de la l√≠nea diagonal\n",
        "    - Dispersi√≥n sim√©trica indica buen ajuste\n",
        "    - Patrones no aleatorios pueden indicar problemas en el modelo\n",
        "    \"\"\")\n",
        "\n",
        "with tab2:\n",
        "    # Gr√°fico de residuos\n",
        "    residuals = ytest - ypred\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(ypred, residuals, alpha=0.5)\n",
        "    ax.axhline(y=0, color='r', linestyle='--')\n",
        "    ax.set_xlabel('Predicciones (log1p)')\n",
        "    ax.set_ylabel('Residuos')\n",
        "    ax.set_title('Gr√°fico de Residuos')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretaci√≥n:**\n",
        "    - Los residuos deber√≠an distribuirse aleatoriamente alrededor del cero\n",
        "    - Patrones visibles indican que el modelo no captura alguna relaci√≥n en los datos\n",
        "    - Residuos heteroced√°sticos (que cambian de varianza) son problem√°ticos\n",
        "    \"\"\")\n",
        "\n",
        "with tab3:\n",
        "    # Distribuci√≥n de errores\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.histplot(residuals, kde=True, ax=ax)\n",
        "    ax.set_xlabel('Error de Predicci√≥n')\n",
        "    ax.set_title('Distribuci√≥n de Errores')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretaci√≥n:**\n",
        "    - Distribuci√≥n centrada en cero indica predicciones no sesgadas\n",
        "    - Forma aproximadamente normal es deseable\n",
        "    - Colas pesadas pueden indicar valores at√≠picos problem√°ticos\n",
        "    \"\"\")\n",
        "\n",
        "# Gr√°fico de valores reales vs predichos en escala original\n",
        "st.subheader(\"üîç Predicciones en Escala Original\")\n",
        "\n",
        "# Convertir a escala original\n",
        "ytest_orig = np.expm1(ytest)\n",
        "ypred_orig = np.expm1(ypred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.scatter(ytest_orig, ypred_orig, alpha=0.5)\n",
        "ax.plot([ytest_orig.min(), ytest_orig.max()], [ytest_orig.min(), ytest_orig.max()], 'k--', lw=2)\n",
        "ax.set_xlabel('Valores Reales (USD)')\n",
        "ax.set_ylabel('Predicciones (USD)')\n",
        "ax.set_title('Predicciones vs Valores Reales (Escala Original)')\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Calcular m√©tricas en escala original\n",
        "mae_orig = mean_absolute_error(ytest_orig, ypred_orig)\n",
        "mape_orig = mean_absolute_percentage_error(ytest_orig, ypred_orig) * 100\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.metric(\"MAE (USD)\", f\"${mae_orig:,.2f}\")\n",
        "with col2:\n",
        "    st.metric(\"MAPE (USD)\", f\"{mape_orig:.2f}%\")\n",
        "\n",
        "# An√°lisis de errores por rango de precio\n",
        "st.subheader(\"üìà An√°lisis de Errores por Rango de Precio\")\n",
        "\n",
        "# Crear categor√≠as de precios\n",
        "price_bins = pd.qcut(ytest_orig, q=5, duplicates='drop')\n",
        "error_analysis = pd.DataFrame({\n",
        "    'Precio Real': ytest_orig,\n",
        "    'Error Absoluto': np.abs(ytest_orig - ypred_orig),\n",
        "    'Rango Precio': price_bins\n",
        "})\n",
        "\n",
        "# Calcular m√©tricas por rango\n",
        "error_by_price = error_analysis.groupby('Rango Precio').agg({\n",
        "    'Precio Real': 'mean',\n",
        "    'Error Absoluto': ['mean', 'median', 'std']\n",
        "}).reset_index()\n",
        "\n",
        "error_by_price.columns = ['Rango Precio', 'Precio Promedio', 'MAE', 'Error Mediano', 'Desviaci√≥n Error']\n",
        "\n",
        "# Gr√°fico de MAE por rango de precio\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.barplot(data=error_by_price, x='Rango Precio', y='MAE', ax=ax)\n",
        "ax.set_title('Error Absoluto Medio por Rango de Precio')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "ax.set_ylabel('MAE (USD)')\n",
        "ax.set_xlabel('Rango de Precio')\n",
        "st.pyplot(fig)\n",
        "\n",
        "st.write(\"\"\"\n",
        "**Interpretaci√≥n:**\n",
        "- Identifica en qu√© rangos de precio el modelo tiene mayor error\n",
        "- Errores consistentes pueden indicar problemas con ciertos tipos de propiedades\n",
        "- Puede sugerir la necesidad de modelos segmentados o m√°s datos en ciertos rangos\n",
        "\"\"\")\n",
        "\n",
        "# Mostrar tabla de an√°lisis\n",
        "st.dataframe(error_by_price.style.background_gradient(subset=['MAE'], cmap='Reds'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7Gr77iqQJ-D",
        "outputId": "c383c1f6-0d54-469b-f2a3-63a1e36ec9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 4_KernelRidge.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 4_KernelRidge.py pages/"
      ],
      "metadata": {
        "id": "ByToVW4qQl_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. ‚öôÔ∏è SGDRegressor: Funci√≥n de optimizaci√≥n**\n",
        "\n",
        "El modelo **SGDRegressor** estima los coeficientes \\( \\beta \\in \\mathbb{R}^p \\) al minimizar una funci√≥n de p√©rdida sobre los datos utilizando **descenso de gradiente estoc√°stico** (SGD). La forma general de la funci√≥n objetivo es:\n",
        "\n",
        "$$\n",
        "\\min_{\\beta} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\mathcal{L}(y_i, \\beta^\\top x_i) + \\lambda R(\\beta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ Componentes:\n",
        "\n",
        "- \\$\\mathcal{L}(y_i, \\hat{y}_i) \\$: funci√≥n de p√©rdida (por defecto: **cuadr√°tica**)\n",
        "  \n",
        "  $$\n",
        "  \\mathcal{L}(y_i, \\hat{y}_i) = \\frac{1}{2}(y_i - \\hat{y}_i)^2\n",
        "  $$\n",
        "\n",
        "- \\$ R(\\beta) \\$: t√©rmino de regularizaci√≥n (puede ser L1, L2 o ambos)\n",
        "  \n",
        "  - **L2 (Ridge):** \\$ R(\\beta) = \\frac{1}{2} \\|\\beta\\|_2^2 \\$\n",
        "  - **L1 (Lasso):** \\$ R(\\beta) = \\|\\beta\\|_1 \\$\n",
        "  - **ElasticNet:** \\$ R(\\beta) = \\alpha \\|\\beta\\|_1 + \\frac{1 - \\alpha}{2} \\|\\beta\\|_2^2 \\$\n",
        "\n",
        "- \\$ \\lambda \\$: par√°metro de penalizaci√≥n en `SGDRegressor`, \\$\\alpha=\\lambda$.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Iteraci√≥n de SGD\n",
        "\n",
        "En cada iteraci√≥n, los par√°metros se actualan con:\n",
        "\n",
        "$$\n",
        "\\beta \\leftarrow \\beta - \\eta \\cdot \\left( \\nabla_\\beta \\mathcal{L} + \\lambda \\nabla_\\beta R(\\beta) \\right)\n",
        "$$\n",
        "\n",
        "donde \\( \\eta \\) es la tasa de aprendizaje (*learning rate*).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è Notas importantes:\n",
        "\n",
        "- El algoritmo trabaja sobre **minibatches** o **una muestra por iteraci√≥n**.\n",
        "- Admite diferentes esquemas de actualizaci√≥n: `'constant'`, `'optimal'`, `'invscaling'`, etc.\n",
        "- Es ideal para conjuntos de datos grandes y dispersos (alta dimensionalidad).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Recomendaci√≥n\n",
        "\n",
        "Ajustar cuidadosamente:\n",
        "\n",
        "- `penalty`: `'l2'`, `'l1'`, `'elasticnet'`\n",
        "- `alpha`: fuerza de regularizaci√≥n\n",
        "- `learning_rate`: tipo de actualizaci√≥n\n",
        "- `eta0`: tasa de aprendizaje inicial\n"
      ],
      "metadata": {
        "id": "E-OTxEkIKlqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 5_SGDRegressor.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from scipy.stats import loguniform, uniform\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_contour\n",
        "\n",
        "st.set_page_config(page_title=\"Optimizaci√≥n de Hiperpar√°metros\", page_icon=\"‚öôÔ∏è\", layout=\"wide\")\n",
        "\n",
        "# ===============================\n",
        "# 1. Carga y Preparaci√≥n de Datos\n",
        "# ===============================\n",
        "@st.cache_data\n",
        "def load_and_prepare_data():\n",
        "    with st.spinner('Descargando y preparando datos...'):\n",
        "        path = kagglehub.dataset_download(\"shashanknecrothapa/ames-housing-dataset\")\n",
        "        csv_file_path = os.path.join(path, \"AmesHousing.csv\")\n",
        "        Xdata = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Limpieza de datos\n",
        "        Xdata = Xdata.sample(frac=0.20, random_state=42)\n",
        "        cols_to_drop = ['Order', 'PID']\n",
        "        Xdata.drop(columns=[col for col in cols_to_drop if col in Xdata.columns], inplace=True)\n",
        "        high_null_cols = Xdata.columns[Xdata.isnull().mean() > 0.4].tolist()\n",
        "        Xdata.drop(columns=high_null_cols, inplace=True)\n",
        "\n",
        "        return Xdata\n",
        "\n",
        "Xdata = load_and_prepare_data()\n",
        "\n",
        "# ===============================\n",
        "# 2. Transformaci√≥n de Variables\n",
        "# ===============================\n",
        "st.header(\"üìê Transformaci√≥n de Variables\")\n",
        "\n",
        "col_sal = \"SalePrice\"\n",
        "\n",
        "# Selector para visualizar transformaci√≥n\n",
        "transform_col = st.selectbox(\"Seleccione columna para visualizar transformaci√≥n\",\n",
        "                            options=Xdata.select_dtypes(include=['int64', 'float64']).columns)\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(Xdata[transform_col], kde=True)\n",
        "    plt.title(f'Distribuci√≥n Original de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "with col2:\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(np.log1p(Xdata[transform_col]), kde=True)\n",
        "    plt.title(f'Distribuci√≥n con log1p de {transform_col}')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "st.info(\"\"\"\n",
        "üí° **Transformaci√≥n logar√≠tmica (log1p):**\n",
        "- Se aplica para manejar distribuciones sesgadas\n",
        "- log1p = log(1 + x) evita problemas con valores cero\n",
        "- Ayuda a cumplir supuestos de normalidad en modelos lineales\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 3. Divisi√≥n de Datos\n",
        "# ===============================\n",
        "st.header(\"‚úÇÔ∏è Divisi√≥n del Dataset\")\n",
        "\n",
        "test_size = st.slider(\"Porcentaje para test\", 10, 40, 30, 5)\n",
        "\n",
        "Xtrain, Xtest = train_test_split(Xdata, test_size=test_size/100, random_state=42)\n",
        "ytrain = np.log1p(Xtrain[col_sal])\n",
        "ytest = np.log1p(Xtest[col_sal])\n",
        "Xtrain = Xtrain.drop(columns=col_sal)\n",
        "Xtest = Xtest.drop(columns=col_sal)\n",
        "\n",
        "st.success(f\"\"\"\n",
        "Divisi√≥n completada:\n",
        "- Entrenamiento: {Xtrain.shape[0]} registros ({100-test_size}%)\n",
        "- Prueba: {Xtest.shape[0]} registros ({test_size}%)\n",
        "\"\"\")\n",
        "\n",
        "# ===============================\n",
        "# 4. Preprocesamiento\n",
        "# ===============================\n",
        "st.header(\"üîß Pipeline de Preprocesamiento\")\n",
        "\n",
        "numeric_cols = Xtrain.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = Xtrain.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "with st.expander(\"Ver detalles de preprocesamiento\"):\n",
        "    st.write(\"**Columnas num√©ricas:**\")\n",
        "    st.write(numeric_cols)\n",
        "    st.write(\"**Transformaciones:** Imputaci√≥n con mediana + Estandarizaci√≥n\")\n",
        "\n",
        "    st.write(\"\\n**Columnas categ√≥ricas:**\")\n",
        "    st.write(categorical_cols)\n",
        "    st.write(\"**Transformaciones:** Imputaci√≥n con 'missing' + One-Hot Encoding\")\n",
        "\n",
        "# ===============================\n",
        "# 5. Optimizaci√≥n de Hiperpar√°metros (SGDRegressor)\n",
        "# ===============================\n",
        "st.header(\"‚öôÔ∏è Optimizaci√≥n de Hiperpar√°metros - SGDRegressor\")\n",
        "\n",
        "# Configuraci√≥n com√∫n\n",
        "cv = st.sidebar.slider(\"N√∫mero de folds para CV\", 3, 10, 3)\n",
        "random_state = st.sidebar.number_input(\"Random state\", 42)\n",
        "scoring = 'neg_mean_squared_error'\n",
        "\n",
        "# Configuraci√≥n de par√°metros\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.subheader(\"Grid Search\")\n",
        "    alpha_min = st.number_input(\"Alpha m√≠nimo (log)\", -4, 2, -4)\n",
        "    alpha_max = st.number_input(\"Alpha m√°ximo (log)\", -4, 2, 2)\n",
        "    alpha_points = st.slider(\"Puntos para alpha\", 5, 20, 10)\n",
        "    l1_ratio_points = st.slider(\"Puntos para l1_ratio\", 5, 20, 10)\n",
        "\n",
        "with col2:\n",
        "    st.subheader(\"Random Search\")\n",
        "    n_iter = st.slider(\"N√∫mero de iteraciones\", 10, 100, 20)\n",
        "    bayesian_trials = st.slider(\"N√∫mero de trials Bayesianos\", 10, 100, 20)\n",
        "\n",
        "if st.button(\"Ejecutar Optimizaci√≥n\"):\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    # Preparar par√°metros\n",
        "    param_grid = {\n",
        "        'regressor__alpha': np.logspace(alpha_min, alpha_max, alpha_points),\n",
        "        'regressor__l1_ratio': np.linspace(0, 1, l1_ratio_points)\n",
        "    }\n",
        "\n",
        "    param_dist = {\n",
        "        'regressor__alpha': loguniform(1e-4, 1e2),\n",
        "        'regressor__l1_ratio': uniform(0, 1)\n",
        "    }\n",
        "\n",
        "    # 1. Grid Search\n",
        "    status_text.text(\"Ejecutando Grid Search...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        Pipeline(steps=[('preprocessing', preprocessor), ('regressor', SGDRegressor(random_state=random_state))]),\n",
        "        param_grid, scoring=scoring, cv=cv\n",
        "    )\n",
        "    grid_search.fit(Xtrain, ytrain)\n",
        "    grid_results = [\n",
        "        (params['regressor__alpha'], params['regressor__l1_ratio'], -score)\n",
        "        for params, score in zip(grid_search.cv_results_['params'],\n",
        "                               grid_search.cv_results_['mean_test_score'])\n",
        "    ]\n",
        "    progress_bar.progress(33)\n",
        "\n",
        "    # 2. Random Search\n",
        "    status_text.text(\"Ejecutando Random Search...\")\n",
        "    random_search = RandomizedSearchCV(\n",
        "        Pipeline(steps=[('preprocessing', preprocessor), ('regressor', SGDRegressor(random_state=random_state))]),\n",
        "        param_dist, n_iter=n_iter, scoring=scoring, cv=cv, random_state=random_state\n",
        "    )\n",
        "    random_search.fit(Xtrain, ytrain)\n",
        "    random_results = [\n",
        "        (params['regressor__alpha'], params['regressor__l1_ratio'], -score)\n",
        "        for params, score in zip(random_search.cv_results_['params'],\n",
        "                               random_search.cv_results_['mean_test_score'])\n",
        "    ]\n",
        "    progress_bar.progress(66)\n",
        "\n",
        "    # 3. Bayesian Optimization\n",
        "    status_text.text(\"Ejecutando Bayesian Optimization...\")\n",
        "\n",
        "    def objective_sgd(trial):\n",
        "        alpha = trial.suggest_float('alpha', 1e-4, 1e2, log=True)\n",
        "        l1_ratio = trial.suggest_float('l1_ratio', 0, 1)\n",
        "        model = Pipeline(steps=[\n",
        "            ('preprocessing', preprocessor),\n",
        "            ('regressor', SGDRegressor(\n",
        "                alpha=alpha,\n",
        "                l1_ratio=l1_ratio,\n",
        "                random_state=random_state\n",
        "            ))\n",
        "        ])\n",
        "        try:\n",
        "            return -cross_val_score(model, Xtrain, ytrain, scoring=scoring, cv=cv).mean()\n",
        "        except:\n",
        "            return float('inf')\n",
        "\n",
        "    study_sgd = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
        "    study_sgd.optimize(objective_sgd, n_trials=bayesian_trials)\n",
        "    progress_bar.progress(100)\n",
        "\n",
        "    # Almacenar resultados\n",
        "    best_params = {\n",
        "        'GridSearch': grid_search.best_params_,\n",
        "        'RandomSearch': random_search.best_params_,\n",
        "        'Bayesian': study_sgd.best_params\n",
        "    }\n",
        "\n",
        "    # ===============================\n",
        "    # 6. Visualizaci√≥n de Resultados\n",
        "    # ===============================\n",
        "    st.success(\"Optimizaci√≥n completada!\")\n",
        "\n",
        "    # Mostrar mejores par√°metros\n",
        "    st.subheader(\"üèÜ Mejores Par√°metros Encontrados\")\n",
        "    cols = st.columns(3)\n",
        "    with cols[0]:\n",
        "        st.metric(\"Grid Search - Alpha\", best_params['GridSearch']['regressor__alpha'])\n",
        "        st.metric(\"Grid Search - L1 Ratio\", best_params['GridSearch']['regressor__l1_ratio'])\n",
        "    with cols[1]:\n",
        "        st.metric(\"Random Search - Alpha\", best_params['RandomSearch']['regressor__alpha'])\n",
        "        st.metric(\"Random Search - L1 Ratio\", best_params['RandomSearch']['regressor__l1_ratio'])\n",
        "    with cols[2]:\n",
        "        st.metric(\"Bayesian - Alpha\", best_params['Bayesian']['alpha'])\n",
        "        st.metric(\"Bayesian - L1 Ratio\", best_params['Bayesian']['l1_ratio'])\n",
        "\n",
        "    # Gr√°ficos de comparaci√≥n\n",
        "    st.subheader(\"üìä Comparaci√≥n de M√©todos de Optimizaci√≥n\")\n",
        "\n",
        "    tab1, tab2, tab3 = st.tabs([\"Grid Search\", \"Random Search\", \"Bayesian Optimization\"])\n",
        "\n",
        "    with tab1:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        x_values = [r[0] for r in grid_results]\n",
        "        y_values = [r[1] for r in grid_results]\n",
        "        scores = [r[2] for r in grid_results]\n",
        "        scatter = ax.scatter(x_values, y_values, c=scores, cmap='viridis')\n",
        "        plt.colorbar(scatter, ax=ax, label='MSE')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_xlabel('alpha')\n",
        "        ax.set_ylabel('l1_ratio')\n",
        "        ax.set_title('Grid Search - SGDRegressor')\n",
        "        ax.grid(True, which='both', ls='--')\n",
        "        st.pyplot(fig)\n",
        "        st.write(f\"Mejor MSE: {-grid_search.best_score_:.4f}\")\n",
        "\n",
        "    with tab2:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        x_values = [r[0] for r in random_results]\n",
        "        y_values = [r[1] for r in random_results]\n",
        "        scores = [r[2] for r in random_results]\n",
        "        scatter = ax.scatter(x_values, y_values, c=scores, cmap='viridis')\n",
        "        plt.colorbar(scatter, ax=ax, label='MSE')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_xlabel('alpha')\n",
        "        ax.set_ylabel('l1_ratio')\n",
        "        ax.set_title('Random Search - SGDRegressor')\n",
        "        ax.grid(True, which='both', ls='--')\n",
        "        st.pyplot(fig)\n",
        "        st.write(f\"Mejor MSE: {-random_search.best_score_:.4f}\")\n",
        "\n",
        "    with tab3:\n",
        "        st.plotly_chart(plot_optimization_history(study_sgd))\n",
        "        st.plotly_chart(plot_param_importances(study_sgd))\n",
        "        st.plotly_chart(plot_contour(study_sgd, params=[\"alpha\", \"l1_ratio\"]))\n",
        "        st.write(f\"Mejor MSE: {study_sgd.best_value:.4f}\")\n",
        "\n",
        "    # An√°lisis comparativo interactivo\n",
        "    st.subheader(\"üîç An√°lisis Comparativo Interactivo\")\n",
        "\n",
        "    # Crear un diccionario con todas las m√©tricas disponibles\n",
        "    metrics_data = {\n",
        "        'MAE': {\n",
        "            'GridSearch': mean_absolute_error(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': mean_absolute_error(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': mean_absolute_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', SGDRegressor(**study_sgd.best_params, random_state=random_state))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'MSE': {\n",
        "            'GridSearch': mean_squared_error(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': mean_squared_error(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': mean_squared_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', SGDRegressor(**study_sgd.best_params, random_state=random_state))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'R2': {\n",
        "            'GridSearch': r2_score(ytrain, grid_search.best_estimator_.predict(Xtrain)),\n",
        "            'RandomSearch': r2_score(ytrain, random_search.best_estimator_.predict(Xtrain)),\n",
        "            'Bayesian': r2_score(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', SGDRegressor(**study_sgd.best_params, random_state=random_state))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain))\n",
        "        },\n",
        "        'MAPE': {\n",
        "            'GridSearch': mean_absolute_percentage_error(ytrain, grid_search.best_estimator_.predict(Xtrain)) * 100,\n",
        "            'RandomSearch': mean_absolute_percentage_error(ytrain, random_search.best_estimator_.predict(Xtrain)) * 100,\n",
        "            'Bayesian': mean_absolute_percentage_error(ytrain, Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', SGDRegressor(**study_sgd.best_params, random_state=random_state))\n",
        "            ]).fit(Xtrain, ytrain).predict(Xtrain)) * 100\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Selector de m√©trica\n",
        "    selected_metric = st.selectbox(\n",
        "        \"Seleccione la m√©trica a visualizar:\",\n",
        "        options=['MAE', 'MSE', 'R2', 'MAPE'],\n",
        "        index=1  # MSE por defecto\n",
        "    )\n",
        "\n",
        "    # Configuraci√≥n de visualizaci√≥n seg√∫n la m√©trica\n",
        "    if selected_metric == 'MAPE':\n",
        "        ylabel = 'MAPE (%)'\n",
        "        title = f'Comparaci√≥n de {selected_metric} entre M√©todos'\n",
        "        fmt = '.2f%'\n",
        "        ascending = True\n",
        "    elif selected_metric == 'R2':\n",
        "        ylabel = 'R¬≤'\n",
        "        title = f'Comparaci√≥n de {selected_metric} entre M√©todos'\n",
        "        fmt = '.4f'\n",
        "        ascending = False\n",
        "    else:\n",
        "        ylabel = selected_metric\n",
        "        title = f'Comparaci√≥n de {selected_metric} entre M√©todos'\n",
        "        fmt = '.4f'\n",
        "        ascending = True\n",
        "\n",
        "    # Ordenar m√©todos seg√∫n el rendimiento\n",
        "    methods = pd.DataFrame({\n",
        "        'Method': ['GridSearch', 'RandomSearch', 'Bayesian'],\n",
        "        'Value': [metrics_data[selected_metric]['GridSearch'],\n",
        "                metrics_data[selected_metric]['RandomSearch'],\n",
        "                metrics_data[selected_metric]['Bayesian']]\n",
        "    }).sort_values('Value', ascending=ascending)\n",
        "\n",
        "    # Crear la figura\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    colors = ['skyblue', 'lightgreen', 'salmon']\n",
        "    bars = ax.bar(methods['Method'], methods['Value'], color=colors)\n",
        "\n",
        "    # Configurar el gr√°fico\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # A√±adir los valores a las barras\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        if selected_metric == 'MAPE':\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.2f}%',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "        else:\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:{fmt}}',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    # Rotar etiquetas del eje x para mejor legibilidad\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Mostrar el gr√°fico en Streamlit\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Mostrar tabla con todas las m√©tricas\n",
        "    st.subheader(\"üìä Resumen de M√©tricas\")\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "    metrics_df['MAPE'] = metrics_df['MAPE'].map('{:.2f}%'.format)\n",
        "    metrics_df['MAE'] = metrics_df['MAE'].map('{:.4f}'.format)\n",
        "    metrics_df['MSE'] = metrics_df['MSE'].map('{:.4f}'.format)\n",
        "    metrics_df['R2'] = metrics_df['R2'].map('{:.4f}'.format)\n",
        "    st.dataframe(metrics_df.style.background_gradient(cmap='Blues', axis=0))\n",
        "\n",
        "    # Mostrar recomendaci√≥n basada en la m√©trica seleccionada\n",
        "    best_method = methods.iloc[0]['Method']\n",
        "    best_value = methods.iloc[0]['Value']\n",
        "\n",
        "    if selected_metric == 'MAPE':\n",
        "        st.success(f\"‚úÖ **Mejor modelo seg√∫n {selected_metric}:** {best_method} ({best_value:.2f}%)\")\n",
        "    elif selected_metric == 'R2':\n",
        "        st.success(f\"‚úÖ **Mejor modelo seg√∫n {selected_metric}:** {best_method} ({best_value:.4f})\")\n",
        "    else:\n",
        "        st.success(f\"‚úÖ **Mejor modelo seg√∫n {selected_metric}:** {best_method} ({best_value:.4f})\")\n",
        "\n",
        "# ===============================\n",
        "# 7. Informaci√≥n Adicional\n",
        "# ===============================\n",
        "with st.expander(\"‚ÑπÔ∏è Instrucciones de Uso\", expanded=True):\n",
        "    st.write(\"\"\"\n",
        "    1. Configura los par√°metros de b√∫squeda en el panel lateral\n",
        "    2. Haz clic en \"Ejecutar Optimizaci√≥n\"\n",
        "    3. Explora los resultados en las diferentes pesta√±as\n",
        "    4. Compara el rendimiento de los diferentes m√©todos\n",
        "\n",
        "    **Tipos de B√∫squeda:**\n",
        "    - **Grid Search:** B√∫squeda exhaustiva en una grilla definida\n",
        "    - **Random Search:** Muestreo aleatorio del espacio de par√°metros\n",
        "    - **Bayesian Optimization:** Optimizaci√≥n inteligente basada en modelos\n",
        "\n",
        "    **Hiperpar√°metros de SGDRegressor:**\n",
        "    - **alpha:** Par√°metro de regularizaci√≥n (controla la penalizaci√≥n)\n",
        "    - **l1_ratio:** Balance entre regularizaci√≥n L1 y L2 (0 = L2, 1 = L1)\n",
        "    \"\"\")\n",
        "# ===============================\n",
        "# 8. Evaluaci√≥n del Modelo en Test\n",
        "# ===============================\n",
        "st.header(\"üìä Evaluaci√≥n del Modelo SGDRegressor en Datos de Test\")\n",
        "\n",
        "# Seleccionar el mejor modelo (usaremos el de Bayesian Optimization por defecto)\n",
        "best_params = study_sgd.best_params\n",
        "final_model = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('regressor', SGDRegressor(**best_params, random_state=random_state))\n",
        "])\n",
        "final_model.fit(Xtrain, ytrain)\n",
        "\n",
        "# Predecir en test\n",
        "ypred = final_model.predict(Xtest)\n",
        "\n",
        "# Calcular m√©tricas\n",
        "test_mae = mean_absolute_error(ytest, ypred)\n",
        "test_mse = mean_squared_error(ytest, ypred)\n",
        "test_r2 = r2_score(ytest, ypred)\n",
        "test_mape = mean_absolute_percentage_error(ytest, ypred) * 100\n",
        "\n",
        "# Mostrar m√©tricas\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "with col1:\n",
        "    st.metric(\"MAE (Test)\", f\"{test_mae:.4f}\")\n",
        "with col2:\n",
        "    st.metric(\"MSE (Test)\", f\"{test_mse:.4f}\")\n",
        "with col3:\n",
        "    st.metric(\"R¬≤ (Test)\", f\"{test_r2:.4f}\")\n",
        "with col4:\n",
        "    st.metric(\"MAPE (Test)\", f\"{test_mape:.2f}%\")\n",
        "\n",
        "# Gr√°ficos de evaluaci√≥n\n",
        "st.subheader(\"üîç Gr√°ficos de Evaluaci√≥n\")\n",
        "\n",
        "tab1, tab2, tab3, tab4 = st.tabs([\"Predicciones vs Reales\", \"Residuos\", \"Distribuci√≥n de Errores\", \"Convergencia\"])\n",
        "\n",
        "with tab1:\n",
        "    # Gr√°fico de predicciones vs valores reales\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(ytest, ypred, alpha=0.5)\n",
        "    ax.plot([ytest.min(), ytest.max()], [ytest.min(), ytest.max()], 'k--', lw=2)\n",
        "    ax.set_xlabel('Valores Reales (log1p)')\n",
        "    ax.set_ylabel('Predicciones (log1p)')\n",
        "    ax.set_title('Predicciones vs Valores Reales (SGDRegressor)')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretaci√≥n:**\n",
        "    - SGD puede mostrar m√°s dispersi√≥n que otros modelos debido a su naturaleza estoc√°stica\n",
        "    - Los puntos deber√≠an agruparse alrededor de la l√≠nea diagonal\n",
        "    - Patrones no lineales pueden indicar que se necesita un modelo m√°s complejo\n",
        "    \"\"\")\n",
        "\n",
        "with tab2:\n",
        "    # Gr√°fico de residuos\n",
        "    residuals = ytest - ypred\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(ypred, residuals, alpha=0.5)\n",
        "    ax.axhline(y=0, color='r', linestyle='--')\n",
        "    ax.set_xlabel('Predicciones (log1p)')\n",
        "    ax.set_ylabel('Residuos')\n",
        "    ax.set_title('Gr√°fico de Residuos (SGDRegressor)')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretaci√≥n:**\n",
        "    - Residuos deber√≠an distribuirse aleatoriamente alrededor del cero\n",
        "    - SGD puede mostrar m√°s variabilidad en los residuos que otros modelos\n",
        "    - Patrones visibles pueden indicar que el learning rate no fue √≥ptimo\n",
        "    \"\"\")\n",
        "\n",
        "with tab3:\n",
        "    # Distribuci√≥n de errores\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.histplot(residuals, kde=True, ax=ax)\n",
        "    ax.set_xlabel('Error de Predicci√≥n')\n",
        "    ax.set_title('Distribuci√≥n de Errores (SGDRegressor)')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretaci√≥n:**\n",
        "    - Distribuci√≥n centrada en cero indica que no hay sesgo sistem√°tico\n",
        "    - La forma de la distribuci√≥n muestra c√≥mo se comportan los errores\n",
        "    - SGD puede producir distribuciones con colas m√°s pesadas que otros m√©todos\n",
        "    \"\"\")\n",
        "\n",
        "with tab4:\n",
        "    # Gr√°fico de convergencia (para SGD)\n",
        "    try:\n",
        "        # Entrenar modelo guardando p√©rdida en cada epoch\n",
        "        partial_model = Pipeline(steps=[\n",
        "            ('preprocessing', preprocessor),\n",
        "            ('regressor', SGDRegressor(**best_params, random_state=random_state))\n",
        "        ])\n",
        "\n",
        "        # Crear lista para guardar p√©rdidas\n",
        "        train_errors = []\n",
        "        test_errors = []\n",
        "\n",
        "        # Mini-batch para simular epochs de SGD\n",
        "        for epoch in range(1, 101):\n",
        "            partial_model.fit(Xtrain, ytrain)\n",
        "            train_errors.append(mean_squared_error(ytrain, partial_model.predict(Xtrain)))\n",
        "            test_errors.append(mean_squared_error(ytest, partial_model.predict(Xtest)))\n",
        "\n",
        "        # Gr√°fico de convergencia\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.plot(range(1, 101), train_errors, 'b-', label='Train MSE')\n",
        "        ax.plot(range(1, 101), test_errors, 'r-', label='Test MSE')\n",
        "        ax.set_xlabel('Epochs')\n",
        "        ax.set_ylabel('MSE')\n",
        "        ax.set_title('Curva de Aprendizaje (SGDRegressor)')\n",
        "        ax.legend()\n",
        "        ax.grid(True)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        st.write(\"\"\"\n",
        "        **Interpretaci√≥n:**\n",
        "        - Muestra c√≥mo evoluciona el error durante el entrenamiento\n",
        "        - SGD deber√≠a converger a un valor estable despu√©s de varias epochs\n",
        "        - Si las curvas no convergen, puede indicar que el learning rate es inadecuado\n",
        "        \"\"\")\n",
        "    except Exception as e:\n",
        "        st.warning(f\"No se pudo generar el gr√°fico de convergencia: {str(e)}\")\n",
        "\n",
        "# Gr√°fico de valores reales vs predichos en escala original\n",
        "st.subheader(\"üí∞ Predicciones en Escala Original (USD)\")\n",
        "\n",
        "# Convertir a escala original\n",
        "ytest_orig = np.expm1(ytest)\n",
        "ypred_orig = np.expm1(ypred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.scatter(ytest_orig, ypred_orig, alpha=0.5)\n",
        "ax.plot([ytest_orig.min(), ytest_orig.max()], [ytest_orig.min(), ytest_orig.max()], 'k--', lw=2)\n",
        "ax.set_xlabel('Valores Reales (USD)')\n",
        "ax.set_ylabel('Predicciones (USD)')\n",
        "ax.set_title('Predicciones vs Valores Reales - Escala Original (SGDRegressor)')\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Calcular m√©tricas en escala original\n",
        "mae_orig = mean_absolute_error(ytest_orig, ypred_orig)\n",
        "mape_orig = mean_absolute_percentage_error(ytest_orig, ypred_orig) * 100\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.metric(\"MAE (USD)\", f\"${mae_orig:,.2f}\")\n",
        "with col2:\n",
        "    st.metric(\"MAPE (USD)\", f\"{mape_orig:.2f}%\")\n",
        "\n",
        "# An√°lisis de errores por rango de precio\n",
        "st.subheader(\"üìà An√°lisis de Errores por Rango de Precio\")\n",
        "\n",
        "# Crear categor√≠as de precios\n",
        "price_bins = pd.qcut(ytest_orig, q=5, duplicates='drop')\n",
        "error_analysis = pd.DataFrame({\n",
        "    'Precio Real': ytest_orig,\n",
        "    'Error Absoluto': np.abs(ytest_orig - ypred_orig),\n",
        "    'Rango Precio': price_bins\n",
        "})\n",
        "\n",
        "# Calcular m√©tricas por rango\n",
        "error_by_price = error_analysis.groupby('Rango Precio').agg({\n",
        "    'Precio Real': 'mean',\n",
        "    'Error Absoluto': ['mean', 'median', 'std']\n",
        "}).reset_index()\n",
        "\n",
        "error_by_price.columns = ['Rango Precio', 'Precio Promedio', 'MAE', 'Error Mediano', 'Desviaci√≥n Error']\n",
        "\n",
        "# Gr√°fico de MAE por rango de precio\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.barplot(data=error_by_price, x='Rango Precio', y='MAE', ax=ax)\n",
        "ax.set_title('Error Absoluto Medio por Rango de Precio (SGDRegressor)')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "ax.set_ylabel('MAE (USD)')\n",
        "ax.set_xlabel('Rango de Precio')\n",
        "st.pyplot(fig)\n",
        "\n",
        "st.write(\"\"\"\n",
        "**Interpretaci√≥n:**\n",
        "- SGD puede tener un rendimiento diferente en distintos rangos de precio\n",
        "- Errores mayores en extremos (propiedades muy baratas o muy caras) son comunes\n",
        "- Puede indicar la necesidad de ajustar par√°metros de regularizaci√≥n\n",
        "\"\"\")\n",
        "\n",
        "# Mostrar tabla de an√°lisis\n",
        "st.dataframe(error_by_price.style.background_gradient(subset=['MAE'], cmap='Reds'))\n",
        "\n",
        "# Comparaci√≥n con modelo naive (promedio)\n",
        "st.subheader(\"ü§î Comparaci√≥n con Modelo Naive (Promedio)\")\n",
        "\n",
        "naive_pred = np.full_like(ytest, ytrain.mean())\n",
        "naive_mae = mean_absolute_error(ytest, naive_pred)\n",
        "improvement = (naive_mae - test_mae) / naive_mae * 100\n",
        "\n",
        "st.metric(\"Mejora sobre modelo naive\", f\"{improvement:.2f}%\",\n",
        "          delta=f\"MAE naive: {naive_mae:.4f}, MAE SGD: {test_mae:.4f}\")\n",
        "\n",
        "st.write(\"\"\"\n",
        "**Interpretaci√≥n:**\n",
        "- Muestra el valor a√±adido del modelo SGD respecto a una predicci√≥n simple\n",
        "- Una mejora positiva indica que el modelo est√° aprendiendo patrones √∫tiles\n",
        "- SGD puede ser especialmente sensible a la escala de los datos y par√°metros de regularizaci√≥n\n",
        "\"\"\")\n",
        "\n",
        "# An√°lisis de sensibilidad a par√°metros\n",
        "st.subheader(\"üéöÔ∏è Sensibilidad a Par√°metros\")\n",
        "\n",
        "# Crear gr√°fico de sensibilidad para alpha y l1_ratio\n",
        "try:\n",
        "    alphas = np.logspace(-3, 2, 10)\n",
        "    l1_ratios = np.linspace(0, 1, 5)\n",
        "\n",
        "    mse_values = []\n",
        "    for alpha in alphas:\n",
        "        for l1_ratio in l1_ratios:\n",
        "            model = Pipeline(steps=[\n",
        "                ('preprocessing', preprocessor),\n",
        "                ('regressor', SGDRegressor(alpha=alpha, l1_ratio=l1_ratio, random_state=random_state))\n",
        "            ])\n",
        "            mse = -cross_val_score(model, Xtrain, ytrain, scoring='neg_mean_squared_error', cv=3).mean()\n",
        "            mse_values.append((alpha, l1_ratio, mse))\n",
        "\n",
        "    sens_df = pd.DataFrame(mse_values, columns=['alpha', 'l1_ratio', 'MSE'])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    for l1_ratio in l1_ratios:\n",
        "        subset = sens_df[sens_df['l1_ratio'] == l1_ratio]\n",
        "        ax.plot(subset['alpha'], subset['MSE'], 'o-', label=f'l1_ratio={l1_ratio:.1f}')\n",
        "\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xlabel('alpha (log scale)')\n",
        "    ax.set_ylabel('MSE')\n",
        "    ax.set_title('Sensibilidad del MSE a alpha y l1_ratio')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    **Interpretaci√≥n:**\n",
        "    - Muestra c√≥mo cambia el rendimiento con diferentes par√°metros\n",
        "    - SGD puede ser muy sensible a la elecci√≥n de alpha (par√°metro de regularizaci√≥n)\n",
        "    - El balance entre L1 y L2 (l1_ratio) afecta la selecci√≥n de caracter√≠sticas\n",
        "    \"\"\")\n",
        "except Exception as e:\n",
        "    st.warning(f\"No se pudo generar el gr√°fico de sensibilidad: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys_J3LlcXBo_",
        "outputId": "b1b8908d-19ca-4002-978e-9e7764cbeca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 5_SGDRegressor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 5_SGDRegressor.py pages/"
      ],
      "metadata": {
        "id": "tL6SBH9nXW_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inicializaci√≥n del Dashboard a partir de t√∫nel local**"
      ],
      "metadata": {
        "id": "I4Ut_UzpK3Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 /usr/local/bin/cloudflared\n",
        "\n",
        "#Ejecutar Streamlit\n",
        "!streamlit run TAM.py &>/content/logs.txt & #Cambiar 0_üëã_Hello.py por el nombre de tu archivo principal\n",
        "\n",
        "#Exponer el puerto 8501 con Cloudflare Tunnel\n",
        "!cloudflared tunnel --url http://localhost:8501 > /content/cloudflared.log 2>&1 &\n",
        "\n",
        "#Leer la URL p√∫blica generada por Cloudflare\n",
        "import time\n",
        "time.sleep(5)  # Esperar que se genere la URL\n",
        "\n",
        "import re\n",
        "found_context = False  # Indicador para saber si estamos en la secci√≥n correcta\n",
        "\n",
        "with open('/content/cloudflared.log') as f:\n",
        "    for line in f:\n",
        "        #Detecta el inicio del contexto que nos interesa\n",
        "        if \"Your quick Tunnel has been created\" in line:\n",
        "            found_context = True\n",
        "\n",
        "        #Busca una URL si ya se encontr√≥ el contexto relevante\n",
        "        if found_context:\n",
        "            match = re.search(r'https?://\\S+', line)\n",
        "            if match:\n",
        "                url = match.group(0)  #Extrae la URL encontrada\n",
        "                print(f'Tu aplicaci√≥n est√° disponible en: {url}')\n",
        "                break  #Termina el bucle despu√©s de encontrar la URL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWx9MTraDhcs",
        "outputId": "6e33774c-328b-492b-c71e-0370c4524579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-24 02:19:23--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2025.5.0/cloudflared-linux-amd64 [following]\n",
            "--2025-05-24 02:19:23--  https://github.com/cloudflare/cloudflared/releases/download/2025.5.0/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/797840ed-70cb-47b8-a6fe-ecb4b3385c94?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250524%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250524T021805Z&X-Amz-Expires=300&X-Amz-Signature=f3770c8694699edf9705fb6b2d1f6d26606145f00cc982260689876750f958d6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-05-24 02:19:23--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/797840ed-70cb-47b8-a6fe-ecb4b3385c94?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250524%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250524T021805Z&X-Amz-Expires=300&X-Amz-Signature=f3770c8694699edf9705fb6b2d1f6d26606145f00cc982260689876750f958d6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37839075 (36M) [application/octet-stream]\n",
            "Saving to: ‚Äòcloudflared-linux-amd64‚Äô\n",
            "\n",
            "cloudflared-linux-a 100%[===================>]  36.09M   142MB/s    in 0.3s    \n",
            "\n",
            "2025-05-24 02:19:24 (142 MB/s) - ‚Äòcloudflared-linux-amd64‚Äô saved [37839075/37839075]\n",
            "\n",
            "Tu aplicaci√≥n est√° disponible en: https://christ-serving-pull-farm.trycloudflare.com\n"
          ]
        }
      ]
    }
  ]
}